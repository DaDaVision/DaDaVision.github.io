<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DaDaVision</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-03T01:22:38.303Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Tengda Zhao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习笔试面试总结</title>
    <link href="http://yoursite.com/2018/09/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/09/03/深度学习笔试面试总结/</id>
    <published>2018-09-03T01:19:27.000Z</published>
    <updated>2018-09-03T01:22:38.303Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="面试" scheme="http://yoursite.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔试面试总结</title>
    <link href="http://yoursite.com/2018/09/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AF%95%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/09/03/机器学习笔试面试总结/</id>
    <published>2018-09-03T01:19:17.000Z</published>
    <updated>2018-09-03T09:14:15.423Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q1. 下列说法正确的是？（多选）</strong></p><p>A. AdaGrad 使用的是一阶导数</p><p>B. L-BFGS 使用的是二阶导数</p><p>C. AdaGrad 使用的是二阶导数<br><a id="more"></a></p><p>D. L-BFGS 使用的是一阶导数</p><p><strong>答案</strong>：AB</p><p><strong>解析</strong>：AdaGrad 是基于梯度下降算法的，AdaGrad算法能够在训练中自动的对学习速率 α 进行调整，对于出现频率较低参数采用较大的 α 更新；相反，对于出现频率较高的参数采用较小的 α 更新。Adagrad非常适合处理稀疏数据。很明显，AdaGrad 算法利用的是一阶导数。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-72719e5327ac8b1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>L-BFGS 是基于牛顿优化算法的，牛顿优化算法使用的是二阶导数。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-83efe7aa07c70bab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>Q2. “增加卷积核的尺寸，一定能提高卷积神经网络的性能。” 这句话是否正确？</strong></p><p>A. 正确</p><p>B. 错误</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：卷积核的尺寸是超参数，不一定增加其尺寸就一定增加神经网络的性能，需要验证选择最佳尺寸。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f6f31ede1eaafc51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>Q3. 假设你在卷积神经网络的第一层中有 5 个卷积核，每个卷积核尺寸为 7×7，具有零填充且步幅为 1。该层的输入图片的维度是 224×224×3。那么该层输出的维度是多少？</strong></p><p>A. 217 x 217 x 3</p><p>B. 217 x 217 x 8</p><p>C. 218 x 218 x 5</p><p>D. 220 x 220 x 7</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：一般地，如果原始图片尺寸为 nxn，filter 尺寸为 fxf，则卷积后的图片尺寸为 (n-f+1)x(n-f+1)，注意 f 一般为奇数。</p><p>若考虑存在填充和步幅，用 s 表示 stride 长度，p 表示 padding 长度，如果原始图片尺寸为 nxn，filter 尺寸为 fxf，则卷积后的图片尺寸为：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f6f31ede1eaafc51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上式中，⌊⋯⌋ 表示向下取整。</p><p>此例中， n=224，p=0，f=7，s=1，因此，该层输出的尺寸为 218x218。</p><p>输出的第三个维度由滤波器的个数决定，即为 5。</p><p><strong>Q4. 假如现在有个神经网络，激活函数是 ReLU，若使用线性激活函数代替 ReLU，那么该神经网络还能表征 XNOR 函数吗？</strong></p><p>A. 可以</p><p>B. 不可以</p><p><strong>答案</strong>：B</p><p>解析：异或（XNOR）关系是非线性的，线性激活函数只能解决线性问题，不能解决非线性问题。如果用线性激活代替 ReLU，则神经网络失去逼近非线性函数的能力。</p><p><strong>Q5. 机器学习训练时，Mini-Batch 的大小优选为2个的幂，如 256 或 512。它背后的原因是什么？</strong></p><p>A. Mini-Batch 为偶数的时候，梯度下降算法训练的更快</p><p>B. Mini-Batch 设为 2 的 幂，是为了符合 CPU、GPU 的内存要求，利于并行化处理</p><p>C. 不使用偶数时，损失函数是不稳定的</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：B</p><p><strong>Q6. 下列哪种方法可以用来减小过拟合？（多选）</strong></p><p>A. 更多的训练数据</p><p>B. L1 正则化</p><p>C. L2 正则化</p><p>D. 减小模型的复杂度</p><p><strong>答案</strong>：ABCD</p><p><strong>解析</strong>：增加训练样本、L1正则化、L2 正则化、减小模型复杂度都能有效避免发生过拟合。</p><p><strong>Q7. 下列说法错误的是？</strong></p><p>A. 当目标函数是凸函数时，梯度下降算法的解一般就是全局最优解</p><p>B. 进行 PCA 降维时，需要计算协方差矩阵</p><p>C. 沿负梯度的方向一定是最优的方向</p><p>D. 利用拉格朗日函数能解带约束的优化问题</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：沿负梯度的方向是函数值减少最快的方向但不一定就是最优方向。</p><p><strong>Q8. K-Means 算法无法聚以下哪种形状的样本？</strong></p><p>A. 圆形分布</p><p>B. 螺旋分布</p><p>C. 带状分布</p><p>D. 凸多边形分布</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：K-Means 算法是基于距离测量的，无法聚非凸形状的样本。</p><p><strong>Q9. 向量 X=[1,2,3,4,-9,0] 的 L1 范数为？</strong></p><p>A. 1</p><p>B. 19</p><p>C. 6</p><p>D. √111</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：L0 范数表示向量中所有非零元素的个数；L1 范数指的是向量中各元素的绝对值之和，又称“稀疏矩阵算子”；L2 范数指的是向量中各元素的平方和再求平方根。</p><p>本例中，L0 范数为 5，L1 范数为 19，L2 范数为 √111。</p><p><strong>Q10. 关于 L1、L2 正则化下列说法正确的是？</strong></p><p>A. L2 正则化能防止过拟合，提升模型的泛化能力，但 L1 做不到这点</p><p>B. L2 正则化技术又称为 Lasso Regularization</p><p>C. L1 正则化得到的解更加稀疏</p><p>D. L2 正则化得到的解更加稀疏</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：L1、L2 正则化都能防止过拟合，提升模型的泛化能力。L1 正则化技术又称为 Lasso Regularization。L1 正则化得到的解更加稀疏，如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-48a462e1fe29342c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>以二维情况讨论，上图左边是 L2 正则化，右边是 L1 正则化。从另一个方面来看，满足正则化条件，实际上是求解蓝色区域与黄色区域的交点，即同时满足限定条件和 Ein 最小化。对于 L2 来说，限定区域是圆，这样，得到的解 w1 或 w2 为 0 的概率很小，很大概率是非零的。</p><p>对于 L1 来说，限定区域是正方形，方形与蓝色区域相交的交点是顶点的概率很大，这从视觉和常识上来看是很容易理解的。也就是说，方形的凸点会更接近 Ein 最优解对应的 wlin 位置，而凸点处必有 w1 或 w2 为 0。这样，得到的解 w1 或 w2 为零的概率就很大了。所以，L1 正则化的解具有稀疏性。</p><p>扩展到高维，同样的道理，L2 的限定区域是平滑的，与中心点等距；而 L1 的限定区域是包含凸点的，尖锐的。这些凸点更接近 Ein 的最优解位置，而在这些凸点上，很多 wj 为 0。</p><p><strong>Q11. 有 N 个样本，一般用于训练，一般用于测试。若增大 N 值，则训练误差和测试误差之间的差距会如何变化？</strong></p><p>A. 增大</p><p>B. 减小</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：增加数据，能够有效减小过拟合，减小训练样本误差和测试样本误差之间的差距。</p><p><strong>Q12. 假定你在神经网络中的隐藏层中使用激活函数 X。在特定神经元给定任意输入，你会得到输出 -0.01。X 可能是以下哪一个激活函数？</strong></p><p>A. ReLU</p><p>B. tanh </p><p>C. Sigmoid</p><p>D. 以上都有可能</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：ReLU 的输出范围是 [0,+∞)，tanh 的输出范围是 (-1,+1)，Sigmoid 的输出范围是 (0,+1)。</p><p><strong>Q13. k-NN 最近邻方法在什么情况下效果较好？</strong></p><p>A. 样本较多但典型性不好 </p><p>B. 样本较少但典型性好 </p><p>C. 样本呈团状分布 </p><p>D. 样本呈链状分布 </p><p><strong>答案</strong>：B </p><p><strong>解析</strong>：K 近邻算法主要依靠的是周围的点，因此如果样本过多，则难以区分，典型性好的容易区分。</p><p>样本呈团状或链状都具有迷惑性，这样 kNN 就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。</p><p><strong>Q14. 下列方法中，可以用于特征降维的方法包括？（多选）</strong></p><p>A. 主成分分析 PCA </p><p>B. 线性判别分析 LDA </p><p>C. AutoEncoder </p><p>D. 矩阵奇异值分解 SVD </p><p>E. 最小二乘法 LeastSquares </p><p><strong>答案</strong>：ABCD </p><p><strong>解析</strong>：主成分分析 PCA 、线性判别分析 LDA 、AutoEncoder、矩阵奇异值分解 SVD 都是用于特征降维的方法。最小二乘法是解决线性回归问题的算法，但是并没有进行降维。</p><p><strong>Q15. 以下哪些方法不可以直接来对文本分类？</strong></p><p>A. K-Means</p><p>B. 决策树</p><p>C. 支持向量机</p><p>D. kNN</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：K-Means 是无监督算法，它之所以不能称为分类是因为它之前并没有类别标签，因此只能聚类。</p><p><strong>Q16. 在回归模型中，下列哪一项在权衡欠拟合（under-fitting）和过拟合（over-fitting）中影响最大？</strong></p><p>A. 多项式阶数</p><p>B. 更新权重 w 时，使用的是矩阵求逆还是梯度下降</p><p>C. 使用常数项</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：选择合适的多项式阶数非常重要。如果阶数过大，模型就会更加复杂，容易发生过拟合；如果阶数较小，模型就会过于简单，容易发生欠拟合。如果有对过拟合和欠拟合概念不清楚的，见下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-5b857076a6456a6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>Q17. 假设你有以下数据：输入和输出都只有一个变量。使用线性回归模型（y=wx+b）来拟合数据。那么使用留一法（Leave-One Out）交叉验证得到的均方误差是多少？</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-e1ce73a05c75fce0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>A. 10/27</p><p>B. 39/27</p><p>C. 49/27</p><p>D. 55/27</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：留一法，简单来说就是假设有 N 个样本，将每一个样本作为测试样本，其它 N-1 个样本作为训练样本。这样得到 N 个分类器，N 个测试结果。用这 N个结果的平均值来衡量模型的性能。</p><p>对于该题，我们先画出 3 个样本点的坐标：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-5946f97fb1555707.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>使用两个点进行线性拟合，分成三种情况，如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-64c71d87b2a23885.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>第一种情况下，回归模型是 y = 2，误差 E1 = 1。</p><p>第二种情况下，回归模型是 y = -x + 4，误差 E2 = 2。</p><p>第三种情况下，回归模型是 y = -1/3x + 2,误差 E3 = 2/3。</p><p>则总的均方误差为：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e407dc44d5888d06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>Q18. 下列关于极大似然估计（Maximum Likelihood Estimate，MLE），说法正确的是（多选）？</strong></p><p>A. MLE 可能并不存在</p><p>B. MLE 总是存在</p><p>C. 如果 MLE 存在，那么它的解可能不是唯一的</p><p>D. 如果 MLE 存在，那么它的解一定是唯一的</p><p><strong>答案</strong>：AC</p><p><strong>解析</strong>：如果极大似然函数 L(θ) 在极大值处不连续，一阶导数不存在，则 MLE 不存在，如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-6b21ad91933d034e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>另一种情况是 MLE 并不唯一，极大值对应两个 θ。如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-c469c4fe5e9dc986.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>Q19. 如果我们说“线性回归”模型完美地拟合了训练样本（训练样本误差为零），则下面哪个说法是正确的？</strong></p><p>A. 测试样本误差始终为零</p><p>B. 测试样本误差不可能为零</p><p>C. 以上答案都不对</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：根据训练样本误差为零，无法推断测试样本误差是否为零。值得一提是，如果测试样本样本很大，则很可能发生过拟合，模型不具备很好的泛化能力！</p><p><strong>Q20. 在一个线性回归问题中，我们使用 R 平方（R-Squared）来判断拟合度。此时，如果增加一个特征，模型不变，则下面说法正确的是？</strong></p><p>A. 如果 R-Squared 增加，则这个特征有意义</p><p>B. 如果R-Squared 减小，则这个特征没有意义</p><p>C. 仅看 R-Squared 单一变量，无法确定这个特征是否有意义。</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：线性回归问题中，R-Squared 是用来衡量回归方程与真实样本输出之间的相似程度。其表达式如下所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-fc03e7e893d7eb39.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br>上式中，分子部分表示真实值与预测值的平方差之和，类似于均方差 MSE；分母部分表示真实值与均值的平方差之和，类似于方差 Var。根据 R-Squared 的取值，来判断模型的好坏：如果结果是 0，说明模型拟合效果很差；如果结果是 1，说明模型无错误。一般来说，R-Squared 越大，表示模型拟合效果越好。R-Squared 反映的是大概有多准，因为，随着样本数量的增加，R-Square必然增加，无法真正定量说明准确程度，只能大概定量。</p><p>对于本题来说，单独看 R-Squared，并不能推断出增加的特征是否有意义。通常来说，增加一个特征，R-Squared 可能变大也可能保持不变，两者不一定呈正相关。</p><p>如果使用校正决定系数（Adjusted R-Square）：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-0c0deca260a65c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>其中，n 是样本数量，p 是特征数量。Adjusted R-Square 抵消样本数量对 R-Square的影响，做到了真正的 0~1，越大越好。</p><p><strong>Q21. 下列关于线性回归分析中的残差（Residuals）说法正确的是？</strong></p><p>A. 残差均值总是为零</p><p>B. 残差均值总是小于零</p><p>C. 残差均值总是大于零</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：线性回归分析中，目标是残差最小化。残差平方和是关于参数的函数，为了求残差极小值，令残差关于参数的偏导数为零，会得到残差和为零，即残差均值为零。</p><p><strong>Q22. 下列关于异方差（Heteroskedasticity）说法正确的是？</strong></p><p>A. 线性回归具有不同的误差项</p><p>B. 线性回归具有相同的误差项</p><p>C. 线性回归误差项为零</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：异方差性是相对于同方差（Homoskedasticity）而言的。所谓同方差，是为了保证回归参数估计量具有良好的统计性质，经典线性回归模型的一个重要假定：总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。</p><p>通常来说，奇异值的出现会导致异方差性增大。</p><p><strong>Q23. 下列哪一项能反映出 X 和 Y 之间的强相关性？</strong></p><p>A. 相关系数为 0.9</p><p>B. 对于无效假设 β=0 的 p 值为 0.0001</p><p>C. 对于无效假设 β=0 的 t 值为 30</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：相关系数的概念我们很熟悉，它反映了不同变量之间线性相关程度，一般用 r 表示。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e5efceb45b6dab94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>其中，Cov(X,Y) 为 X 与 Y 的协方差，Var[X] 为 X 的方差，Var[Y] 为 Y 的方差。r 取值范围在 [-1,1] 之间，r 越大表示相关程度越高。A 选项中，r=0.9 表示 X 和 Y 之间有较强的相关性。</p><p>而 p 和 t 的数值大小没有统计意义，只是将其与某一个阈值进行比对,以得到二选一的结论。例如，有两个假设：</p><ul><li>无效假设（null hypothesis）H0：两参量间不存在“线性”相关。</li><li>备择假设（alternative hypothesis）H1：两参量间存在“线性”相关。</li></ul><p>如果阈值是 0.05，计算出的 p 值很小，比如为 0.001，则可以说“有非常显著的证据拒绝 H0 假设,相信 H1 假设。即两参量间存在“线性”相关。p 值只用于二值化判断，因此不能说 p=0.06 一定比 p=0.07 更好。</p><p><strong>Q24. 下列哪些假设是我们推导线性回归参数时遵循的（多选）？</strong></p><p>A. X 与 Y 有线性关系（多项式关系）</p><p>B. 模型误差在统计学上是独立的</p><p>C. 误差一般服从 0 均值和固定标准差的正态分布</p><p>D. X 是非随机且测量没有误差的</p><p><strong>答案</strong>：ABCD</p><p><strong>解析</strong>：在进行线性回归推导和分析时，我们已经默认上述四个条件是成立的。</p><p><strong>Q25. 为了观察测试 Y 与 X 之间的线性关系，X 是连续变量，使用下列哪种图形比较适合？</strong></p><p>A. 散点图</p><p>B. 柱形图</p><p>C. 直方图</p><p>D. 以上都不对</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：散点图反映了两个变量之间的相互关系，在测试 Y 与 X 之间的线性关系时，使用散点图最为直观。</p><p><strong>Q26. 一般来说，下列哪种方法常用来预测连续独立变量？</strong></p><p>A. 线性回归</p><p>B. 逻辑回顾</p><p>C. 线性回归和逻辑回归都行</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：线性回归一般用于实数预测，逻辑回归一般用于分类问题。</p><p><strong>Q27. 个人健康和年龄的相关系数是 -1.09。根据这个你可以告诉医生哪个结论？</strong></p><p>A. 年龄是健康程度很好的预测器</p><p>B. 年龄是健康程度很糟的预测器</p><p>C. 以上说法都不对</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：因为相关系数的范围是 [-1,1] 之间，所以，-1.09 不可能存在。</p><p><strong>Q28. 下列哪一种偏移，是我们在最小二乘直线拟合的情况下使用的？图中横坐标是输入 X，纵坐标是输出 Y。</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-32f0dd9b646c9298.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>A. 垂直偏移（vertical offsets）</p><p>B. 垂向偏移（perpendicular offsets）</p><p>C. 两种偏移都可以</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：线性回归模型计算损失函数，例如均方差损失函数时，使用的都是 vertical offsets。perpendicular offsets 一般用于主成分分析（PCA）中。</p><p><strong>Q29. 假如我们利用 Y 是 X 的 3 阶多项式产生一些数据（3 阶多项式能很好地拟合数据）。那么，下列说法正确的是（多选）？</strong></p><p>A. 简单的线性回归容易造成高偏差（bias）、低方差（variance）</p><p>B. 简单的线性回归容易造成低偏差（bias）、高方差（variance）</p><p>C. 3 阶多项式拟合会造成低偏差（bias）、高方差（variance）</p><p>D. 3 阶多项式拟合具备低偏差（bias）、低方差（variance）</p><p><strong>答案</strong>：AD</p><p><strong>解析</strong>：偏差和方差是两个相对的概念，就像欠拟合和过拟合一样。如果模型过于简单，通常会造成欠拟合，伴随着高偏差、低方差；如果模型过于复杂，通常会造成过拟合，伴随着低偏差、高方差。</p><p>用一张图来形象地表示偏差与方差的关系：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-dde5b5280cc26c44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>偏差（bias）可以看成模型预测与真实样本的差距，想要得到 low bias，就得复杂化模型，但是容易造成过拟合。方差（variance）可以看成模型在测试集上的表现，想要得到 low variance，就得简化模型，但是容易造成欠拟合。实际应用中，偏差和方差是需要权衡的。若模型在训练样本和测试集上都表现的不错，偏差和方差都会比较小，这也是模型比较理想的情况。</p><p><strong>Q30. 假如你在训练一个线性回归模型，有下面两句话：</strong></p><pre><code>**1. 如果数据量较少，容易发生过拟合。****2. 如果假设空间较小，容易发生过拟合。**</code></pre><p><strong>关于这两句话，下列说法正确的是？</strong></p><p>A. 1 和 2 都错误</p><p>B. 1 正确，2 错误</p><p>C. 1 错误，2 正确</p><p>D. 1 和 2 都正确</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：先来看第 1 句话，如果数据量较少，容易在假设空间找到一个模型对训练样本的拟合度很好，容易造成过拟合，该模型不具备良好的泛化能力。</p><p>再来看第 2 句话，如果假设空间较小，包含的可能的模型就比较少，也就不太可能找到一个模型能够对样本拟合得很好，容易造成高偏差、低方差，即欠拟合。</p><p><strong>Q31. 假如我们使用 Lasso 回归来拟合数据集，该数据集输入特征有 100 个（X1，X2，…，X100）。现在，我们把其中一个特征值扩大 10 倍（例如是特征 X1），然后用相同的正则化参数对 Lasso 回归进行修正。</strong></p><p><strong>那么，下列说法正确的是？</strong></p><p>A. 特征 X1 很可能被排除在模型之外</p><p>B. 特征 X1 很可能还包含在模型之中</p><p>C. 无法确定特征 X1 是否被舍弃</p><p>D. 以上说法都不对</p><p><strong>答案</strong>： B</p><p><strong>解析</strong>：Lasso 回归类似于线性回归，只不过它在线性回归的基础上，增加了一个对所有参数的数值大小约束，如下所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f824a5250cab98b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>其中，t 为正则化参数。Lasso 回归其实就是在普通线性回归的损失函数的基础上增加了个 β 的约束。那么 β 的约束为什么要使用这种形式，而不使用 β 的平方约束呢？原因就在于第一范数的约束下，一部分回归系数刚好可以被约束为 0。这样的话，就达到了特征选择的效果。如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-0fd152a1d71f7c9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>左边是第二范式，右边是第一范式。第一范数约束下，β 更有可能被约束成 0。这点非常类似于 L1 和 L2 正则化的区别。</p><p>因此，Lasso 回归适用于样本数量较少，特征维度较大的情形，便于从较多特征中进行特征选择。例如 DNA 数据，特征维度很大，我们只希望通过 Lasso 回归找出与某些疾病有关的 DNA 片段。</p><p>本题中，将特征 X1 数值扩大 10 倍，他对应的回归系数将相应会减小，但不为 0，以此来保证仍然满足 β 的正则化约束。</p><p><strong>Q32. 关于特征选择，下列对 Ridge 回归和 Lasso 回归说法正确的是？</strong></p><p>A. Ridge 回归适用于特征选择</p><p>B. Lasso 回归适用于特征选择</p><p>C. 两个都适用于特征选择</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：上一题我们已经介绍过，Lasso 回归会让一部分回归系数刚好可以被约束为 0，起到特征选择的效果。</p><p>Ridge 回归又称岭回归，它是普通线性回归加上 L2 正则项，用来防止训练过程中出现的过拟合。L2 正则化效果类似上一题左图，限定区域是圆，这样，得到的回归系数为 0 的概率很小，很大概率是非零的。因此，比较来说，Lasso 回归更容易得到稀疏的回归系数，有利于舍弃冗余或无用特征，适用于特征选择。</p><p><strong>Q33. 如果在线性回归模型中增加一个特征变量，下列可能发生的是（多选）？</strong></p><p>A. R-squared 增大，Adjust R-squared 增大</p><p>B. R-squared 增大，Adjust R-squared 减小</p><p>C. R-squared 减小，Adjust R-squared 减小</p><p>D. R-squared 减小，Adjust R-squared 增大</p><p><strong>答案</strong>：AB</p><p><strong>解析</strong>：线性回归问题中，R-Squared 是用来衡量回归方程与真实样本输出之间的相似程度。其表达式如下所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-1c12567aef4f9b69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上式中，分子部分表示真实值与预测值的平方差之和，类似于均方差 MSE；分母部分表示真实值与均值的平方差之和，类似于方差 Var。一般来说，R-Squared 越大，表示模型拟合效果越好。R-Squared 反映的是大概有多准，因为，随着样本数量的增加，R-Squared 必然增加，无法真正定量说明准确程度，只能大概定量。</p><p>单独看 R-Squared，并不能推断出增加的特征是否有意义。通常来说，增加一个特征特征，R-Squared 可能变大也可能保持不变，两者不一定呈正相关。</p><p>如果使用校正决定系数（Adjusted R-Squared）：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-93556aa2d3107428.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>其中，n 是样本数量，p 是特征数量。Adjusted R-Squared 抵消样本数量对 R-Squared 的影响，做到了真正的 0~1，越大越好。</p><p>增加一个特征变量，如果这个特征有意义，Adjusted R-Square 就会增大，若这个特征是冗余特征，Adjusted R-Squared 就会减小。</p><p><strong>Q34. 下面三张图展示了对同一训练样本，使用不同的模型拟合的效果（蓝色曲线）。那么，我们可以得出哪些结论（多选）？</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-61e845f7b298f9b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>A. 第 1 个模型的训练误差大于第 2 个、第 3 个模型</p><p>B. 最好的模型是第 3 个，因为它的训练误差最小</p><p>C. 第 2 个模型最为“健壮”，因为它对未知样本的拟合效果最好</p><p>D. 第 3 个模型发生了过拟合</p><p>E. 所有模型的表现都一样，因为我们并没有看到测试数据</p><p><strong>答案</strong>：ACD</p><p><strong>解析</strong>：1、2、3 模型分别对应的多项式阶数由小到大，即模型由简单到复杂。模型越简单，容易发生欠拟合；模型越复杂，容易发生过拟合。第 1 个模型过于简单，出现欠拟合；第 3 个模型过于复杂，对训练样本拟合得很好，但在测试样本上效果会很差，即过拟合；第 2 个模型最为“健壮”，在训练样本和测试样本上拟合效果都不错！</p><p><strong>Q35. 下列哪些指标可以用来评估线性回归模型（多选）？</strong></p><p>A. R-Squared</p><p>B. Adjusted R-Squared</p><p>C. F Statistics</p><p>D. RMSE / MSE / MAE</p><p><strong>答案</strong>：ABCD</p><p><strong>解析</strong>：R-Squared 和 Adjusted R-Squared 的概念，我们在 Q3 有过介绍，它们都可以用来评估线性回归模型。F Statistics 是指在零假设成立的情况下，符合F分布的统计量，多用于计量统计学中。</p><p>RMSE 指的是均方根误差：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-3066338bd93bb507.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>MSE 指的是均方误差：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-ac45f3e8cfb10e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>MAE 指的是评价绝对误差：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-bc1ee5c524fbd792.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>以上指标都可以用来评估线性回归模型。</p><p><strong>Q36. 线性回归中，我们可以使用正规方程（Normal Equation）来求解系数。下列关于正规方程说法正确的是？</strong></p><p>A. 不需要选择学习因子</p><p>B. 当特征数目很多的时候，运算速度会很慢</p><p>C. 不需要迭代训练</p><p><strong>答案</strong>：ABC</p><p><strong>解析</strong>：求解线性回归系数，我们一般最常用的方法是梯度下降，利用迭代优化的方式。除此之外，还有一种方法是使用正规方程，原理是基于最小二乘法。下面对正规方程做简要的推导。</p><p>已知线性回归模型的损失函数 Ein 为：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-fd966d40c0a2848c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>对 Ein 计算导数，令 ∇Ein=0：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-d48a982d7ae724c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>然后就能计算出 W：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-a5484827933b999f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>以上就是使用正规方程求解系数 W 的过程。可以看到，正规方程求解过程不需要学习因子，也没有迭代训练过程。当特征数目很多的时候，XTX 矩阵求逆会很慢，这时梯度下降算法更好一些。</p><p>如果 XTX 矩阵不可逆，是奇异矩阵怎么办呢？其实，大部分的计算逆矩阵的软件程序，都可以处理这个问题，也会计算出一个逆矩阵。所以，一般伪逆矩阵是可解的。</p><p><strong>Q37. 如果 Y 是 X（X1，X2，…，Xn）的线性函数：</strong></p><p><strong>Y = β0 + β1X1 + β2X2 + ··· + βnXn</strong></p><p><strong>则下列说法正确的是（多选）？</strong></p><p>A. 如果变量 Xi 改变一个微小变量 ΔXi，其它变量不变。那么 Y 会相应改变 βiΔXi。</p><p>B. βi 是固定的，不管 Xi 如何变化</p><p>C. Xi 对 Y 的影响是相互独立的，且 X 对 Y 的总的影响为各自分量 Xi 之和</p><p><strong>答案</strong>：ABC</p><p><strong>解析</strong>：这题非常简单，Y 与 X（X1，X2，…，Xn）是线性关系，故能得出 ABC 结论。</p><p><strong>Q38. 构建一个最简单的线性回归模型需要几个系数（只有一个特征）？</strong></p><p>A. 1 个</p><p>B. 2 个</p><p>C. 3 个</p><p>D. 4 个</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：最简单的线性回归模型，只有一个特征，即 Y = aX + b，包含 a 和 b 两个系数。</p><p><strong>Q39. 下面两张图展示了两个拟合回归线（A 和 B），原始数据是随机产生的。现在，我想要计算 A 和 B 各自的残差之和。注意：两种图中的坐标尺度一样。</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-9efa7b49191ac4ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>关于 A 和 B 各自的残差之和，下列说法正确的是？</strong></p><p>A. A 比 B 高</p><p>B. A 比 B 小</p><p>C. A 与 B 相同</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：A 和 B 中各自的残差之和应该是相同的。线性回归模型的损失函数为：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-1aaeddbdb1f9da6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>对损失函数求导，并令 ∇J=0，即可得到 XW-Y=0，即残差之和始终为零。</p><p><strong>Q40. 如果两个变量相关，那么它们一定是线性关系吗？</strong></p><p>A. 是</p><p>B. 不是</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：相关不一定是线性关系，也有可能是非线性相关。</p><p><strong>Q41. 两个变量相关，它们的相关系数 r 可能为 0。这句话是否正确？</strong></p><p>A. 正确</p><p>B. 错误</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：一般来说，相关系数 r=0 是两变量相互独立的必要不充分条件。也就是说，如果两个变量相互独立，那么相关系数 r 一定为 0，如果相关系数 r=0，则不一定相互独立。相关系数 r=0 只能说明两个变量之间不存在线性关系，仍然可能存在非线性关系。</p><p>那么，若两个变量相关，存在非线性关系，那么它们的相关系数 r 就为 0。</p><p><strong>Q42. 加入使用逻辑回归对样本进行分类，得到训练样本的准确率和测试样本的准确率。现在，在数据中增加一个新的特征，其它特征保持不变。然后重新训练测试。则下列说法正确的是？</strong></p><p>A. 训练样本准确率一定会降低</p><p>B. 训练样本准确率一定增加或保持不变</p><p>C. 测试样本准确率一定会降低</p><p>D. 测试样本准确率一定增加或保持不变</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：在模型中增加更多特征一般会增加训练样本的准确率，减小 bias。但是测试样本准确率不一定增加，除非增加的特征是有效特征。</p><p>这题对应的知识点也包括了增加模型复杂度，虽然会减小训练样本误差，但是容易发生过拟合。</p><p><strong>Q43. 下面这张图是一个简单的线性回归模型,图中标注了每个样本点预测值与真实值的残差。计算 SSE 为多少？</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-4dd8f7be18e93aed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>A. 3.02</p><p>B. 0.75</p><p>C. 1.01</p><p>D. 0.604</p><p><strong>答案</strong>：A</p><p><strong>解析</strong>：SSE 是平方误差之和（Sum of Squared Error），SSE = (-0.2)^2 + (0.4)^2 + (-0.8)^2 + (1.3)^2 + (-0.7)^2 = 3.02</p><p><strong>Q44. 假设变量 Var1 和 Var2 是正相关的，那么下面那张图是正确的？图中，横坐标是 Var1，纵坐标是 Var2，且对 Var1 和 Var2 都做了标准化处理。</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-f6de97674698194d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>A. Plot 1</p><p>B. Plot 2</p><p><strong>答案</strong>：B</p><p>解析：显然，Plot 2 显示出 Var2 与 Var1 是正相关的，例如 Var2 = Var1。Plot 1 显示出 Var2 与 Var1 是负相关的，例如 Var2 = -Var1。</p><p><strong>Q45. 假设一个公司的薪资水平中位数是 $35,000，排名第 25% 和 75% 的薪资分别是 $21,000 和 $ 53,000。如果某人的薪水是 $1，那么它可以被看成是异常值（Outlier）吗？</strong></p><p>A. 可以</p><p>B. 不可以</p><p>C. 需要更多的信息才能判断</p><p>D. 以上说法都不对</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：异常值（Outlier）指样本中的个别值，其数值明显偏离它（或他们）所属样本的其余观测值，也称异常数据，离群值。目前人们对异常值的判别与剔除主要采用物理判别法和统计判别法两种方法。</p><p>所谓物理判别法就是根据人们对客观事物已有的认识，判别由于外界干扰、人为误差等原因造成实测数据值偏离正常结果，在实验过程中随时判断，随时剔除。</p><p>统计判别法是给定一个置信概率，并确定一个置信限，凡超过此限的误差，就认为它不属于随机误差范围，将其视为异常值剔除。当物理识别不易判断时，一般采用统计识别法。</p><p>该题中，所给的信息量过少，无法肯定一定是异常值。</p><p><strong>Q46. 关于“回归（Regression）”和“相关（Correlation）”，下列说法正确的是？注意：x 是自变量，y 是因变量。</strong></p><p>A. 回归和相关在 x 和 y 之间都是互为对称的</p><p>B. 回归和相关在 x 和 y 之间都是非对称的</p><p>C. 回归在 x 和 y 之间是非对称的，相关在 x 和 y 之间是互为对称的</p><p>D. 回归在 x 和 y 之间是对称的，相关在 x 和 y 之间是非对称的</p><p><strong>答案</strong>：C</p><p><strong>解析</strong>：相关（Correlation）是计算两个变量的线性相关程度，是对称的。也就是说，x 与 y 的相关系数和 y 与 x 的相关系数是一样的，没有差别。</p><p>回归（Regression）一般是利用 特征 x 预测输出 y，是单向的、非对称的。</p><p><strong>Q47. 仅仅知道变量的均值（Mean）和中值（Median），能计算的到变量的偏斜度（Skewness）吗？</strong></p><p>A. 可以</p><p>B. 不可以</p><p><strong>答案</strong>：B</p><p><strong>解析</strong>：偏斜度是对统计数据分布偏斜方向及程度的度量。偏斜度是利用 3 阶矩定义的，其计算公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-5755183c130bc9de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Q1. 下列说法正确的是？（多选）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A. AdaGrad 使用的是一阶导数&lt;/p&gt;
&lt;p&gt;B. L-BFGS 使用的是二阶导数&lt;/p&gt;
&lt;p&gt;C. AdaGrad 使用的是二阶导数&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="面试" scheme="http://yoursite.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（十一）：二进制中1的个数</title>
    <link href="http://yoursite.com/2018/09/02/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
    <id>http://yoursite.com/2018/09/02/剑指Offer（十一）：二进制中1的个数/</id>
    <published>2018-09-02T09:10:48.000Z</published>
    <updated>2018-09-02T09:15:54.767Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p><a id="more"></a><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。</p><p>举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现<strong>减1的结果是把最右边的一个1开始的所有位都取反了</strong>。这个时候如果我们再把原来的整数和减去1之后的结果做<strong>与运算</strong>，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。</p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">     int  NumberOf1(int n) &#123;</span><br><span class="line">         int count = 0;</span><br><span class="line">         while(n)&#123;</span><br><span class="line">             ++count;</span><br><span class="line">             n = (n - 1) &amp; n;</span><br><span class="line">         &#125;</span><br><span class="line">         return count;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>在Python中，由于负数使用补码表示的，对于负数，最高位为1，而负数在计算机是以补码存在的，往右移，符号位不变，符号位1往右移，最终可能会出现全1的情况，导致死循环。与0xffffffff相与，就可以消除负数的影响。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def NumberOf1(self, n):</span><br><span class="line">        # write code here</span><br><span class="line">        count = 0</span><br><span class="line">        if n&lt;0:</span><br><span class="line">            n = n &amp; 0xffffffff</span><br><span class="line">        while n:</span><br><span class="line">            count += 1</span><br><span class="line">            n = n &amp; (n-1)</span><br><span class="line">        return count</span><br></pre></td></tr></table></figure></p><p>或者可以使用一个更直观的方法，直接位移即可，代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def NumberOf1(self, n):</span><br><span class="line">        # write code here</span><br><span class="line">        return sum([(n &gt;&gt; i &amp; 1) for i in range(0,32)])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="其它" scheme="http://yoursite.com/tags/%E5%85%B6%E5%AE%83/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（十）：矩形覆盖</title>
    <link href="http://yoursite.com/2018/09/02/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%8D%81%EF%BC%89%EF%BC%9A%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/"/>
    <id>http://yoursite.com/2018/09/02/剑指Offer（十）：矩形覆盖/</id>
    <published>2018-09-02T08:01:03.000Z</published>
    <updated>2018-09-02T09:05:33.733Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p><a id="more"></a><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>以2x8的矩形为例。示意图如下：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-186ab26d39d6ccaf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我们先把2x8的覆盖方法记为f(8)。用第一个1x2小矩阵覆盖大矩形的最左边时有两个选择，竖着放或者横着放。当竖着放的时候，右边还剩下2x7的区域，这种情况下的覆盖方法记为f(7)。接下来考虑横着放的情况。当1x2的小矩形横着放在左上角的时候，左下角也横着放一个1x2的小矩形，而在右边还剩下2x6的区域，这种情况下的覆盖方法记为f(6)。因此f(8)=f(7)+f(6)。此时我们可以看出，这仍然是<strong>斐波那契数列</strong>。</p><p>注：这里用循环就好，用递归的话，时间复杂度比较高。</p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int rectCover(int number) &#123;</span><br><span class="line">if(number &lt;= 2)&#123;</span><br><span class="line">            return number;</span><br><span class="line">        &#125;</span><br><span class="line">        int first = 1, second = 2, third = 0;</span><br><span class="line">        for(int i = 3; i &lt;= number; i++)&#123;</span><br><span class="line">            third = first + second;</span><br><span class="line">            first = second;</span><br><span class="line">            second = third;</span><br><span class="line">        &#125;</span><br><span class="line">        return third;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def rectCover(self, number):</span><br><span class="line">        # write code here</span><br><span class="line">        if number &lt;= 2:</span><br><span class="line">            return number</span><br><span class="line">        first, second, third = 1, 2, 0</span><br><span class="line">        for i in range(3, number+1):</span><br><span class="line">            third = first + second</span><br><span class="line">            first = second</span><br><span class="line">            second = third</span><br><span class="line">        return third</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="递归" scheme="http://yoursite.com/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（九）：变态跳台阶</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（九）：变态跳台阶/</id>
    <published>2018-09-01T15:42:23.000Z</published>
    <updated>2018-09-02T08:05:01.812Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p><a id="more"></a><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>我想说，这青蛙真变态，真能跳。</p><ul><li>当n=1时，结果为1；</li><li>当n=2时，结果为2；</li><li>当n=3时，结果为4；</li></ul><p>以此类推，我们使用数学归纳法不难发现，跳法f(n)=2^(n-1)。</p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int jumpFloorII(int number) &#123;</span><br><span class="line">if(number == 0)&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;</span><br><span class="line">        int total = 1;</span><br><span class="line">        for(int i = 1; i &lt; number; i++)&#123;</span><br><span class="line">            total *= 2;</span><br><span class="line">        &#125;</span><br><span class="line">        return total;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def jumpFloorII(self, number):</span><br><span class="line">        # write code here</span><br><span class="line">        if number &lt;= 2:</span><br><span class="line">            return number</span><br><span class="line">        total = 1</span><br><span class="line">        for _ in range(1, number):</span><br><span class="line">            total *= 2</span><br><span class="line">        return total</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="递归" scheme="http://yoursite.com/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（八）：跳台阶</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E8%B7%B3%E5%8F%B0%E9%98%B6/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（八）：跳台阶/</id>
    <published>2018-09-01T15:30:29.000Z</published>
    <updated>2018-09-02T08:05:07.039Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。<br><a id="more"></a></p><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>首先我们考虑最简单的情况。如果只有1级台阶，那么显然只一种跳法。如果有2级台阶，那就有两种跳法：一种是分两次跳，每次跳1级；另一种是一次跳2级。</p><p>接着，我们来讨论一般情况。我们把n级台阶时的跳法看成是n的函数，记为f(n)。当n&gt;2时，第一次跳的时候就有两种不同的选择：一是第一次只跳1级，此时跳法数目等于后面剩下的n-1级台阶的跳法数目，即为f(n-1)；另外一种选择是跳一次跳2级，此时跳法数目等于后面剩下的n-2级台阶的跳法数目，即为f(n-2)。因此n级台阶的不同跳法的总数<strong>f(n)=f(n-1)+f(n-2)</strong>。分析到这里，我们不难看出这实际上就是<strong>斐波那契数列</strong>了。</p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int jumpFloor(int number) &#123;</span><br><span class="line">        if(number &lt;= 0)&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;</span><br><span class="line">        else if(number &lt; 3)&#123;</span><br><span class="line">            return number;</span><br><span class="line">        &#125;</span><br><span class="line">        int first = 1, second = 2, third = 0;</span><br><span class="line">        for(int i = 3; i &lt;= number; i++)&#123;</span><br><span class="line">            third = first + second;</span><br><span class="line">            first = second;</span><br><span class="line">            second = third;</span><br><span class="line">        &#125;</span><br><span class="line">        return third;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def jumpFloor(self, number):</span><br><span class="line">        # write code here</span><br><span class="line">        if number &lt; 3:</span><br><span class="line">            return number</span><br><span class="line">        first, second, third = 1, 2, 0</span><br><span class="line">        for i in range(3, number+1):</span><br><span class="line">            third = first + second</span><br><span class="line">            first = second</span><br><span class="line">            second = third</span><br><span class="line">        return third</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。&lt;br&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="递归" scheme="http://yoursite.com/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（七）：裴波那契数列</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E8%A3%B4%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（七）：裴波那契数列/</id>
    <published>2018-09-01T15:03:28.000Z</published>
    <updated>2018-09-01T15:21:50.072Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。（n&lt;=39）</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。（n&lt;=39）</p><a id="more"></a><p>斐波那契数列公式为：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-8953edbbc9043289.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>这道题递归很好写，但是存在<strong>很严重的效率问题。</strong>我们以求解f(10)为例类分析递归的求解过程。想求f(10)，需要先求得f(9)和f(8)。同样，想求得f(9)，需要先求的f(8)和f(7)….我们可以用树形结构来表示这种依赖关系，如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-8a743d1342848c9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我们不难发现在这棵树中有<strong>很多结点是重复的，而且重复的结点数会随着n的增加而急剧增加，这意味计算量会随着n的增加而急剧增大</strong>。事实上，递归方法计算的时间复杂度是以n的指数的方式递增的。</p><p>所以，使用简单的循环方法来实现。</p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>Ｃ++:</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Fibonacci</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n &lt;= <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> first = <span class="number">0</span>, second = <span class="number">1</span>, third = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            third = first + second;</span><br><span class="line">            first = second;</span><br><span class="line">            second = third;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> third;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> n</span><br><span class="line">        first, second, third = <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">            third = first + second</span><br><span class="line">            first = second</span><br><span class="line">            second = third</span><br><span class="line">        <span class="keyword">return</span> third</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。（n&amp;lt;=39）&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。（n&amp;lt;=39）&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="递归" scheme="http://yoursite.com/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（六）：旋转数组的最小数字</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（六）：旋转数组的最小数字/</id>
    <published>2018-09-01T12:22:31.000Z</published>
    <updated>2018-09-01T14:13:02.283Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为<strong>数组的旋转</strong>。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p><strong>把一个数组最开始的若干个元素搬到数组的末尾</strong>，我们称之为<strong>数组的旋转</strong>。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</p><a id="more"></a><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>我们注意到旋转之后的数组实际上可以划分为<strong>两个排序的子数组</strong>，而且前面的子数组的元素大于或者等于后面子数组的元素。我们还注意到最小的元素刚好是这两个子数组的分界线。在排序的数组中可以用二分查找实现O(logn)的查找。本题给出的数组在一定程度上是排序的，因此我们可以试着用二分查找法的思路来寻找这个最小的元素。</p><ul><li>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</li><li>接着我们可以找到数组中间的元素。如果中间元素位于前面的递增子数组，那么它应该大于或者等于第一个指针指向的元素。此时最小元素应该位于该中间元素之后，然后我们把第一个指针指向该中间元素，移动之后第一个指针仍然位于前面的递增子数组中。</li><li>同样，如果中间元素位于后面的递增子数组，那么它应该小于或者等于第二个指针指向的元素。此时最小元素应该位于该中间元素之前，然后我们把第二个指针指向该中间元素，移动之后第二个指针仍然位于后面的递增子数组中。</li><li>第一个指针总是指向前面递增数组的元素，第二个指针总是指向后面递增数组的元素。最终它们会指向两个相邻的元素，而第二个指针指向的刚好是最小的元素，这就是循环结束的条件。</li></ul><p>示意图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-704dff113e4514da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>特殊情况：</p><ul><li><p>如果把排序数组的0个元素搬到最后面，这仍然是旋转数组，我们的代码需要支持这种情况。如果发现数组中的一个数字小于最后一个数字，就可以直接返回第一个数字了。</p></li><li><p>下面这种情况，即第一个指针指向的数字、第二个指针指向的数字和中间的数字三者相等，我们无法判断中间的数字1是数以前面的递增子数组还是后面的递增子数组。正样的话，我们只能进行顺序查找。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-2dc939898031e973.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int minNumberInRotateArray(vector&lt;int&gt; rotateArray) &#123;</span><br><span class="line">        int size = rotateArray.size();//数组长度</span><br><span class="line">        if(size == 0)&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;</span><br><span class="line">        int left = 0;//左指针</span><br><span class="line">        int right = size - 1;//右指针</span><br><span class="line">        int mid = 0;//中间指针</span><br><span class="line">        while(rotateArray[left] &gt;= rotateArray[right])&#123;//确保旋转</span><br><span class="line">            if(right - left == 1)&#123;//左右指针相邻</span><br><span class="line">                mid = right;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">            mid = left + (right - left) / 2;//计算中间指针位置</span><br><span class="line">            //特殊情况：如果无法确定中间元素是属于前面还是后面的递增子数组，只能顺序查找</span><br><span class="line">            if(rotateArray[left] == rotateArray[right] &amp;&amp; rotateArray[mid] == rotateArray[left])&#123;</span><br><span class="line">                return MinInOrder(rotateArray, left, right);</span><br><span class="line">            &#125;</span><br><span class="line">            //中间元素位于前面的递增子数组，此时最小元素位于中间元素的后面</span><br><span class="line">            if(rotateArray[mid] &gt;= rotateArray[left])&#123;</span><br><span class="line">                left = mid;</span><br><span class="line">            &#125;</span><br><span class="line">            //中间元素位于后面的递增子数组，此时最小元素位于中间元素的前面</span><br><span class="line">            else&#123;</span><br><span class="line">                right = mid;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return rotateArray[mid];</span><br><span class="line">    &#125;</span><br><span class="line">private:</span><br><span class="line">    //顺序寻找最小值</span><br><span class="line">    int MinInOrder(vector&lt;int&gt; &amp;num, int left, int right)&#123;</span><br><span class="line">        int result = num[left];</span><br><span class="line">        for(int i = left + 1; i &lt; right; i++)&#123;</span><br><span class="line">            if(num[i] &lt; result)&#123;</span><br><span class="line">                result = num[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li></ul><p><strong>Python2.7：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def minNumberInRotateArray(self, rotateArray):</span><br><span class="line">        # write code here</span><br><span class="line">        if len(rotateArray) == 0:</span><br><span class="line">            return 0</span><br><span class="line">        left = 0</span><br><span class="line">        right = len(rotateArray) - 1</span><br><span class="line">        mid = 0</span><br><span class="line">        while rotateArray[left] &gt;= rotateArray[right]:</span><br><span class="line">            if right - left == 1:</span><br><span class="line">                mid = right</span><br><span class="line">                break</span><br><span class="line">            mid = left + (right - left) // 2</span><br><span class="line">            if rotateArray[left] == rotateArray[mid] and rotateArray[mid] == rotateArray[right]:</span><br><span class="line">                return self.minInorder(rotateArray, left, right)</span><br><span class="line">            if rotateArray[mid] &gt;= rotateArray[left]:</span><br><span class="line">                left = mid</span><br><span class="line">            else:</span><br><span class="line">                right = mid</span><br><span class="line">        return rotateArray[mid]</span><br><span class="line">    </span><br><span class="line">    def minInorder(self, array, left, right):</span><br><span class="line">        result = array[left]</span><br><span class="line">        for i in range(left+1, right+1):</span><br><span class="line">            if array[i] &lt; result:</span><br><span class="line">                result = array[i]</span><br><span class="line">        return result</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;把一个数组最开始的若干个元素搬到数组的末尾，我们称之为&lt;strong&gt;数组的旋转&lt;/strong&gt;。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;把一个数组最开始的若干个元素搬到数组的末尾&lt;/strong&gt;，我们称之为&lt;strong&gt;数组的旋转&lt;/strong&gt;。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="数组" scheme="http://yoursite.com/tags/%E6%95%B0%E7%BB%84/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（五）：用两个栈实现队列</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（五）：用两个栈实现队列/</id>
    <published>2018-09-01T11:51:19.983Z</published>
    <updated>2018-09-01T12:23:38.162Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>用<strong>两个栈来实现一个队列，完成队列的Push和Pop操作</strong>。 队列中的元素为int类型。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。<br><a id="more"></a></p><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>创建两个栈stack1和stack2，<strong>使用两个“先进后出”的栈实现一个“先进先出”的队列</strong>。</p><p>我们通过一个具体的例子分析往该队列插入和删除元素的过程。首先插入一个元素a，不妨先把它插入到stack1，此时stack1中的元素有{a}，stack2为空。再压入两个元素b和c，还是插入到stack1中，此时stack1的元素有{a,b,c}，其中c位于栈顶，而stack2仍然是空的。</p><p>这个时候我们试着从队列中删除一个元素。按照先入先出的规则，由于a比b、c先插入队列中，最先删除的元素应该是a。元素a存储在stack1中，但并不在栈顶，因此不能直接进行删除操作。注意stack2我们一直没有使用过，现在是让stack2发挥作用的时候了。如果我们把stack1中的元素逐个弹出压入stack2，元素在stack2中的顺序正好和原来在stack1中的顺序相反。因此经过3次弹出stack1和要入stack2操作之后，stack1为空，而stack2中的元素是{c,b,a}，这个时候就可以弹出stack2的栈顶a了。此时的stack1为空，而stack2的元素为{b,a}，其中b在栈顶。</p><p>因此我们的思路是：<strong>当stack2中不为空时，在stack2中的栈顶元素是最先进入队列的元素，可以弹出。如果stack2为空时，我们把stack1中的元素逐个弹出并压入stack2。由于先进入队列的元素被压倒stack1的栈底，经过弹出和压入之后就处于stack2的栈顶，有可以直接弹出。如果有新元素d插入，我们直接把它压入stack1即可。</strong></p><p>流程示意图：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-2e93b8b8d543dcbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Solution</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">    void push(int node) &#123;</span><br><span class="line">        stack1.push(node);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int pop() &#123;</span><br><span class="line">        if(stack2.empty())&#123;</span><br><span class="line">            while(stack1.size() &gt; 0)&#123;</span><br><span class="line">                int data = stack1.top();</span><br><span class="line">                stack1.pop();</span><br><span class="line">                stack2.push(data);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        int pop_element = stack2.top();</span><br><span class="line">        stack2.pop();</span><br><span class="line">        return pop_element;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">    stack&lt;int&gt; stack1;</span><br><span class="line">    stack&lt;int&gt; stack2;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong></p><p>对于python来讲，栈就是用list实现的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">class Solution:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.stack1 = []</span><br><span class="line">        self.stack2 = []</span><br><span class="line">    def push(self, node):</span><br><span class="line">        # write code here</span><br><span class="line">        self.stack1.append(node)</span><br><span class="line">    def pop(self):</span><br><span class="line">        # return xx</span><br><span class="line">        if len(self.stack2) == 0:</span><br><span class="line">            while self.stack1:</span><br><span class="line">                self.stack2.append(self.stack1.pop())</span><br><span class="line">        return self.stack2.pop()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;用&lt;strong&gt;两个栈来实现一个队列，完成队列的Push和Pop操作&lt;/strong&gt;。 队列中的元素为int类型。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。&lt;br&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="栈" scheme="http://yoursite.com/tags/%E6%A0%88/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（四）：重建二叉树</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（四）：重建二叉树/</id>
    <published>2018-09-01T08:35:08.000Z</published>
    <updated>2018-09-01T09:16:32.414Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>输入某二叉树的<strong>前序遍历</strong>和<strong>中序遍历</strong>的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><a id="more"></a><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-a95a5f2e70bca737.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>通常树有如下几种遍历方式：</p><ul><li>前序遍历：先访问根结点，再访问左子结点，最后访问右子结点。<strong>根左右</strong></li><li>中序遍历：先访问左子结点，再访问根结点，最后访问右子结点。<strong>左根右</strong></li><li>后序遍历：先访问左子结点，再访问右子结点，最后访问根结点。<strong>左右根</strong></li></ul><p>本题为前序遍历和中序遍历，<strong>最少需要两种遍历方式，才能重建二叉树。</strong></p><p>前序遍历序列中，第一个数字总是树的根结点的值。在中序遍历序列中，根结点的值在序列的中间，左子树的结点的值位于根结点的值的左边，而右子树的结点的值位于根结点的值的右边。剩下的我们可以<strong>递归</strong>来实现，具体如图：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e417b447c5ba0b2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Definition for binary tree</span><br><span class="line"> * struct TreeNode &#123;</span><br><span class="line"> *     int val;</span><br><span class="line"> *     TreeNode *left;</span><br><span class="line"> *     TreeNode *right;</span><br><span class="line"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span><br><span class="line"> * &#125;;</span><br><span class="line"> */</span><br><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    TreeNode* reConstructBinaryTree(vector&lt;int&gt; pre,vector&lt;int&gt; vin) &#123;</span><br><span class="line">if(pre.size() == 0)&#123;//如果为空，返回NULL</span><br><span class="line">            return NULL;</span><br><span class="line">        &#125;</span><br><span class="line">        //依次是前序遍历左子树，前序遍历右子树，中序遍历左子树，中序遍历右子树</span><br><span class="line">        vector&lt;int&gt; left_pre, right_pre, left_vin, right_vin;</span><br><span class="line">        //中序遍历第一个节点一定为根节点</span><br><span class="line">        TreeNode* head = new TreeNode(pre[0]);</span><br><span class="line">        //找到中序遍历的根节点</span><br><span class="line">        int root = 0;</span><br><span class="line">        //遍历找到中序遍历根节点索引值</span><br><span class="line">        for(int i = 0; i &lt; pre.size(); i++)&#123;</span><br><span class="line">            if(pre[0] == vin[i])&#123;</span><br><span class="line">                root = i;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">       //利用中序遍历的根节点，对二叉树节点进行归并</span><br><span class="line">        for(int i = 0; i &lt; root; i++)&#123;</span><br><span class="line">            left_vin.push_back(vin[i]);</span><br><span class="line">            left_pre.push_back(pre[i + 1]);//前序遍历第一个为根节点</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        for(int i = root + 1; i &lt; pre.size(); i++)&#123;</span><br><span class="line">            right_vin.push_back(vin[i]);</span><br><span class="line">            right_pre.push_back(pre[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        //递归，再对其进行上述所有步骤，即再区分子树的左、右子子数，直到叶节点</span><br><span class="line">        head-&gt;left = reConstructBinaryTree(left_pre, left_vin);</span><br><span class="line">        head-&gt;right = reConstructBinaryTree(right_pre, right_vin);</span><br><span class="line">        return head;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># class TreeNode:</span><br><span class="line">#     def __init__(self, x):</span><br><span class="line">#         self.val = x</span><br><span class="line">#         self.left = None</span><br><span class="line">#         self.right = None</span><br><span class="line">class Solution:</span><br><span class="line">    # 返回构造的TreeNode根节点</span><br><span class="line">    def reConstructBinaryTree(self, pre, tin):</span><br><span class="line">        # write code here</span><br><span class="line">        if len(pre) == 0:</span><br><span class="line">            return None</span><br><span class="line">        elif len(pre) == 1:</span><br><span class="line">            return TreeNode(pre[0])</span><br><span class="line">        else:</span><br><span class="line">            root = TreeNode(pre[0])</span><br><span class="line">            pos = tin.index(pre[0])</span><br><span class="line">            root.left = self.reConstructBinaryTree(pre[1:pos+1], tin[:pos])</span><br><span class="line">            root.right = self.reConstructBinaryTree(pre[pos+1:], tin[pos+1:])</span><br><span class="line">        return root</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;输入某二叉树的&lt;strong&gt;前序遍历&lt;/strong&gt;和&lt;strong&gt;中序遍历&lt;/strong&gt;的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="二叉树" scheme="http://yoursite.com/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（三）：从尾到头打印链表</title>
    <link href="http://yoursite.com/2018/09/01/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/"/>
    <id>http://yoursite.com/2018/09/01/剑指Offer（三）：从尾到头打印链表/</id>
    <published>2018-09-01T07:51:52.000Z</published>
    <updated>2018-09-01T08:36:10.846Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><p>输入一个链表，返回一个反序的链表。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：牛客网</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>输入一个链表，返回一个反序的链表。</p><a id="more"></a><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>通常，这种情况下，我们<strong>不希望修改原链表的结构</strong>。返回一个反序的链表，这就是经典的<strong>“后进先出”</strong>，我们可以使用<strong>栈</strong>实现这种顺序。<strong>每经过一个结点的时候，把该结点放到一个栈中。当遍历完整个链表后，再从栈顶开始逐个输出结点的值，给一个新的链表结构，这样链表就实现了反转。</strong></p><h3 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h3><p><strong>C++:</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*  struct ListNode &#123;</span></span><br><span class="line"><span class="comment">*        int val;</span></span><br><span class="line"><span class="comment">*        struct ListNode *next;</span></span><br><span class="line"><span class="comment">*        ListNode(int x) :</span></span><br><span class="line"><span class="comment">*              val(x), next(NULL) &#123;</span></span><br><span class="line"><span class="comment">*        &#125;</span></span><br><span class="line"><span class="comment">*  &#125;;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; printListFromTailToHead(ListNode* head) &#123;</span><br><span class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; nodes;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; result;</span><br><span class="line">        ListNode* node = head;</span><br><span class="line">        <span class="keyword">while</span>(node != <span class="literal">NULL</span>)&#123;</span><br><span class="line">            nodes.push(node-&gt;val);</span><br><span class="line">            node = node-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span>(!nodes.empty())&#123;</span><br><span class="line">            result.push_back(nodes.top());</span><br><span class="line">            nodes.pop();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong></p><p>对于python来讲，不用如此麻烦，我们可以直接使用<strong>列表</strong>的插入方法，<strong>每次插入数据，只插入在首位</strong>s。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回从尾部到头部的列表值序列，例如[1,2,3]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printListFromTailToHead</span><span class="params">(self, listNode)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">while</span> listNode:</span><br><span class="line">            result.insert(<span class="number">0</span>, listNode.val)</span><br><span class="line">            listNode = listNode.next</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;输入一个链表，返回一个反序的链表。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：牛客网&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;输入一个链表，返回一个反序的链表。&lt;/p&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="链表" scheme="http://yoursite.com/tags/%E9%93%BE%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>topK问题</title>
    <link href="http://yoursite.com/2018/09/01/topK%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/09/01/topK问题/</id>
    <published>2018-09-01T03:41:51.000Z</published>
    <updated>2018-09-01T04:06:41.377Z</updated>
    
    <content type="html"><![CDATA[<p>问题描述：有 N (N&gt;1000000)个数,求出其中的前K个最小的数（又被称作topK问题）。</p><p>针对topK类问题，通常比较好的方案是<strong>【分治+trie树/hash+小顶堆】</strong>，即先将数据集按照hash算法分解成多个小数据集，然后使用<strong>trie</strong>树或者hash表统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出频率最高的前K个数，最后在所有top K中求出最终的top K。<br><a id="more"></a></p><p>实际上，最优的解决方案应该是最符合实际设计需求的方案，在实际应用中，可能有足够大的内存，那么直接将数据扔到内存中一次性处理即可，也可能机器有多个核，这样可以采用多线程处理整个数据集。</p><p>第一种方法将数据<strong>全部排序</strong> ，然后在排序后的集合中进行查找，最快的排序算法的时间复杂度一般为<strong>O（nlogn）</strong>，如快速排序。但是在32位的机器上，每个float类型占4个字节，1亿个浮点数就要占用400MB的存储空间，对于一些可用内存小于400M的计算机而言，很显然是不能一次将全部数据读入内存进行排序的。其实即使内存能够满足要求（我机器内存都是8GB），该方法也并不高效，因为题目的目的是寻找出最大的10000个数即可，而排序却是将所有的元素都排序了，做了很多的无用功。</p><p>第二种方法为<strong>局部淘汰法</strong>，该方法与排序方法类似，用一个容器保存前10000个数，然后将剩余的所有数字——与容器内的最小数字相比，如果所有后续的元素都比容器内的10000个数还小，那么容器内这个10000个数就是最大10000个数。如果某一后续元素比容器内最小数字大，则删掉容器内最小元素，并将该元素插入容器，最后遍历完这1亿个数，得到的结果容器中保存的数即为最终结果了。此时的时间复杂度为O（n+m^2），其中m为容器的大小，即10000。</p><p> 第三种方法是<strong>分治法</strong>，将1亿个数据分成100份，每份100万个数据，找到每份数据中最大的10000个，最后在剩下的100<em>10000个数据里面找出最大的10000个。如果100万数据选择足够理想，那么可以过滤掉1亿数据里面99%的数据。100万个数据里面查找最大的10000个数据的方法如下：用快速排序的方法，将数据分为2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大的那堆个数N大于10000个，继续对大堆快速排序一次分成2堆，如果大堆个数N小于10000个，就在小的那堆里面快速排序一次，找第10000-n大的数字；递归以上过程，就可以找到第1w大的数。参考上面的找出第1w大数字，就可以类似的方法找到前10000大数字了。此种方法需要每次的内存空间为10^6</em>4=4MB，一共需要101次这样的比较。</p><p>第四种方法是<strong>Hash法</strong>。如果这1亿个书里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的10000个数。</p><p>第五种方法采用<strong>最小堆</strong>。首先读入前10000个数来创建大小为10000的最小堆，建堆的时间复杂度为O（mlogm）（m为数组的大小即为10000），然后遍历后续的数字，并于堆顶（最小）数字进行比较。如果比最小的数小，则继续读取后续数字；如果比堆顶数字大，则替换堆顶元素并重新调整堆为最小堆。整个过程直至1亿个数全部遍历完为止。然后按照中序遍历的方式输出当前堆中的所有10000个数字。该算法的时间复杂度为O（nmlogm），空间复杂度是10000（常数）。</p><p>还有没有更简单的算法呢？答案是肯定的。 </p><p>第六种方法<strong>利用快速排序的分划函数找到分划位置K，则其前面的内容即为所求。</strong>该算法是一种非常有效的处理方式，时间复杂度是<strong>O(N)</strong>（证明可以参考算法导论书籍）。对于能一次加载到内存中的数组，该策略非常优秀。如果能完整写出代码，那么相信面试官会对你刮目相看的。</p><p>下面，给出第六种方法的Python代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(L, left, right)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将L[left:right]进行一次快速排序的partition，返回分割点</span></span><br><span class="line"><span class="string">   :param L: 数据List</span></span><br><span class="line"><span class="string">    :param left: 排序起始位置</span></span><br><span class="line"><span class="string">   :param right: 排序终止位置</span></span><br><span class="line"><span class="string">   :return: 分割点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> left &lt; right:</span><br><span class="line">        <span class="keyword">print</span> left</span><br><span class="line">        key = L[left]</span><br><span class="line">        low = left</span><br><span class="line">        high = right</span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> L[high] &gt;= key:</span><br><span class="line">                high = high - <span class="number">1</span></span><br><span class="line">            L[low] = L[high]</span><br><span class="line">            <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> L[low] &lt;= key:</span><br><span class="line">                low = low + <span class="number">1</span></span><br><span class="line">            L[high] = L[low]</span><br><span class="line">        L[low] = key</span><br><span class="line">    <span class="keyword">return</span> low</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topK</span><span class="params">(L, K)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    求L中的前K个最小值</span></span><br><span class="line"><span class="string">   :param L: 数据List</span></span><br><span class="line"><span class="string">    :param K: 最小值的数目</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> len(L) &lt; K:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    low = <span class="number">0</span></span><br><span class="line">    high = len(L) - <span class="number">1</span></span><br><span class="line">    j = partition(L, low, high)</span><br><span class="line">    <span class="keyword">while</span> j != K: <span class="comment"># 划分位置不是K则继续处理</span></span><br><span class="line">      <span class="keyword">if</span> K &gt; j: <span class="comment">#k在分划点后面部分</span></span><br><span class="line">         low = j + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            high = j  <span class="comment"># K在分划点前面部分</span></span><br><span class="line">      j = partition(L, low, high)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;问题描述：有 N (N&amp;gt;1000000)个数,求出其中的前K个最小的数（又被称作topK问题）。&lt;/p&gt;
&lt;p&gt;针对topK类问题，通常比较好的方案是&lt;strong&gt;【分治+trie树/hash+小顶堆】&lt;/strong&gt;，即先将数据集按照hash算法分解成多个小数据集，然后使用&lt;strong&gt;trie&lt;/strong&gt;树或者hash表统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出频率最高的前K个数，最后在所有top K中求出最终的top K。&lt;br&gt;
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="海量数据" scheme="http://yoursite.com/tags/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE/"/>
    
      <category term="topK" scheme="http://yoursite.com/tags/topK/"/>
    
  </entry>
  
  <entry>
    <title>判别式模型和生成式模型</title>
    <link href="http://yoursite.com/2018/08/26/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2018/08/26/判别式模型和生成式模型/</id>
    <published>2018-08-26T14:12:53.000Z</published>
    <updated>2018-08-27T02:28:12.798Z</updated>
    
    <content type="html"><![CDATA[<h1 id="判别式模型-与-生成式模型"><a href="#判别式模型-与-生成式模型" class="headerlink" title="判别式模型 与 生成式模型"></a>判别式模型 与 生成式模型</h1><p>生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于：</p><p>对于输入x，类别标签y：</p><ul><li><strong>判别式模型</strong>估计<strong>条件概率分布$P(y|x)$</strong>（y在x下的概率 ），直接对条件概率$p(y|x;θ)$建模 。</li></ul><p><strong>举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。</strong></p><ul><li><strong>生成式模型</strong>估计它们的<strong>联合概率分布$P(x,y)$</strong>。会对x和y的联合分布$p(x,y)$建模，然后通过贝叶斯公式来求得$p(y_{i}|x)$，然后选取使得$p(y_{i}|x)$最大的$y_{i}$，即： </li></ul><a id="more"></a><p><img src="https://upload-images.jianshu.io/upload_images/12654931-fe1eb0dcb7a6c930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。</strong> </p><p><strong>生成式模型可以根据贝叶斯公式得到判别式模型，但反过来不行</strong>。</p><h2 id="判别式模型常见的主要有："><a href="#判别式模型常见的主要有：" class="headerlink" title="判别式模型常见的主要有："></a><strong>判别式模型常见的主要有：</strong></h2><ul><li>线性回归(Linear Regression)</li><li>逻辑回归(Logistic Regression)</li><li>决策树</li><li>神经网络(NN)</li><li>支持向量机(SVM)</li><li>高斯过程(Gaussian Process)</li><li>条件随机场(CRF)</li><li>CART(Classification and Regression Tree)</li><li>Boosting</li></ul><h2 id="生成式模型常见的主要有："><a href="#生成式模型常见的主要有：" class="headerlink" title="生成式模型常见的主要有："></a><strong>生成式模型常见的主要有：</strong></h2><ul><li>朴素贝叶斯</li><li>K近邻(KNN)</li><li>混合高斯模型</li><li>隐马尔科夫模型(HMM)</li><li>贝叶斯网络</li><li>Sigmoid Belief Networks</li><li>马尔科夫随机场(Markov Random Fields)</li><li>深度信念网络(DBN)</li></ul><p>​    假设你现在有一个分类问题，x是特征，y是类标记。用生成模型学习一个联合概率分布$P（x，y）$，而用判别模型学习一个条件概率分布$P（y|x）$。<br>    用一个简单的例子来说明这个这个问题。假设x就是两个（1或2），y有两类（0或1），有如下如下样本（1，0）、（1，0）、（2，0）、（2，1）<br>则学习到的<strong>联合概率分布$P（x，y）$</strong>（生成模型）如下：</p><table><thead><tr><th></th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=1</td><td>1/2</td><td>0</td></tr><tr><td>x=2</td><td>1/4</td><td>1/4</td></tr></tbody></table><p>而学习到的<strong>条件概率分布</strong>$P（y|x）$（判别模型）如下： </p><table><thead><tr><th></th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=1</td><td>1</td><td>0</td></tr><tr><td>x=2</td><td>1/2</td><td>1/2</td></tr></tbody></table><p>​    在实际分类问题中，<strong>判别模型可以直接用来判断特征的类别情况</strong>，而<strong>生成模型，需要加上贝叶斯法则</strong>，然后应用到分类中。但是，生成模型的概率分布可以还有其他应用，就是说生成模型更一般更普适。不过判别模型更直接，更简单。 </p><h1 id="两个模型的对比"><a href="#两个模型的对比" class="headerlink" title="两个模型的对比"></a>两个模型的对比</h1><p><img src="https://upload-images.jianshu.io/upload_images/12654931-8a5d642f888471b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;判别式模型-与-生成式模型&quot;&gt;&lt;a href=&quot;#判别式模型-与-生成式模型&quot; class=&quot;headerlink&quot; title=&quot;判别式模型 与 生成式模型&quot;&gt;&lt;/a&gt;判别式模型 与 生成式模型&lt;/h1&gt;&lt;p&gt;生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于：&lt;/p&gt;
&lt;p&gt;对于输入x，类别标签y：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;判别式模型&lt;/strong&gt;估计&lt;strong&gt;条件概率分布$P(y|x)$&lt;/strong&gt;（y在x下的概率 ），直接对条件概率$p(y|x;θ)$建模 。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成式模型&lt;/strong&gt;估计它们的&lt;strong&gt;联合概率分布$P(x,y)$&lt;/strong&gt;。会对x和y的联合分布$p(x,y)$建模，然后通过贝叶斯公式来求得$p(y_{i}|x)$，然后选取使得$p(y_{i}|x)$最大的$y_{i}$，即： &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="判别式模型" scheme="http://yoursite.com/tags/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="生成式模型" scheme="http://yoursite.com/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer（二）：替换空格</title>
    <link href="http://yoursite.com/2018/08/25/%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/"/>
    <id>http://yoursite.com/2018/08/25/剑指Offer（二）：替换空格/</id>
    <published>2018-08-25T13:42:37.000Z</published>
    <updated>2018-09-01T05:10:42.427Z</updated>
    
    <content type="html"><![CDATA[<h1 id="剑指Offer-二-：替换空格"><a href="#剑指Offer-二-：替换空格" class="headerlink" title="剑指Offer(二)：替换空格"></a>剑指Offer(二)：替换空格</h1><p>摘要</p><p>请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。</p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本系列文章为《剑指Offer》刷题笔记。</p><p>刷题平台：[牛客网]</p><h2 id="二、题目"><a href="#二、题目" class="headerlink" title="二、题目"></a>二、题目</h2><p>请实现一个函数，将一个[字符串]中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。<br><a id="more"></a></p><h3 id="1、思路"><a href="#1、思路" class="headerlink" title="1、思路"></a>1、思路</h3><p>最简单的方法就是从头到尾遍历，但是时间复杂度为O(n^2)。</p><p>本文采用一种时间复杂度为O(n)的方法。</p><p>我们可以先遍历一次字符串，这样就可以统计出字符串空格的总数，并可以由此计算出替换之后的字符串的总长度。<strong>每替换一个空格，长度增加2</strong>，因此替换以后字符串的长度等于原来的长度加上2乘以空格数目。以”We are happy”为例，”We are happy”这个字符串的长度为14（包括结尾符号”\n”），里面有两个空格，因此替换之后字符串的长度是18。</p><p>我们从字符串的尾部开始复制和替换。首先准备两个指针，P1和P2，P1指向原始字符串的末尾，而P2指向替换之后的字符串的末尾。接下来我们向前移动指针P1，逐个把它指向的字符复制到P2指向的位置，直到碰到第一个空格为止。碰到第一个空格之后，把P1向前移动1格，在P2之前插入字符串”%20”。由于”%20”的长度为3，同时也要把P2向前移动3格。</p><p>移动示意图：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-8645629703febdaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h3 id="2、编程实现"><a href="#2、编程实现" class="headerlink" title="2、编程实现"></a>2、编程实现</h3><p><strong>C++：</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">replaceSpace</span><span class="params">(<span class="keyword">char</span> *str,<span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span>(str == <span class="literal">NULL</span> &amp;&amp; length &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*original_length为字符串str的实际长度*/</span></span><br><span class="line">        <span class="keyword">int</span> original_length = <span class="number">0</span>;<span class="comment">//原始长度</span></span><br><span class="line">        <span class="keyword">int</span> number_blank = <span class="number">0</span>;<span class="comment">//空格数</span></span><br><span class="line">        <span class="keyword">int</span> i;</span><br><span class="line">        <span class="keyword">while</span>(str[i++] != <span class="string">'\0'</span>)&#123;<span class="comment">//遍历字符串</span></span><br><span class="line">            ++original_length;<span class="comment">//长度+1</span></span><br><span class="line">            <span class="keyword">if</span>(str[i] == <span class="string">' '</span>)&#123;</span><br><span class="line">                ++number_blank;<span class="comment">//遇到空格+1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*new_length为把空格替换成'%20'之后的长度*/</span></span><br><span class="line">        <span class="keyword">int</span> new_length = original_length + <span class="number">2</span> * number_blank;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">int</span> index_original = original_length;<span class="comment">//原始字符串末尾索引值</span></span><br><span class="line">        <span class="keyword">int</span> index_new = new_length;<span class="comment">//计算长度后的字符串末尾索引值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">/*index_original指针开始向前移动，如果遇到空格，替换成'%20'，否则进行复制操作*/</span></span><br><span class="line">        <span class="keyword">while</span>(index_original &gt;= <span class="number">0</span> &amp;&amp; index_new &gt; index_original)&#123;</span><br><span class="line">            <span class="keyword">if</span>(str[index_original] == <span class="string">' '</span>)&#123;</span><br><span class="line">                str[index_new--] = <span class="string">'0'</span>;</span><br><span class="line">                str[index_new--] = <span class="string">'2'</span>;</span><br><span class="line">                str[index_new--] = <span class="string">'%'</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                str[index_new--] = str[index_original];</span><br><span class="line">            &#125;</span><br><span class="line">            --index_original;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p><strong>Python2.7：</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s 源字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceSpace</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">return</span> s.replace(<span class="string">' '</span>, <span class="string">'%20'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;剑指Offer-二-：替换空格&quot;&gt;&lt;a href=&quot;#剑指Offer-二-：替换空格&quot; class=&quot;headerlink&quot; title=&quot;剑指Offer(二)：替换空格&quot;&gt;&lt;/a&gt;剑指Offer(二)：替换空格&lt;/h1&gt;&lt;p&gt;摘要&lt;/p&gt;
&lt;p&gt;请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。&lt;/p&gt;
&lt;h2 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h2&gt;&lt;p&gt;本系列文章为《剑指Offer》刷题笔记。&lt;/p&gt;
&lt;p&gt;刷题平台：[牛客网]&lt;/p&gt;
&lt;h2 id=&quot;二、题目&quot;&gt;&lt;a href=&quot;#二、题目&quot; class=&quot;headerlink&quot; title=&quot;二、题目&quot;&gt;&lt;/a&gt;二、题目&lt;/h2&gt;&lt;p&gt;请实现一个函数，将一个[字符串]中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。&lt;br&gt;
    
    </summary>
    
      <category term="剑指Offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87Offer/"/>
    
    
      <category term="笔试" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AF%95/"/>
    
      <category term="剑指Offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="字符串" scheme="http://yoursite.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    
  </entry>
  
  <entry>
    <title>为什么梯度的负方向是梯度下降最快的方向</title>
    <link href="http://yoursite.com/2018/08/25/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%B4%9F%E6%96%B9%E5%90%91%E6%98%AF%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E5%90%91/"/>
    <id>http://yoursite.com/2018/08/25/为什么梯度的负方向是梯度下降最快的方向/</id>
    <published>2018-08-25T11:52:01.000Z</published>
    <updated>2018-08-25T13:11:20.688Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是梯度？"><a href="#什么是梯度？" class="headerlink" title="什么是梯度？"></a>什么是梯度？</h2><p>​    对于梯度下降算法（Gradient Descent Algorithm），我们都已经很熟悉了。无论是在线性回归（Linear Regression）、逻辑回归（Logistic Regression）还是神经网络（Neural Network）等等，都会用到梯度下降算法。我们先来看一下梯度下降算法的直观解释：</p><p>​    假设我们位于山的某个山腰处，山势连绵不绝，不知道怎么下山。于是决定走一步算一步，也就是每次沿着当前位置最陡峭最易下山的方向前进一小步，然后继续沿下一个位置最陡方向前进一小步。这样一步一步走下去，一直走到觉得我们已经到了山脚。这里的<strong>下山最陡的方向就是梯度的负方向</strong>。    </p><p>​    首先理解什么是<strong>梯度</strong>？通俗来说，<strong>梯度就是表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在当前位置的导数</strong>。</p><p>$∇=\frac{df(\theta)}{d\theta}$</p><a id="more"></a><p>上式中，$\theta$是自变量，$f(\theta)$是关于$\theta$的函数，$\theta$表示梯度。</p><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>​    如果函数$f(\theta)$是凸函数，那么就可以使用梯度下降算法进行优化。梯度下降算法的公式我们已经很熟悉了：</p><p>$\theta=\theta_{0}−η⋅∇f(\theta_{0})$</p><p>​    其中，$\theta_{0}$是自变量参数，即下山位置坐标，$η$是学习因子，即下山每次前进的一小步（步进长度），$\theta$是更新后的$\theta_{0}$，即下山移动一小步之后的位置。</p><p>​    梯度下降算法的公式非常简单！但是”沿着梯度的反方向（坡度最陡）“是我们日常经验得到的，其本质的原因到底是什么呢？为什么局部下降最快的方向就是梯度的负方向呢？也许很多朋友还不太清楚。没关系，接下来我将以通俗的语言来详细解释梯度下降算法公式的数学推导过程。</p><h2 id="一阶泰勒展开式"><a href="#一阶泰勒展开式" class="headerlink" title="一阶泰勒展开式"></a>一阶泰勒展开式</h2><p>​    这里需要一点数学基础，对泰勒展开式有些了解。简单地来说，泰勒展开式利用的就是函数的局部线性近似这个概念。我们以一阶泰勒展开式为例：</p><p>$f(\theta)\approx f(\theta_{0})+(\theta-\theta_{0})\cdot\bigtriangledown f(\theta_{0})$</p><p>不懂上面的公式？没有关系。用下面这张图来解释。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-d3059cfb700bba95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>凸函数$f(\theta)​$的某一小段$[\theta_{0},\theta]​$由上图黑色曲线表示，可以利用线性近似的思想求出$f(\theta)​$的值，如上图红色直线。该直线的斜率等于$f(\theta)​$在$\theta_{0}​$处的导数。则根据直线方程，很容易得到$f(\theta)​$的近似表达式为：</p><p>$f(\theta)\approx f(\theta_{0})+(\theta-\theta_{0})\cdot\bigtriangledown f(\theta_{0})$</p><p>这就是一阶泰勒展开式的推导过程，主要利用的数学思想就是曲线函数的线性拟合近似。</p><h2 id="梯度下降数学原理"><a href="#梯度下降数学原理" class="headerlink" title="梯度下降数学原理"></a>梯度下降数学原理</h2><p>知道了一阶泰勒展开式之后，接下来就是重点了！我们来看一下梯度下降算法是如何推导的。</p><p>先写出一阶泰勒展开式的表达式：</p><p>$f(\theta)≈f(\theta_{0})+(\theta−\theta_{0})⋅∇f(\theta_{0})$</p><p>其中，$\theta−\theta_{0}$是微小矢量，它的大小就是我们之前讲的步进长度η，类比于下山过程中每次前进的一小步，η为标量，而$\theta−\theta_{0}$的单位向量用$v$表示。则$\theta−\theta_{0}$可表示为：</p><p>$\theta−\theta_{0}=ηv$</p><p>特别需要注意的是，$\theta−\theta_{0}$不能太大，因为太大的话，线性近似就不够准确，一阶泰勒近似也不成立了。替换之后，$f(\theta)$的表达式为：</p><p>$f(\theta)≈f(\theta_{0})+ηv⋅∇f(\theta_{0})$</p><p>重点来了，局部下降的目的是希望每次$\theta$更新，都能让函数值$f(\theta)$变小。也就是说，上式中，我们希望$f(\theta)&lt;f(\theta_{0})$。则有：</p><p>$f(\theta)−f(\theta_{0})≈ηv⋅∇f(\theta_{0})&lt;0$</p><p>因为$η$为标量，且一般设定为正值，所以可以忽略，不等式变成了：</p><p>$v⋅∇f(\theta_{0})&lt;0$</p><p>上面这个不等式非常重要！$v$和$∇f(\theta_{0})$都是向量，$∇f(\theta_{0})$是当前位置的梯度方向，$v$表示下一步前进的单位向量，是需要我们求解的，有了它，就能根据$\theta−\theta_{0}=ηv$确定$\theta$值了。</p><p>想要两个向量的乘积小于零，我们先来看一下两个向量乘积包含哪几种情况：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f7f779a9600abcb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>A和B均为向量，$α$为两个向量之间的夹角。A和B的乘积为：</p><p>$A⋅B=||A||⋅||B||⋅cos(α)$</p><p>$||A||$和$||B||$均为标量，在$||A||$和$||B||$确定的情况下，只要$cos(α)=−1$，即A和B完全反向，就能让A和B的向量乘积最小（负最大值）。</p><p>顾名思义，当$v$与$∇f(\theta_{0})$互为反向，即$v$为当前梯度方向的负方向的时候，能让$v⋅∇f(\theta_{0})$最大程度地小，也就保证了$v$的方向是局部下降最快的方向。</p><p>知道$v$是$∇f(\theta_{0})$的反方向后，可直接得到：</p><p>$v=−\frac{∇f(\theta_{0})}{||∇f(\theta_{0})||}$</p><p>之所以要除以$∇f(\theta_{0})$的模$||∇f(\theta_{0})||$，是因为$v$是单位向量。</p><p>求出最优解$v$之后，带入到$\theta−\theta_{0}=ηv$中，得：</p><p>$v=\theta_{0}−η\frac{∇f(\theta_{0})}{||∇f(\theta_{0})||}$</p><p>一般地，因为$||∇f(\theta_{0})||$是标量，可以并入到步进因子$η$中，即简化为：</p><p>$\theta=\theta_{0}−η∇f(\theta_{0})$</p><p>这样，我们就推导得到了梯度下降算法中$\theta$的更新表达式。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们通过一阶泰勒展开式，利用线性近似和向量相乘最小化的思想搞懂了梯度下降算法的数学原理。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是梯度？&quot;&gt;&lt;a href=&quot;#什么是梯度？&quot; class=&quot;headerlink&quot; title=&quot;什么是梯度？&quot;&gt;&lt;/a&gt;什么是梯度？&lt;/h2&gt;&lt;p&gt;​    对于梯度下降算法（Gradient Descent Algorithm），我们都已经很熟悉了。无论是在线性回归（Linear Regression）、逻辑回归（Logistic Regression）还是神经网络（Neural Network）等等，都会用到梯度下降算法。我们先来看一下梯度下降算法的直观解释：&lt;/p&gt;
&lt;p&gt;​    假设我们位于山的某个山腰处，山势连绵不绝，不知道怎么下山。于是决定走一步算一步，也就是每次沿着当前位置最陡峭最易下山的方向前进一小步，然后继续沿下一个位置最陡方向前进一小步。这样一步一步走下去，一直走到觉得我们已经到了山脚。这里的&lt;strong&gt;下山最陡的方向就是梯度的负方向&lt;/strong&gt;。    &lt;/p&gt;
&lt;p&gt;​    首先理解什么是&lt;strong&gt;梯度&lt;/strong&gt;？通俗来说，&lt;strong&gt;梯度就是表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在当前位置的导数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;$∇=\frac{df(\theta)}{d\theta}$&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降" scheme="http://yoursite.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>协方差与相关系数</title>
    <link href="http://yoursite.com/2018/08/25/%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    <id>http://yoursite.com/2018/08/25/协方差与相关系数/</id>
    <published>2018-08-25T02:52:46.000Z</published>
    <updated>2018-08-28T08:14:48.396Z</updated>
    
    <content type="html"><![CDATA[<p>什么是协方差（Covariance）？</p><blockquote><p>协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。</p></blockquote><p>以上是某百科的解释。</p><p><strong>1、协方差是怎么来的？</strong></p><a id="more"></a><p>​    简单地来说，<strong>协方差</strong>就是反映两个变量 X 和 Y 的相互关系。这种相互关系大致分为三种：<strong>正相关</strong>、<strong>负相关</strong>、<strong>不相关</strong>。</p><p>​    什么是正相关呢？例如房屋面积（X）越大，房屋总价（Y）越高，则房屋面积与房屋总价是正相关的；</p><p>​    什么是负相关呢？例如一个学生打游戏的时间（X）越多，学习成绩（Y）越差，则打游戏时间与学习成绩是负相关的；</p><p>​    什么是不相关呢？例如一个人皮肤的黑白程度（X）与他的身体健康程度（Y）并无明显关系，所以是不相关的。</p><p>我们先来看第一种情况，令变量 X 和变量 Y 分别为：</p><p>X = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]</p><p>Y = [12 15 17 21 22 21 18 23 26 25 22 28 24 28 30 33 28 34 36 35]</p><p>在坐标上描绘出 X 和 Y 的联合分布：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-c7b704496e93a645.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>显然，Y 在整体趋势上是随着 X 的增加而增加的，即 Y 与 X 的变化是同向的。这种情况，我们就称 X 与 Y 是<strong>正相关</strong>的。</p><p>我们再来看第二种情况，令变量 X 和变量 Y 分别为：</p><p>X = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]</p><p>Y = [35 35 29 29 28 28 27 26 26 23 21 22 25 19 16 19 20 16 15 16]</p><p>在坐标上描绘出 X 和 Y 的联合分布：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-449b98898b06e5dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>显然，Y 在整体趋势上是随着 X 的增加而减少的，即 Y 与 X 的变化是反向的。这种情况，我们就称 X 与 Y 是<strong>负相关</strong>的。</p><p>我们再来看第三种情况，令变量 X 和变量 Y 分别为：</p><p>X = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]</p><p>Y = [16 16 28 17 20 26 20 17 21 15 12 29 24 25 16 15 21 13 17 25]</p><p>在坐标上描绘出 X 和 Y 的联合分布：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-783fd5ff0bdbf5d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>显然，Y 在整体趋势上与 X 的并无正相关或者负相关的关系。这种情况，我们就称 X 与 Y 是<strong>不相关</strong>的。</p><p>回过头来，我们来看 X 与 Y 正相关的情况，令 EX、EY 分别是 X 和 Y 的<strong>期望</strong>值。什么是期望呢？在这里我们可以把它看成是<strong>平均值</strong>，即 EX 是变量 X 的平均值，EY 是变量 Y 的平均值。把 EX 和 EY 在图中表示出来得到下面的图形：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-5af683128993d898.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上图中，整个区域被 EX 和 EY 分割成 I、II、III、IV 四个区域，且 X 和 Y 大部分分布在 I、III 区域内，只有少部分分布在 II、IV 区域内。</p><p>在区域 I 中，满足 X&gt;EX，Y&gt;EY，则有 (X-EX)(Y-EY)&gt;0；</p><p>在区域 II 中，满足 X&lt;EX，Y&gt;EY，则有 (X-EX)(Y-EY)&lt;0；</p><p>在区域 III 中，满足 X&lt;EX，Y&lt;EY，则有 (X-EX)(Y-EY)&gt;0；</p><p>在区域 IV 中，满足 X&gt;EX，Y&lt;EY，则有 (X-EX)(Y-EY)&lt;0。</p><p>显然，在区域 I、III 中，(X-EX)(Y-EY)&gt;0；在区域 II、IV 中，(X-EX)(Y-EY)&lt;0。而 X 和 Y 正相关时，数据大部分是分布在 I、III 区域内，只有少部分分布在 II、IV 区域。因此，从平均角度来看，正相关满足：</p><p>$E(X-EX)(Y-EY)&gt;0$</p><p>上式表示的是 (X-EX)(Y-EY) 的期望大于零，即 (X-EX)(Y-EY) 的平均值大于零。</p><p>然后，再来看 X 和 Y 负相关的情况：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-85f9087cf9fbcd63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上图中，X 和 Y 大部分分布在 II、IV 区域内，只有少部分分布在 I、III 区域内。</p><p>同样，在区域 I、III 中，(X-EX)(Y-EY)&gt;0；在区域 II、IV 中，(X-EX)(Y-EY)&lt;0。而 X 和 Y 负相关时，数据大部分是分布在 II、IV 区域内，只有少部分分布在 I、III 区域。因此，从平均角度来看，负相关满足：</p><p>$E(X-EX)(Y-EY)&lt;0$</p><p>上式表示的是 (X-EX)(Y-EY) 的期望小于零，即 (X-EX)(Y-EY) 的平均值小于零。</p><p>最后，再来看 X 和 Y 不相关的情况：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-783fd5ff0bdbf5d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上图中，X 和 Y 在 I、II、III、IV 区域内近似均匀分布。</p><p>同样，在区域 I、III 中，(X-EX)(Y-EY)&gt;0；在区域 II、IV 中，(X-EX)(Y-EY)&lt;0。而 X 和 Y 不相关时，数据在各区域内均匀分布，从平均角度来看，不相关满足：</p><p>$E(X-EX)(Y-EY)=0$</p><p>上式表示的是 (X-EX)(Y-EY) 的期望等于零，即 (X-EX)(Y-EY) 的平均值等于零。</p><p>综上所述，我们得到以下结论：</p><ul><li>当 X 和 Y 正相关时：$E(X-EX)(Y-EY)&gt;0$</li><li>当 X 和 Y 负相关时： $E(X-EX)(Y-EY)&lt;0$</li><li>当 X 和 Y 不相关时： $E(X-EX)(Y-EY)=0$</li></ul><p>因此，我们就引出了<strong>协方差</strong>的概念，它是表示 X 和 Y 之间相互关系的数字特征。我们定义协方差为： </p><p>$Cov(X,Y)=E(X-EX)(Y-EY)$</p><p>根据之前讨论的结果，</p><ul><li><strong>当 Cov(X,Y) &gt; 0 时，X 与 Y 正相关；</strong></li><li><strong>当 Cov(X,Y) &lt; 0 时，X 与 Y 负相关；</strong></li><li><strong>当 Cov(X,Y) = 0 时，X 与 Y 不相关。</strong></li></ul><p>值得一提的是，<strong>E</strong> 代表求期望值。也可以用平均值来计算协方差：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e5737a0988684364.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这里，<strong>之所以除以 N-1 而不是 N 的原因是对总体样本期望的无偏估计</strong>(抽取的部分样本不能代表整体的情况，抽样越多越接近真实情况  ，在多次重复下，它们的平均数接近所估计的参数真值。 )。顺便提一下，如果令 Y = X，则协方差表示的正是 X 的方差。</p><p>下面，我们根据协方差的公式，分别计算上面三种情况下 X 与 Y 的协方差。</p><p>X 与 Y 正相关时，Cov(X,Y) = 37.3684；</p><p>X 与 Y 负相关时，Cov(X,Y) = -34.0789；</p><p>X 与 Y 不相关时，Cov(X,Y) = -1.0263。</p><p><strong>2、相关系数与协方差有什么关系？</strong></p><p>我们已经知道了什么是协方差以及协方差公式是怎么来的，如果知道两个变量 X 与 Y 的协方差与零的关系，我们就能推断出 X 与 Y 是正相关、负相关还是不相关。那么有一个问题：协方差数值大小是否代表了相关程度呢？也就是说如果协方差为 100 是否一定比协方差为 10 的正相关性强呢？</p><p>请看下面这个例子！</p><p>变量 X1 与 Y1 分别为：</p><p>X1 = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30]</p><p>Y1 = [12 12 13 15 16 16 17 19 21 22 22 23 23 26 25 28 29 29 31 32]</p><p>变量 X2 和 Y2 分别为：</p><p>X2 = [110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300]</p><p>Y2 = [113 172 202 206 180 184 242 180 256 209 288 255 240 278 319 322 345 289 333 372]</p><p>X1、Y1 和 X2、Y2 分别联合分布图，如下所示：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-20451d31c197597e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>显然，从图中可以看出，X1、Y1 和 X2、Y2 都呈正相关，而且 X1 与 Y1 正相关的程度明显比 X2 与 Y2 更大一些。接下来，我们计算两幅图的协方差看看是不是这样。</p><p>Cov(X1,Y1) = 37.5526</p><p>Cov(X2,Y2) = 3730.26</p><p>意外！X2 与 Y2 的协方差竟然比 X1 与 Y1 的协方差还大 100 倍。看来并不是协方差越大，正相关程度越高。这到底是为什么呢？</p><p>其实，出现这种情况的原因是<strong>两种情况数值变化的幅值不同（或者量纲不同）</strong>。计算协方差的时候我们并没有把不同变量幅值差异性考虑进来，在比较协方差的时候也就<strong>没有一个统一的量纲标准</strong>。</p><p>所以，为了消除这一影响，为了准确得到变量之间的相似程度，我们需要把协方差除以各自变量的标准差。这样就得到了<strong>相关系数</strong>的表达式：</p><p> $\rho=\frac{Cov(X,Y)}{\sigma X\sigma Y}$</p><p>可见，相关系数就是在协方差的基础上除以变量 X 和 Y 的标准差。其中标准差的计算公式为： </p><p>$\sigma X = \sqrt{\frac{1}{N-1}\sum^{N}<em>{i=1}(X</em>{i}-\overline{X_{i}})}$</p><p>$\sigma Y = \sqrt{\frac{1}{N-1}\sum^{N}<em>{i=1}(Y</em>{i}-\overline{Y_{i}})}$</p><p>​    为什么除以各自变量的标准差就能消除幅值影响呢？这是因为<strong>标准差本身反映了变量的幅值变化程度</strong>，<strong>除以标准差正好能起到抵消的作用，让协方差标准化</strong>。这样，相关系数的范围就被归一化到 [-1,1] 之间了。</p><p>下面，我们就来分别计算上面这个例子中 X1、Y1 和 X2、Y2 的相关系数。</p><p>ρ(X1,Y1) = 0.9939</p><p>ρ(X2,Y2) = 0.9180</p><p>好了，我们得到 X1 与 Y1 的相关系数大于 X2 与 Y2 的相关系数。这符合实际情况。也就是说，根据相关系数，我们就能判定两个变量的相关程度，得到以下结论：</p><ul><li><strong>相关系数大于零，则表示两个变量正相关，且相关系数越大，正相关性越高；</strong></li><li><strong>相关系数小于零，则表示两个变量负相关，且相关系数越小，负相关性越高；</strong></li><li><strong>相关系数等于零，则表示两个变量不相关。</strong></li></ul><p>​    回过头来看一下协方差与相关系数的关系，其实，<strong>相关系数是协方差的标准化、归一化形式，消除了量纲、幅值变化不一的影响</strong>。实际应用中，在比较不同变量之间相关性时，使用相关系数更为科学和准确。但是协方差在机器学习的很多领域都有应用，而且非常重要！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;什么是协方差（Covariance）？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;以上是某百科的解释。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、协方差是怎么来的？&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="协方差" scheme="http://yoursite.com/tags/%E5%8D%8F%E6%96%B9%E5%B7%AE/"/>
    
      <category term="相关系数" scheme="http://yoursite.com/tags/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>DeepIOT神经网络压缩</title>
    <link href="http://yoursite.com/2018/08/24/DeepIOT%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9/"/>
    <id>http://yoursite.com/2018/08/24/DeepIOT神经网络压缩/</id>
    <published>2018-08-24T10:48:33.000Z</published>
    <updated>2018-08-27T12:01:06.884Z</updated>
    
    <content type="html"><![CDATA[<!-- DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic FrameworkShuochao YaoUniversity of Illinois Urbana ChampaignYiran ZhaoUniversity of Illinois Urbana ChampaignAston ZhangUniversity of Illinois Urbana ChampaignLu SuState University of New York at BualoTarek AbdelzaherUniversity of Illinois Urbana Champaign --><p>​    物联网在我们的日常感知场景中越来越受欢迎，有很多应用，比如智能家居/城市，在我们的日常生活中有大量的嵌入式/移动设备，下一个自然的步骤是构建能够集体学习的智能感知系统 。另一方面，DL最近在复杂的传感和识别任务上取得了很大进展 。</p><p>​    然而,挑战仍然存在,当我们部署先进的dl模型在低端很多设备参考这个数字从一篇分析先进的图像识别cnn模型,这些模型通常需要大约10 Giga-operations和100 mb的大小会导致记忆的问题,时间,能量消耗物联网设备上部署这些模型时。<br><a id="more"></a><br><img src="https://upload-images.jianshu.io/upload_images/12654931-1133a30c19324047.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    深度学习的最新进展激发了在传感应用中使用深度中立网络的动力，但是在<strong>受限的嵌入式设备</strong>上的<strong>过度的资源需求</strong>仍然是一个重要的障碍。最近探索的解决方案空间是在设备使用前，以某种方式<strong>压缩(近似或简化)深层神经网络</strong>。一种新的<strong>压缩解决方案</strong>，叫做<strong>DeepIoT</strong>，它在这个空间中做出了两个关键的贡献。</p><ul><li><p>首先，与目前针对压缩特殊性型神经网络的解决方案不同的是，DeepIoT提出了一种统一普适的方法，它<strong>压缩了所有常用的用于传感应用的深度学习结构</strong>，包括完<strong>全连接的、卷积的和递归的神经网络</strong>，<strong>以及它们的组合</strong>。</p></li><li><p>其次，与<strong>稀疏加权矩阵</strong>或<strong>假定加权矩阵内的线性结构</strong>的解决方案不同，DeepIoT通过<strong>寻找最少数量的非冗余隐藏元素（例如每层所需的滤波器和维度</strong>）<strong>将神经网络结构压缩为更小的密集矩阵</strong>，<strong>同时保留传感应用的性能相同</strong>。即不降低准确率。重要的是，它使用一种方法来<strong>获得参数冗余的全局视图</strong>，该方法被证明可以产生更好的压缩。</p></li></ul><p>由DeepIoT生成的压缩模型可以直接使用现有的深度学习库，这些库在嵌入式和移动系统上运行，无需进一步的修改。结果表明了<strong>深度神经网络在资源受限的嵌入式设备上的开发潜力</strong>。</p><p><strong>一、已有方法介绍</strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-4eb2b88e8d6697fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    <strong>1、大密度参数矩阵——&gt;稀疏矩阵</strong></p><p>​    有研究试图解决这个问题，现有的最先进的压缩算法是一种基于量级的修剪方法如上所示，该算法查看权重矩阵，并迭代地删除小于3列的实体。最终，它将把这个大而密集的矩阵转换成一个大而稀疏的矩阵。通常这种方法只能将参数的数量减少到10个。</p><ul><li><strong>基于幅值</strong></li></ul><p>1、<strong>基于幅度的fine-tuning压缩方法</strong>。</p><p>​    该方法是<strong>迭代地去除低幅度的重量连接</strong>;然而，它<strong>需要更多的资源消耗实现稀疏矩阵</strong>。</p><p>2、<strong>剪枝</strong></p><p>​    积极的修剪方法增加了不可挽回的网络损害的潜在风险。虽然有一种连接拼接的压缩算法，为一定的阈值提供了恢复的机会然而，该算法仍然侧重于权重水平而不是结构水平。</p><ul><li><strong>基于因子分解</strong></li></ul><p>  除了基于幅度的方法之外，另一系列作品着重于基于因子分解的方法，其通过<strong>利用参数中的低秩结构来降低神经网络的复杂性</strong>。 </p><p>3、利用各种<strong>矩阵分解</strong>方法和fine-tunning近似卷积运算，以减少神经网络的执行时间。 </p><p>4、应用<strong>稀疏编码</strong>和基于矩阵分解的方法分别降低全连接层和卷积层的复杂度。</p><p>​    主要原因是稀疏矩阵实现效率低。一方面，<strong>稀疏矩阵需要记录</strong>，这导致了2倍的内存消耗。另一方面，<strong>稀疏矩阵相乘很难优化</strong> 。</p><p>​    然而，基于分解的方法与基于量值的方法相比通常获得较低的压缩比，并且低等级假设可能损害最终网络性能。 </p><p>5、应用<strong>频域信息</strong>进行模型压缩。然而，需要额外的实现来加速频域表示，并且该方法不适用于具有小卷积滤波器大小的现代CNN。 </p><p>6、Hinton等人提出了一个<strong>teacher-student框架</strong>，将整个模型中的知识提炼成一个模型。然而，框架更侧重于将模型集合压缩为单一模型而不是结构压缩。</p><p><strong>二、系统介绍</strong></p><p>​    DeepIoT通过<strong>决定每层中最小元素的数量来压缩常用的深度神经网络结构以用于传感应用</strong>。 先前关于神经网络压缩的照明研究使疏密密集将参数矩阵转换为大型稀疏矩阵。 相比之下，<strong>DeepIoT最大限度地减少了每层中的元素数量，从而将参数转化为一组小密集矩阵</strong>。<strong>小密集矩阵不需要为元素索引额外存储</strong>，并且对处理进行了有效优化。</p><p>​    DeepIoT借用了广泛使用的被称为<strong>Dropout</strong>的<strong>深度学习正则化方法隐藏元素</strong>的想法。 <strong>Dropout操作给每个隐藏元素一个Dropout概率</strong>。 在<strong>dropout过程</strong>中，<strong>隐藏元素可根据其dropout概率进行修剪</strong>。 可以生成“<strong>细化</strong>”的网络结构。 但是，这些dropout概率通常设置为预定义值，例如0.5。 这种预定义的值不是最优的概率，从而导致对解空间的有效探索较少。如果我们能够<strong>获得每个隐藏元素的最优dropout概率</strong>，那么我们就有可能<strong>生成 最优的薄网络结构，在保留了传感应用的精度，同时最大限度地降低了传感系统的资源消耗</strong>。DeepIoT的一个重要目的是<strong>为神经网络中的每个隐藏元素找到最优的dropout概率</strong>。</p><p>​    注意，Dropout可以很容易地应用于所有常用的神经网络结构。</p><p>​    1、在<strong>全连接的神经网络</strong>中，神经元在每一<strong>层</strong>都被丢弃;</p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-b911647b450f5464.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>直接删除隐藏的元素/神经元，而不是权重</strong>。最后，我们将一个大的稠密权矩阵转化为一个细网络结构的小密度权矩阵。因此，我们可以绕过稀疏矩阵的表示 .</p><p><strong>一开始没有最优压缩决策，所以前期是计算梯度变化，抑制dropout概率，并逐渐压缩，而不是直接删除。</strong></p><p>​    2、在<strong>卷积神经网络</strong>中，<strong>卷积核</strong>被丢弃在每一层;<br><img src="https://upload-images.jianshu.io/upload_images/12654931-20ce37c77a5f48ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    对于CNN的每一层，都有一组卷积核来学习输入图像的特征。DeepIoT尝试删除冗余卷积核，以减少CNN层的卷积核数量。 </p><p>​    3、并且在<strong>递归神经网络</strong>中，减小每层的<strong>维度</strong>。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-0bed97057f74ccdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>对于RNN层，DeepIoT直接降低了RNN的维度，用于模型压缩。 </p><p>​    意味着<strong>DeepIoT可以应用于所有常用的神经网络结构及其组合</strong>。</p><p>​    DeepIoT的贡献在于<strong>利用新颖的压缩器神经网络将每个层的模型参数作为输入，学习参数冗余，并相应地生成Dropout概率</strong>。<strong>由于不同层之间存在参数互联，因此我们设计压缩神经网络作为递归神经网络，能够全局共享冗余信息并逐层生成dropout概率</strong>。</p><p><strong>压缩机神经网络</strong>与原神经网络<strong>协同优化</strong>，通过<strong>compressor-critic框架</strong>进行压缩，<strong>尽量减小原始传感应用程序的损失功能</strong>。actor-critic框架模拟了著名的基于增强学习的actor-critic算法的思想，以<strong>迭代的方式</strong>优化了两个网络。</p><p>评估实验。</p><p>1、首先由三个任务组成，这三个任务使嵌入式系统能够以基本模式（包括<strong>手写文本</strong>，<strong>视觉</strong>和<strong>语音</strong>）与人类进行交互，与其他类似尺寸的神经网络相比，我们生成的神经网络具有更高的准确性。</p><p>2、第二组提供了两个应用压缩神经网络来解决<strong>以人为中心的情境感知任务</strong>的例子;即资源受限情景中的<strong>人类活动识别</strong>和<strong>用户识别</strong>。</p><p>DeepIOT特性：</p><ul><li>由于压缩过程是迭代进行的，<strong>必须具有恢复能力</strong>以提高最终性能。</li><li>以前的方法主要依赖于一些特定的压缩度量，我们希望DeepIoT<strong>基于参数冗余进行压缩</strong>。</li><li>深度神经网络是一种深层结构，因此深度神经网络应该具有<strong>全局性</strong>，<strong>必须了解全局冗余度</strong>。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12654931-30483f2f5494237f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上图我们使用一个压缩3层全连接神经网络结构的例子来说明DeepIoT的整个流程。 （绿色方框表示原始神经网络的参数。橙色代表dropout操作）所示。使用DeepIoT压缩传感应用的神经网络结构的基本步骤可概括如下：</p><p>（1）   <strong>插入操作</strong>。将概率$p^{(l)}$随机置零的隐藏元素(图1中的红色方框)插入到原始神经网络的内部层中。  </p><p>（2）   <strong>构建压缩神经网络</strong>。它将图层中的<strong>原始神经网络权重矩阵$W^{l}​$ </strong>（图1中的绿框）<strong>作为输入进行压缩</strong>，<strong>在不同层之间学习和共享参数冗余，并生成最优dropout概率$p^{(l)}​$</strong>，<strong>然后送回到原始神经网络中的dropout操作</strong>。</p><p>（3）  用<strong>compressor-critic框架对压缩神经网络和原始神经网络进行</strong>迭代优化<strong>。 </strong>对压缩神经网络进行迭代优化，使其产生更好的退出概率，从而为原神经网络生成更有效的网络结构。 </p><p>​    </p><p>​    <strong><em>学习参数冗余 </em></strong>（橙色）<br><img src="https://upload-images.jianshu.io/upload_images/12654931-3a5db6cbaa29bc0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    基于参数冗余的DeepIOT。在理想的情况下，DeepIoT可以学习一个带参数的最优压缩机$\mu$，函数从神经网络中获取权重矩阵W。学习参数冗余，并生成可用于压缩的隐藏神经元的最优Dropout概率 。</p><p>​    <strong><em>参数冗余的全局视图</em></strong><br><img src="https://upload-images.jianshu.io/upload_images/12654931-59f84a197fb5b9a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    但我们如何设计这个最优压缩函数与参数冗余的全局视图。我们知道深神经网络本身就是完美函数逼近器。只要有足够的容量，它就可以逼近所有已有的连续函数。因此，我们将<strong>压缩函数设计成一种递归神经网络。</strong>它<strong>从原始神经网络中逐层提取权重矩阵，了解全局冗余信息，生成压缩dropout概率</strong>。这里我们称压缩函数、压缩器和要压缩的原始神经网络为“critic”，因为critic向压缩器提供反馈，让它知道根据“监督”得出的dropout概率是好是坏 。</p><p><strong>Compressor-Critic Framework</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-53bd25e684eb5f0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    DeepIoT的目标函数是根据compressor学习到的损失概率来学习压缩模型的期望损失。我们反复训练，这意味着当训练compressor时，我们把所有的东西都固定在critic，反之亦然 。</p><p>​    对原神经网络和压缩神经网络进行迭代优化，使压缩神经网络以<strong>软删除</strong>的方式逐渐压缩原神经网络。 </p><p>​    我们把最初的神经网络称为$F_{W}(x|z)$，我们称之为Critic。它将x作为输入，并基于二进制dropout 记为Z和模型参数w生成预测，这些参数w指的是一组权重$W=w^{(l)}$。 。我们假设$F_{W}(x|z)$是一个预先训练好的模型。$z~\mu_{\phi}(W)$表示压缩神经网络。它将Critic的权重作为输入，并根据其自身参数生成mask向量Z的概率分布。为了优化压缩器，使其在Critic中去掉隐藏元素，DeepIoT遵循了上图的目标函数。</p><p>​    其中$l$是Critic的目标函数。目标函数可以解释为原始神经网络的期望损失乘以压缩机产生的dropout概率。</p><p>​    DeeploT以迭代的方式优化Compressor和Critic。通过对Compressor采用梯度下降法，迭代求解，降低了目标函数中定义的期望损失。如何去做？由于存在<strong>离散采样操作</strong>，在计算图中，反向传播不直接适用。因此，我们使用<strong>无偏似然比估计量</strong>来计算。</p>]]></content>
    
    <summary type="html">
    
      &lt;!-- DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework

Shuochao Yao
University of Illinois Urbana Champaign
Yiran Zhao
University of Illinois Urbana Champaign
Aston Zhang
University of Illinois Urbana Champaign
Lu Su
State University of New York at Bualo
Tarek Abdelzaher
University of Illinois Urbana Champaign
 --&gt;
&lt;p&gt;​    物联网在我们的日常感知场景中越来越受欢迎，有很多应用，比如智能家居/城市，在我们的日常生活中有大量的嵌入式/移动设备，下一个自然的步骤是构建能够集体学习的智能感知系统 。另一方面，DL最近在复杂的传感和识别任务上取得了很大进展 。&lt;/p&gt;
&lt;p&gt;​    然而,挑战仍然存在,当我们部署先进的dl模型在低端很多设备参考这个数字从一篇分析先进的图像识别cnn模型,这些模型通常需要大约10 Giga-operations和100 mb的大小会导致记忆的问题,时间,能量消耗物联网设备上部署这些模型时。&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经网络压缩" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>HAN</title>
    <link href="http://yoursite.com/2018/08/24/HAN/"/>
    <id>http://yoursite.com/2018/08/24/HAN/</id>
    <published>2018-08-24T08:03:04.000Z</published>
    <updated>2018-08-24T09:33:30.288Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文：</strong>Hierarchical Attention Networks for Document Classification</p><p><strong>发表会议：*</strong>NAACL2016*</p><p><strong>前言：</strong>本文针对文本分类任务提出了一个层次化attention机制模型(<strong>HAN</strong>)，有两个显著的特点：(1)采用“词-句子-文章”的层次化结构来表示一篇文本。(2)该模型有两个层次的attention机制，分别存在于词层次(word level)和句子层次(sentence level)。从而使该模型具有对文本中重要性不同的句子和词的能力给予不同“注意力”的能力。作者在6个数据集合上进行了测试，并且相较以往的模型效果提升显著。最后，通过可视化说明该模型可以选择出含有丰富信息的词语和句子。 </p><a id="more"></a><p><strong>一 、写作动机</strong></p><p>​    文本分类是一项基础的NLP任务，在主题分类，情感分析，垃圾邮件检测等应用上有广泛地应用。其目标是给每篇文本分配一个类别标签。本文中模型的直觉是，不同的词和句子对文本信息的表达有不同的影响，词和句子的重要性是严重依赖于上下文的，即使是相同的词和句子，在不同的上下文中重要性也不一样。就像人在阅读一篇文本时，对文本不同的内容是有着不同的注意度的。而本文在attention机制的基础上，联想到文本是一个层次化的结构，提出用词向量来表示句子向量，再由句子向量表示文档向量，并且在词层次和句子层次分别引入attention操作的模型。</p><p>​    分层构建只不过加上了两个Attention层，用于分别对句子和文档中的单词、句子的重要性进行建模。其主要思想是，首先考虑文档的分层结构：单词构成句子，句子构成文档，所以建模时也分这两部分进行。其次，不同的单词和句子具有不同的信息量，不能单纯的统一对待所以引入Attention机制。而且引入Attention机制除了提高模型的精确度之外还可以进行单词、句子重要性的分析和可视化，让我们对文本分类的内部有一定了解。模型主要可以分为下面四个部分，如下图所示：</p><ol><li>a word sequence encoder,</li><li>a word-level attention layer,</li><li>a sentence encoder</li><li>a sentence-level attention layer.<br><img src="https://upload-images.jianshu.io/upload_images/12654931-4eff2296d8389ddf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></li></ol><p>这里对Attention机制进行一个补充介绍，可以参考论文“FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG -TERM MEMORY PROBLEMS”里面的插图：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-0feab3f9a475bb34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong>1. 基于GRU的词序列编码器</strong></p><p>GRU是RNN的一个变种，使用门机制来记录序列当前的状态。隐藏层有两个门(gate)，重置门(reset gate)rt和更新门(update gate)!zt。这两个门一起来控制当前状态有多少信息要更新。在时刻t，隐藏层状态的计算公式：</p><p> $h_{t} =(1-z_{t})\odot h_{t-1} +z_{t}\odot  \tilde{h_t }$</p><p>更新门(update gate)zt是用来决定有多少过去的信息被保留，以及有多少新信息被加进来：</p><p> $z_{t} =\sigma (W_{z}x_{t} +U_{z}h_{t-1} +b_{z}  )$</p><p>这里xt是在时刻t输入的单词的词向量，候选状态ht~的计算方法和普通的RNN相似： </p><p>$\tilde{h_{t} }=tanh(W_{h}x_{t}+r_{t}\odot (U_{h}h_{t-1}  ) +b_{h}   ) $</p><p>重置门rt决定有多少过去的信息作用于候选状态，如果rt是0，即忘记之前的所有状态： </p><p>$r_{t}=\sigma(W_{r}x_{t}+U_{r}h_{t-1}+b_{r} )$</p><p><strong>2 层次化attention</strong></p><p><strong>2.1 词编码器（Word Encoder）</strong></p><p>由词序列组成$w_{it}$,$t\in [0,T]$组成的句子,首先把词转化成词向量，$x_{it}=W_{e}w_{it}$,然后用双向的GRU网络,可以将正向和反向的上下文信息结合起来，获得隐藏层输出。</p><p>$x_{it}=W_{e}w_{it},t\in[1,T]$</p><p> $\overrightarrow{h_{it}}=\overrightarrow{GRU}({x_{it}})$</p><p>$\overleftarrow{h_{it}}=\overleftarrow{GRU}({x_{it}})$</p><p>对于一个给定的词语$w_{it}$,经过GRU网络后，我们获得了一种新的表示：$h_{it}=[\overrightarrow{h_{it}},\overleftarrow{h_{it}}]$</p><p>$h_{it}$包含了$w_{it}$周围两个方向的信息。</p><p><strong>2.2 词级别的attention机制</strong></p><p>attention机制的目的是要把一个句子中，对句子的含义最重要，贡献最大的词语找出来。我们通过将$h_{it}$输入到一个单层的感知机(MLP)中得到的结果$u_{it}$作为$h_{it}$的隐含表示。</p><p> $u_{it}=tanh(W_{w}h_{it}+b_{w})$</p><p>为了衡量单词的重要性,我们用$u_{it}$和一个随机初始化的上下文向量$u_{w}$的相似度来表示，然后经过softmax操作获得了一个归一化的attention权重矩阵$\alpha _{it}$，代表句子i中第t个词的权重。</p><p> $\alpha_{i}=\frac{exp\left( u_{i}^\top u_{s} \right) }{\sum_{t}^{}{exp\left( u_{t}^\top u_{s} \right) } } $</p><p>有了attention权重矩阵以后，我们可以将句子向量$s_{i}$看作组成这些句子的词向量的加权求和。这里的上下文向量$u_{w}$是在训练网络的过程中学习获得的。我们可以把$u_{w}$当作一种询问的高级表示，比如“哪些词含有比较重要的信息？”</p><p>$s_{i}=\sum_{t}{\alpha_{it}h_{it}} $</p><p><strong>2.3 语句编码器(Sentence Encoder)</strong></p><p>得到了句子向量表示$s_{i}$以后，我们用类似的办法获得文档向量：</p><p>  $\overrightarrow{h_{i}}=\overrightarrow{GRU}(s_{i}),i\in[1,L]$</p><p> $\overleftarrow{h_{i}}=\overleftarrow{GRU}(s_{i}),i\in[L,1]$</p><p>对于给定的句子$s_{i}$我们得到了相应的句子表示$h_{i}=[\overrightarrow{h_{i}},\overleftarrow{h_{i}}]$。这样获得的表示可以包含两个方向的上下文信息。</p><p><strong>2.4 句子级别的attention</strong></p><p>和词级别的attention类似，我们也提出了一个句子级别的上下文向量$u_{s}$,来衡量一个句子在整篇文本的重要性。</p><p>$u_{i}=tanh(W_{s}h_{i}+b_{s})$</p><p> $\alpha_{i}=\frac{exp\left( u_{i}^\top u_{s} \right) }{\sum_{t}^{}{exp\left( u_{t}^\top u_{s} \right) } } $</p><p> $v=\sum_{i}{\alpha_{i}h_{i}}$</p><p>我们获得了整篇文章的向量表示v,最后可以使用全链接的softmax层进行分类。 </p><p><strong>总结</strong></p><p>​    本文提出了一种基于层次化attention的文本分类模型，可以利用attention机制识别出一句话中比较重要的词语，利用重要的词语形成句子的表示，同样识别出重要的句子，利用重要句子表示来形成整篇文本的表示。实验证明，该模型确实比基准模型获得了更好的效果，可视化分析也表明，该模型能很好地识别出重要的句子和词语。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文：&lt;/strong&gt;Hierarchical Attention Networks for Document Classification&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;发表会议：*&lt;/strong&gt;NAACL2016*&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;前言：&lt;/strong&gt;本文针对文本分类任务提出了一个层次化attention机制模型(&lt;strong&gt;HAN&lt;/strong&gt;)，有两个显著的特点：(1)采用“词-句子-文章”的层次化结构来表示一篇文本。(2)该模型有两个层次的attention机制，分别存在于词层次(word level)和句子层次(sentence level)。从而使该模型具有对文本中重要性不同的句子和词的能力给予不同“注意力”的能力。作者在6个数据集合上进行了测试，并且相较以往的模型效果提升显著。最后，通过可视化说明该模型可以选择出含有丰富信息的词语和句子。 &lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Attention" scheme="http://yoursite.com/tags/Attention/"/>
    
      <category term="Hierarchical Attention Networks" scheme="http://yoursite.com/tags/Hierarchical-Attention-Networks/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中Attention注意力机制</title>
    <link href="http://yoursite.com/2018/08/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADAttention%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>http://yoursite.com/2018/08/24/深度学习中Attention注意力机制/</id>
    <published>2018-08-24T04:55:45.000Z</published>
    <updated>2018-08-24T07:42:32.197Z</updated>
    
    <content type="html"><![CDATA[<p>​    最近两年，注意力模型（Attention Model）被广泛使用在<strong>自然语言处理</strong>、<strong>图像识别</strong>及<strong>语音识别</strong>等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。</p><p>​    本文以机器翻译为例，深入浅出地介绍了深度学习中注意力机制的原理及关键计算机制，同时也抽象出其本质思想，并介绍了注意力模型在图像及语音等领域的典型应用场景。</p><p>​    注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。</p><a id="more"></a><p><strong>人类的视觉注意力</strong></p><p>​    从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-fb4da1bea0bf6c0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图1 人类的视觉注意力 </p><p>​    视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p><p>​    这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。</p><p>​    图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。</p><p>​    深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p><p><strong>Encoder-Decoder框架</strong></p><p>​    要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。</p><p>​    Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-673d1e3c89384f28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图2 抽象的文本处理领域的Encoder-Decoder框架</p><p>​    文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：</p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-b81ce17f917e41ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： </p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-18ac98f85ce18c51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息y1,y2,……,y i-1!来生成i时刻要生成的单词yi：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-07e7143d8889b113.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。</p><p>​    Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。</p><p><strong>Attention模型</strong></p><p>​    本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。</p><p><strong>Soft Attention模型</strong></p><p>​    图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-4a14bb2ebaae352b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p><p>​    而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。</p><p>​    如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p><p>​    在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。</p><p>​    没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p><p>​    上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）(Chase,0.2) (Jerry,0.5)</p><p>​    每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。</p><p>​    同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-87bc6ecb6c37d755.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图3 引入注意力模型的Encoder-Decoder框架</p><p>即生成目标句子单词的过程成了下面的形式：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-758accd42659461c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下： </p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-22d97a1a2eb5c5fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入Xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式： </p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-07cda8310b043204.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设Ci下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-b7b36d1a95dd63e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？</p><p>​    为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-9dad02d0e28ad036.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图5 RNN作为具体模型的Encoder-Decoder框架</p><p>那么用图6可以较为便捷地说明注意力分配概率分布值的通用计算过程。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-443c9b6cca038058.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    图6 注意力分配概率计算</p><p>​    对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yj时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p><p>​    绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-9f0d5e50c90d7165.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图7 英语-德语翻译的注意力概率分布</p><p>上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。</p><p>​    目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的</p><p>步骤，而注意力模型其实起的是相同的作用。</p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-181c0ac7d346c4d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图8 Google 神经网络机器翻译系统结构图</p><p>​    图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><p><strong>Attention机制的本质思想</strong></p><p>​    如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-59f1d669793d5f88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图9 Attention机制的本质思想</p><p>​    我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-665dfe15e5428ea0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>​    当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>​    从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>​    至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-a4b0e1012e5bd916.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图10 三阶段计算Attention过程</p><p>​    在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Keyi，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-c93d879ffaca8c95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e0e5d2da5ca0ec70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    第二阶段的计算结果!ai即为Valuei对应的权重系数，然后进行加权求和即可得到Attention数值：<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e99ce9f941c56f5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。 </p><p><strong>Self Attention模型</strong></p><p>​    通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p><p>​    在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>​    如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-a3ec94c5bab0da8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图11 可视化Self Attention实例<br><img src="https://upload-images.jianshu.io/upload_images/12654931-c06b7b882ea54e69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图12 可视化Self Attention实例</p><p>​    从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</p><p>​    很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p><p>​    但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p><p><strong>Attention机制的应用</strong></p><p>​    前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以<strong>自然语言处理</strong>中的<strong>机器翻译</strong>任务作为例子，下面分别再从<strong>图像处理</strong>领域和<strong>语音识别</strong>选择典型应用实例来对其应用做简单说明。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f85fefd11ac30319.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图13 图片-描述任务的Encoder-Decoder框架</p><p><strong>图片描述（Image-Caption）</strong>是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考图13）。</p><p>​    此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f037cc39c3734b55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图14 图片生成句子中每个单词时的注意力聚焦区域</p><p>​    图15给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-171a7bb3b3837e09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图15 图像描述任务中Attention机制的聚焦作用<br><img src="https://upload-images.jianshu.io/upload_images/12654931-f7f34eccc3a8c2b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>图16 语音识别中音频序列和输出字符之间的Attention</p><p>​    语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。</p><p>​    图16可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。</p><p>​    上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。    </p><p><strong>目前主流attention主流方法</strong></p><ol><li><p>首先是Object Recognition。</p></li><li><p>然后是Image Caption。</p></li><li><p>NLP中的Machine Translation.</p></li><li><p>不使用RNN结构。在特征图上生成空间分布的权重，然后再对特征图加权求和，试图让网络学出来对不同物体区域加以不同关注度。</p></li><li><p>总结与泛化。划重点：attention机制听起来高达上，其实就是学出一个权重分布，再拿这个权重分布施加在原来的特征之上，就可以叫做attention。</p><p>简单来说：</p><p>（1）这个加权可以是保留所有分量均做加权（即soft attention）；也可以是在分布中以某种采样策略选取部分分量（即hard attention）。</p><p>（2）这个加权可以作用在原图上，也就是RAM和DRAM；也可以作用在特征图上，如后续的好多文章（例如image caption)。</p><p>（3）这个加权可以作用在空间尺度上，给不同空间区域加权；也可以作用在channel尺度上，给不同通道特征加权；甚至特征图上每个元素加权。</p><p>（4）这个加权还可以作用在不同时刻历史特征上，如Machine Translation，以及我前段时间做的视频相关的工作。</p><p>所以说，Attention是啥啊？就是一个权重分布！</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;​    最近两年，注意力模型（Attention Model）被广泛使用在&lt;strong&gt;自然语言处理&lt;/strong&gt;、&lt;strong&gt;图像识别&lt;/strong&gt;及&lt;strong&gt;语音识别&lt;/strong&gt;等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。&lt;/p&gt;
&lt;p&gt;​    本文以机器翻译为例，深入浅出地介绍了深度学习中注意力机制的原理及关键计算机制，同时也抽象出其本质思想，并介绍了注意力模型在图像及语音等领域的典型应用场景。&lt;/p&gt;
&lt;p&gt;​    注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Attention" scheme="http://yoursite.com/tags/Attention/"/>
    
      <category term="注意力机制" scheme="http://yoursite.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>LSTM原理与实现</title>
    <link href="http://yoursite.com/2018/08/24/LSTM%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/08/24/LSTM原理与实现/</id>
    <published>2018-08-24T02:54:25.000Z</published>
    <updated>2018-08-24T04:56:23.188Z</updated>
    
    <content type="html"><![CDATA[<ul><li>[RNN]</li><li>[LSTM网络]</li><li>[LSTM核心思想]</li><li>[逐步理解LSTM]<ul><li>[遗忘门]</li><li>[输入门]</li><li>[输出门]</li></ul></li><li>[LSTM变体]</li><li>[多层LSTM]</li><li>LSTM实现手写数字<ul><li>[设置LSTM参数]</li><li>[初始化权值参数]</li></ul></li></ul><a id="more"></a> <h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>​    人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。<br>    传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。<br>    RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-db35984a77cb0f57.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>RNN 包含循环</em></p><p>​    在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。<br>    这些循环使得 RNN 看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：</p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-5afb587b1e3aaf95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>展开的 RNN</em></p><p>​    链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。<br>    并且 RNN 也已经被人们应用了！在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。建议参考 Andrej Karpathy 的博客文章——<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 来看看更丰富有趣的 RNN 的成功应用。<br>    而这些成功应用的关键之处就是 LSTM 的使用，这是一种特别的 RNN，比标准的 RNN 在很多的任务上都表现得更好。几乎所有的令人振奋的关于 RNN 的结果都是通过 LSTM 达到的。</p><h2 id="长期依赖（Long-Term-Dependencies）问题"><a href="#长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="长期依赖（Long-Term Dependencies）问题"></a>长期依赖（Long-Term Dependencies）问题</h2><p>​    RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。<br>    有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-bffa58cf1e08bf41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​                                不太长的相关信息和位置间隔</p><p>​    但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。<br>    不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-7cf6762040d5918c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>相当长的相关信息和位置间隔*</p><p>​    在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。<a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Bengio, et al. (1994)</a>等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。<br>    然而，幸运的是，LSTM 并没有这个问题！</p><h2 id="LSTM-网络"><a href="#LSTM-网络" class="headerlink" title="LSTM 网络"></a>LSTM 网络</h2><p>​    Long Short Term Memory 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener">Hochreiter &amp; Schmidhuber (1997)</a>提出，并在近期被<a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en" target="_blank" rel="noopener">Alex Graves</a>进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。<br>    LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！<br>    所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-13bc1a955ebc6668.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>标准 RNN 中的重复模块包含单一的层</em></p><p>​    LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-421311f51e09bc2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>LSTM 中的重复模块包含四个交互的层</em></p><p>​    不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-a7fd4db3a625cbf2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>LSTM 中的图标</em></p><p>​    在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p><h2 id="LSTM-的核心思想"><a href="#LSTM-的核心思想" class="headerlink" title="LSTM 的核心思想"></a>LSTM 的核心思想</h2><p>​    LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。<br>    细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-cac54fa82daf1acd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作(向量或者矩阵的pointwise 乘法指按元素相乘,点乘（不改变向量长度、大小）</p><p>a = [a1 a2 a3]</p><p>b = [b1 b2 b3]</p><p>a和b的pointwise product</p><p>a • b = a1<strong><em>b1+a2\</em></strong>b2+a3*b3 )。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-faccc7bf241aa8b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p><p>​    LSTM 拥有三个门，来保护和控制细胞状态。这三个门分别输入门、遗忘门和输出门。 LSTM通过三个这样的本结构来实现信息的保护和控制。</p><h2 id="逐步理解LSTM"><a href="#逐步理解LSTM" class="headerlink" title="逐步理解LSTM"></a>逐步理解LSTM</h2><p>现在我们就开始通过三个门逐步的了解LSTM的原理</p><h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p>​    在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为<strong>忘记门层</strong>完成。该门会读取<code>h_{t-1}</code>和<code>x_t</code>，输出一个在 0 到 1 之间的数值给每个在细胞状态<code>C_{t-1}</code>中的数字。1 表示“完全保留”，0 表示“完全舍弃”。 </p><p>​    让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前<strong>主语</strong>的性别，因此正确的<strong>代词</strong>可以被选择出来。当我们看到新的<strong>主语</strong>，我们希望忘记旧的<strong>主语</strong>。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-06ffb4507903c13d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>决定丢弃信息</em> </p><p>其中ht−1表示的是上一个cell的输出，xt表示的是当前细胞的输入。σ表示sigmod函数。</p><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><p>​    下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，C^t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。 在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。 </p><p><img src="https://upload-images.jianshu.io/upload_images/12654931-0bed48af76b88e10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>确定更新的信息</em> </p><p>​    现在是更新旧细胞状态的时间了，Ct−1更新为Ct。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。</p><p>​    我们把旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上it∗C~t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。</p><p>​    在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-e18424a8c9c39b35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>更新细胞状态</em> </p><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p>​    最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p><p>​    在语言模型的例子中，因为他就看到了一个 <strong>代词</strong>，可能需要输出与一个 <strong>动词 </strong>相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-d5f6c68ccc9245c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><em>输出信息</em> </p><h2 id="LSTM-的变体"><a href="#LSTM-的变体" class="headerlink" title="LSTM 的变体"></a>LSTM 的变体</h2><p>​    我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。<br>    其中一个流形的 LSTM 变体，就是由 <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="noopener">Gers &amp; Schmidhuber (2000)</a> 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-7e38dcfe1cc7b575.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong><em>peephole 连接</em></strong></p><p>​    上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。</p><p>​    另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-6c6982add34f8466.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><strong><em>coupled 忘记门和输入门</em></strong></p><p>​    另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">Cho, et al. (2014)</a> 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。<br><img src="https://upload-images.jianshu.io/upload_images/12654931-050d4d229257b73f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>​    其中， rt表示重置门，zt表示更新门。重置门决定是否将之前的状态忘记。(作用相当于合并了 LSTM 中的遗忘门和传入门）当rt趋于0的时候，前一个时刻的状态信息ht−1会被忘掉，隐藏状态h^t会被重置为当前输入的信息。更新门决定是否要将隐藏状态更新为新的状态h^t（作用相当于 LSTM 中的输出门） 。</p><p>​    和 LSTM 比较一下：<br>- GRU 少一个门，同时少了细胞状态Ct。<br>- 在 LSTM 中，通过遗忘门和传入门控制信息的保留和传入；GRU 则通过重置门来控制是否要保留原来隐藏状态的信息，但是不再限制当前信息的传入。<br>- 在 LSTM 中，虽然得到了新的细胞状态 Ct，但是还不能直接输出，而是需要经过一个过滤的处理:ht=ot∗tanh(Ct)；同样，在 GRU 中, 虽然我们也得到了新的隐藏状态h^t， 但是还不能直接输出，而是通过更新门来控制最后的输出：ht=(1−zt)∗ht−1+zt∗h^t。</p><h2 id="多层LSTM"><a href="#多层LSTM" class="headerlink" title="多层LSTM"></a>多层LSTM</h2><p><strong>多层LSTM是将LSTM进行叠加，其优点是能够在高层更抽象的表达特征，并且减少神经元的个数，增加识别准确率并且降低训练时间。</strong></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>​    LSTM 是我们在 RNN 中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是<strong>注意力</strong>！” 这个想法是让 RNN 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="noopener">Xu, et al.(2015)</a>已经这么做了——如果你希望深入探索<strong>注意力</strong>可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索……<br>    注意力也不是 RNN 研究领域中唯一的发展方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="noopener">Kalchbrenner, et al. (2015)</a> 提出的 Grid LSTM 看起来也是很有前途。使用生成模型的 RNN，诸如<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="noopener">Gregor, et al. (2015)</a> <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="noopener">Chung, et al. (2015)</a> 和 <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="noopener">Bayer &amp; Osendorfer (2015)</a> 提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！</p><h2 id="LSTM实现手写数字"><a href="#LSTM实现手写数字" class="headerlink" title="LSTM实现手写数字"></a>LSTM实现手写数字</h2><p>这里我们利用的数据集是tensorflow提供的一个手写数字数据集。该数据集是一个包含n张28*28的数据集。</p><h3 id="设置LSTM参数"><a href="#设置LSTM参数" class="headerlink" title="设置LSTM参数"></a>设置LSTM参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># configuration</span></span><br><span class="line"><span class="comment">#                        O * W + b -&gt; 10 labels for each image, O[? 28], W[28 10], B[10]</span></span><br><span class="line"><span class="comment">#                       ^ (O: output 28 vec from 28 vec input)</span></span><br><span class="line"><span class="comment">#                       |</span></span><br><span class="line"><span class="comment">#      +-+  +-+       +--+</span></span><br><span class="line"><span class="comment">#      |1|-&gt;|2|-&gt; ... |28| time_step_size = 28</span></span><br><span class="line"><span class="comment">#      +-+  +-+       +--+</span></span><br><span class="line"><span class="comment">#       ^    ^    ...  ^</span></span><br><span class="line"><span class="comment">#       |    |         |</span></span><br><span class="line"><span class="comment"># img1:[28] [28]  ... [28]</span></span><br><span class="line"><span class="comment"># img2:[28] [28]  ... [28]</span></span><br><span class="line"><span class="comment"># img3:[28] [28]  ... [28]</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># img128 or img256 (batch_size or test_size 256)</span></span><br><span class="line"><span class="comment">#      each input size = input_vec_size=lstm_size=28</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># configuration variables</span></span><br><span class="line">input_vec_size = lstm_size = <span class="number">28</span> <span class="comment"># 输入向量的维度</span></span><br><span class="line">time_step_size = <span class="number">28</span> <span class="comment"># 循环层长度</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">test_size = <span class="number">256</span></span><br></pre></td></tr></table></figure><p>这里设置将batch_size设置为128，time_step_size表示的是lstm神经元的个数，这里设置为28个，input_vec_size表示一次输入的像素数。 </p><h3 id="初始化权值参数"><a href="#初始化权值参数" class="headerlink" title="初始化权值参数"></a>初始化权值参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.random_normal(shape, stddev=<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, W, B, lstm_size)</span>:</span></span><br><span class="line">    <span class="comment"># X, input shape: (batch_size, time_step_size, input_vec_size)</span></span><br><span class="line">    <span class="comment"># XT shape: (time_step_size, batch_size, input_vec_size)</span></span><br><span class="line">    <span class="comment">#对这一步操作还不是太理解，为什么需要将第一行和第二行置换</span></span><br><span class="line">    XT = tf.transpose(X, [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])  <span class="comment"># permute time_step_size and batch_size,[28, 128, 28]</span></span><br><span class="line">    <span class="comment"># XR shape: (time_step_size * batch_size, input_vec_size)</span></span><br><span class="line">    XR = tf.reshape(XT, [<span class="number">-1</span>, lstm_size]) <span class="comment"># each row has input for each lstm cell (lstm_size=input_vec_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Each array shape: (batch_size, input_vec_size)</span></span><br><span class="line">    X_split = tf.split(XR, time_step_size, <span class="number">0</span>) <span class="comment"># split them to time_step_size (28 arrays),shape = [(128, 28),(128, 28)...]</span></span><br><span class="line">    <span class="comment"># Make lstm with lstm_size (each input vector size). num_units=lstm_size; forget_bias=1.0</span></span><br><span class="line">    lstm = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1.0</span>, state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get lstm cell output, time_step_size (28) arrays with lstm_size output: (batch_size, lstm_size)</span></span><br><span class="line">    <span class="comment"># rnn..static_rnn()的输出对应于每一个timestep，如果只关心最后一步的输出，取outputs[-1]即可</span></span><br><span class="line">    outputs, _states = rnn.static_rnn(lstm, X_split, dtype=tf.float32)  <span class="comment"># 时间序列上每个Cell的输出:[... shape=(128, 28)..]</span></span><br><span class="line">    <span class="comment"># tanh activation</span></span><br><span class="line">    <span class="comment"># Get the last output</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(outputs[<span class="number">-1</span>], W) + B, lstm.state_size <span class="comment"># State size to initialize the state</span></span><br></pre></td></tr></table></figure><p>​    init_weigths函数利用正态分布随机生成参数的初始值，model的四个参数分别为：X为输入的数据，W表示的是28<em>10的权值(标签为0-9)，B表示的是偏置，维度和W一样。这里首先将一批128</em>（28*28）的图片放进神经网络。然后进行相关的操作(注释已经写得很明白了，这里就不再赘述)，然后利用WX+B求出预测结果，同时返回lstm的尺寸</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">py_x, state_size = model(X, W, B, lstm_size)</span><br><span class="line"></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))</span><br><span class="line">train_op = tf.train.RMSPropOptimizer(<span class="number">0.001</span>, <span class="number">0.9</span>).minimize(cost)</span><br></pre></td></tr></table></figure><p>然后通过交叉熵计算误差，反复训练得到最优值。 </p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;[RNN]&lt;/li&gt;
&lt;li&gt;[LSTM网络]&lt;/li&gt;
&lt;li&gt;[LSTM核心思想]&lt;/li&gt;
&lt;li&gt;[逐步理解LSTM]&lt;ul&gt;
&lt;li&gt;[遗忘门]&lt;/li&gt;
&lt;li&gt;[输入门]&lt;/li&gt;
&lt;li&gt;[输出门]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;[LSTM变体]&lt;/li&gt;
&lt;li&gt;[多层LSTM]&lt;/li&gt;
&lt;li&gt;LSTM实现手写数字&lt;ul&gt;
&lt;li&gt;[设置LSTM参数]&lt;/li&gt;
&lt;li&gt;[初始化权值参数]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
      <category term="RNN" scheme="http://yoursite.com/tags/RNN/"/>
    
  </entry>
  
</feed>
