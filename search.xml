<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pytorch0.4.0升级概述新手教程]]></title>
    <url>%2F2018%2F06%2F20%2FPytorch0-4-0%E5%8D%87%E7%BA%A7%E6%A6%82%E8%BF%B0%E6%96%B0%E6%89%8B%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[pytorch0.4支持了Windows系统的开发，在首页即可使用pip安装pytorch和torchvision。 说白了，以下文字就是来自官方文档60分钟入门的简要翻译. pytorch是啥python的科学计算库，使得NumPy可用于GPU计算，并提供了一个深度学习平台使得灵活性和速度最大化 入门Tensors(张量) Tensors与NumPy的ndarrays类似，另外可以使用GPU加速计算 未初始化的5*3的矩阵:x = torch.empty(5, 3) 随机初始化的矩阵:x = torch.rand(5, 3) 全零矩阵,定义数据类型:x = torch.zeros(5, 3, dtype=torch.long) 由数据构造矩阵:x = torch.tensor([5.5, 3]) 由已存在张量构造矩阵，性质与之前张量一致: 12x = x.new_ones(5, 3, dtype=torch.double) x = torch.randn_like(x, dtype=torch.float) 获取维度:print(x.size()) Operations有多种operation的格式，这里考虑加法 12y = torch.rand(5, 3)print(x + y) 1print(torch.add(x, y)) 123result = torch.empty(5, 3)torch.add(x, y, out=result)print(result) 123# adds x to yy.add_(x)print(y) operations中需要改变张量本身的值，可以在operation后加,比如`x.copy(y), x.t_()` 索引:print(x[:, 1]) 改变维度:x.view(-1, 8) 和Numpy的联系torch tensor 和 numpy array之间可以进行相互转换，他们会共享内存位置，改变一个，另一个会跟着改变。 tensor to array1234a = torch.ones(5)b = a.numpy()a.add_(1)print(a,b) array to tensor123456import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b) CUDA Tensorstensor可以使用.to方法将其移动到任何设备。 123456789# let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # a CUDA device object y = torch.ones_like(x, device=device) # directly create a tensor on GPU x = x.to(device) # or just use strings ``.to(&quot;cuda&quot;)`` z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double)) # ``.to`` can also change dtype together! Autograd(自动求导)pytorch神经网络的核心模块就是autograd，autograd模块对Tensors上的所有operations提供了自动求导。 Tensortorch.Tensor是模块中的核心类，如果设置属性.requires_grad = True,开始追踪张量上的所有节点操作，指定其是否计算梯度。使用.backward()方法进行所有梯度的自动求导，张量的梯度会累积到.grad属性中。 .detach()停止张量的追踪，从梯度计算中分离出来；另外在评估模型时一般使用代码块with torch.no_grad():,因为模型中通常训练的参数也会有.requires_grad = True,这样写可以停止全部张量的梯度更新。 Function类是autograd的变体，Tensor和Function相互交错构建成无环图，编码了完整的计算过程，每个Variable(变量)都有.grad_fn属性，引用一个已经创建了的Tensor的Function. 如上，使用.backward()计算梯度。如果张量是一个标量(只有一个元素),不需要对.backward()指定参数；如果张量不止一个元素，需要指定.backward()的参数，其匹配张量的维度。 1234567891011121314151617import torchx = torch.ones(2, 2, requires_grad=True)print(x)y = x + 2print(y)print(y.grad_fn)z = y * y * 3out = z.mean()print(z, out)a = torch.randn(2, 2)a = ((a * 3) / (a - 1))print(a.requires_grad)a.requires_grad_(True) # 改变a张量内在的属性print(a.requires_grad)b = (a * a).sum()print(b.grad_fn) Gradients反向传播时，由于out是一个标量，out.backward()等效于out.backward(torch.tensor(1)) 123456789101112131415161718192021out.backward()print(x.grad)x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000: y = y * 2print(y)gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(gradients)print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): print((x ** 2).requires_grad) 神经网络神经网络可以用torch.nn构建。nn依赖于autograd定义模型和求导，nn.Module定义网络层，方法forward(input)返回网络输出。 举例说明，如下是对数字图片分类的卷积网络架构。 这是一个简单的前馈神经网络，将输入数据依次通过几层网络层后最终得到输出。 神经网络典型的训练步骤如下： 定义神经网络及学习的参数(权重) 迭代输入数据 将输入数据输入到网络结构中 计算代价函数 误差向后传播 更新网络权重 weight = weight - learning_rate * gradient 定义网络123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) out: 1234567Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 可以仅定义forward()函数，当使用autograd时backward()被自动定义。可以在forward()函数中使用任何operation操作。 net.parameters()返回模型中的可学习参数。 123params = list(net.parameters())print(len(params))print(params[0].size()) # conv1&apos;s .weight 使所有参数的梯度归零然后开始计算梯度 12net.zero_grad()out.backward(torch.randn(1, 10)) 代价函数代价函数将(output,target)作为输入，计算output与target之间的距离。 nn模块中有几种不同的代价函数选择，最简单的是nn.MSELoss，计算均方误差 eg： 1234567output = net(input)target = torch.arange(1, 11) # a dummy target, for exampletarget = target.view(1, -1) # make it the same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target)print(loss) 按照向后传播的方向传播loss，使用grad_fn可以查看整个流程的计算图 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 使用loss.backward()，流程中所有requres_grad=True的张量累积它的梯度至.grad 123print(loss.grad_fn) # MSELossprint(loss.grad_fn.next_functions[0][0]) # Linearprint(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 向后传播loss.backward()传播误差， 123456789net.zero_grad() # zeroes the gradient buffers of all parametersprint(&apos;conv1.bias.grad before backward&apos;)print(net.conv1.bias.grad)loss.backward()print(&apos;conv1.bias.grad after backward&apos;)print(net.conv1.bias.grad) 更新权重误差每次传播后，需要对权重进行更新，简单的更新方式如下： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) torch.optim实现了这一过程，并有着不同的更新规则GD, Nesterov-SGD, Adam, RMSProp， 1234567891011import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update note: 每次迭代时由于梯度的累积，需要手动将梯度归零optimizer.zero_grad()]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
        <tag>Pytorch0.4.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[唐老师研究生毕业演讲]]></title>
    <url>%2F2018%2F06%2F20%2F%E5%94%90%E8%80%81%E5%B8%88%E7%A0%94%E7%A9%B6%E7%94%9F%E6%AF%95%E4%B8%9A%E6%BC%94%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[尊敬的何院士、各位嘉宾、各位老师、同学们： 大家下午好！]]></content>
      <categories>
        <category>毕业演讲</category>
      </categories>
      <tags>
        <tag>毕业演讲</tag>
        <tag>唐老师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10+Anaconda+tensorflow1.8+pytorch0.4.0注意事项]]></title>
    <url>%2F2018%2F06%2F12%2Fwin10-Anaconda-tensorflow1-8-pytorch0-4-0%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[注意事项 Anaconda会自带python3.6，所以为了避免环境配置乱套，删除本机独立安装的python。 不要按网上配置虚拟环境，比如TensorFlow配置一个虚拟环境，pytorch配置一个虚拟环境……keras……cafee……只要一个环境即可。 win10+Anaconda3-5.2.0+tensorflow1.8+pytorch0.4.0 在windows PowerShell中确定python环境在Anaconda中 Anaconda注意：自动添加到环境变量其它事项一路安装，普通套路。 TensorFlow1、TensorFlow或者pytorch，无论什么框架，越新越好，新的功能多bug少，这里用的TensorFlow1.8。 2、TensorFlow和pytorch最好都用conda装，但是这里conda install tensorflow只会给你比较低的版本，所以TensorFlow安装在这里使用了：1conda install --channel https://conda.anaconda.org/conda-forge tensorflow 可能会下载很慢，如果有部分下载未成功，重新执行命令即可，之前下载的会保留。 Pytorch同样遵循使用最新框架原则。我自己电脑用的pytorch-CPU版本，所以执行：12conda install conda install pytorch-cpu -c pytorchpip install torchvision 在实验室服务器用的是cuda9.0版本12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 安装成功之后效果： PyCharm注意事项在不用虚拟幻境情况下，好处就是不用切换环境配置了，直接一个 Anaconda\python.exe全部搞定，如图： 附录鄙视链：caffe-pytorch-tensorflow-caffe2- caffetorch-slim-tensorflow-keras]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Anaconda</tag>
        <tag>tensorflow</tag>
        <tag>pytorch</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch0.4.0重大更新]]></title>
    <url>%2F2018%2F06%2F11%2FPyTorch0.4.0%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[一、重大核心变化包括 Tensor/Variable 合并 零维张量 数据类型 迁移指南 二、现添加的新特征包括 Tensors： 全面支持高级索引 快速傅立叶变换 神经网络： 计算时的存储权衡 bottleneck-识别代码中热点（hotspots）的工具 torch.distributions 24 个基础的概率分布 增加cdf、方差、信息熵、困惑度等 分布式训练 易于使用的 Launcher utility NCCL2 后端 C++拓展 Windows 支持 ONNX 改进 RNN 支持 三、性能改进 四、Bug 修复 五、torchvision的一些变化1.torchvision.transform中函数torchvision.transforms.`Scale(*args, **kwargs)即将被函数torchvision.transforms.Resize`(size, interpolation=2)代替。（参考官方文档：点击打开链接） 2.torchvision.transform中函数torchvision.transforms.`RandomSizedCrop(*args, **kwargs)即将被函数torchvision.transforms.RandomResizedCrop`(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)代替。（参考官方文档：点击打开链接） 六、安装方式——Windows安装【方法一】pip直接安装。官网（点击打开链接）给出的安装步骤如下图所示（根据CUDA版本以及Python版本选择）。 Run this command: 12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 这里我是下载好torch-0.4.0-cp36-cp36m-win_amd64.whl文件到本地之后才安装的。进入Anaconda Prompt，然后进入文件所在目录： 打开cmd命令提示符，先利用anaconda创建一个虚拟环境，命名为pytorch4 1conda create -n pytorch4 python=3.6 激活刚才创建好的虚拟环境 1activate pytorch4 安装pytorch0.4.0 1pip install torch-0.4.0-cp35-cp35m-win_amd64.whl 注：根据自己的配置选择whl下载来链接 安装torchvision1pip install torchvision 简单测试安装是否成功123pythonimport torchprint(torch.__version__) 如果输出0.4.0，那么恭喜Windows下的PyTorch0.4.0安装成功！]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>PyTorch</tag>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mnist手写数字识别]]></title>
    <url>%2F2018%2F06%2F09%2FMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[minist_mpl.py1234567891011121314151617181920212223242526272829303132333435 #encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense,Activation,Dropoutfrom keras.optimizers import SGD,Adadeltafrom keras.models import save_modelimport matplotlib.pyplot as plt(x_train,y_train),(x_test,y_test) = mnist.load_data()x_train = x_train.reshape(60000,28*28).astype('float32') #转换数据格式x_test = x_test.reshape(10000,28*28).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train,10) #one-hot编码y_test = keras.utils.to_categorical(y_test,10)print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)modle = Sequential()#第一层隐层，64个神经元modle.add(Dense(256,activation='relu',input_dim=28*28))#第二层隐层，64个神经元modle.add(Dense(256,activation='relu'))modle.add(Dropout(0.5))#输出层，10个神经元modle.add(Dense(10,activation='softmax'))sgd = SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)modle.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy'])modle.fit(x_train,y_train,epochs=10,batch_size=128)score = modle.evaluate(x_test,y_test,batch_size=128)print(score)modle.save('MLP_minist.h5') 123456789101112131415161718192021222324252627282930313233343536373839#encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequential,save_modelfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2Dfrom keras.optimizers import SGD,Adadelta(x_train,y_train),(x_test,y_test) = mnist.load_data() #加载数据print(x_train.shape,x_test.shape)x_train = x_train.reshape(60000,28,28,1).astype('float32') #二维数据x_test = x_test.reshape(10000,28,28,1).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train) #one-hot编码y_test = keras.utils.to_categorical(y_test)num_classes = y_test.shape[1]model = Sequential() #创建序列模型model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1))) #第一层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Conv2D(64,(3,3),activation='relu')) #第二层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Flatten()) #铺平当前节点model.add(Dense(128,activation='relu')) #全连接层model.add(Dropout(0.5)) #随机失活model.add(Dense(num_classes,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) #编译模型model.fit(x_train,y_train,batch_size=128,epochs=10) #训练模型score = model.evaluate(x_test,y_test,batch_size=128) #评价模型print(score) #打印分类准确率model.save('CNN_minist.h5')]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Keras</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交叉验证集、测试集]]></title>
    <url>%2F2018%2F06%2F09%2F%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%2F</url>
    <content type="text"><![CDATA[什么有交叉验证：主要是因为训练集较小。无法直接像前面那样只分出训练集，验证集，测试就可以了（简单交叉验证）。 最佳的数据分类情况是把数据集分为三部分，分别为：训练集(train set)，验证集(validation set)和测试集(test set) 验证集和测试集两者的主要区别是：验证集用于进一步确定模型中的超参数(例如正则项系数、ANN{Artificial Neural Network}中隐含层的节点个数、网络层数、迭代次数、学习率)而测试集只是用于评估模型的精确度(即泛化能力)！ 举个例子：假设建立一个BP神经网络，对于隐含层的节点数目，我们并没有很好的方法去确定。一般将节点数设定为某一具体的值，通过训练集训练出相应的参数后，再由交叉验证集去检测该模型的误差；然后再改变节点数，重复上述过程，直到交叉验证误差最小。此时的节点数可以认为是最优节点数，即该节点数(这个参数)是通过交叉验证集得到的。 而测试集是在确定了所有参数之后，根据测试误差来评判这个学习模型的；也可以说是用来评估模型的泛化能力。所以，验证集主要主要是用于模型的调参。 “交叉验证法“ (cross validation)先将数据集D 划分为k 个大小相似的互斥子集， 即D = D**1 ∪ D**2 ∪**… ∪ Dk, Di ∩ Dj = ø (i≠j) . 每个子集Di 都尽可能保持数据分布的一致性，即从D 中通过分层采样得到. 然后，每次用k-1 个子集的并集作为训练集，余下的那个子集作为测试集;这样就可获得k组训练/测试集，从而可进行k 次训练和测试，最终返回的是这k 个测试结果的均值**。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为” k 折**/倍**交叉验证“ (k-fold cross validation). k 最常用的取值是10 ，此时称为10折交叉验证; 其他常用的k 值有5、20 等. 为了避免其他属性携带的信息被训练集中未出现的属性值”抹去”，在估计概率值时通常要进行”平滑” (smoothing) ，常用”拉普拉斯修正“，所以，P(c)和P(xi|c)修正为： 其中N 表示训练集D 中可能的类别数，Ni表示第 与留出法相似，将数据集D 划分为k 个子集同样存在多种划分方式.为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p 次。最终的评估结果是这p 次k 折交叉验证结果的均值，例如常见的有”10 次10 折交叉验证。 假定数据集D中包含m个样本，若令k=m ，则得到了交叉验证法的一个特例:留一法(Leave- One-Out比，简称LOO) . 显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集**—**每个子集包含一个样本;留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D 训练出的模型很相似.因此，留一法的评估结果往往被认为比较准确.然而，留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1 百万个样本，则需训练1 百万个模型)，而这还是在未考虑算法调参的情况下.另外，留一法的估计结果也未必永远比其他评估方法准确;”没有免费的午餐”定理对实验评估方法同样适用.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP+回溯-单词拆分 II]]></title>
    <url>%2F2018%2F06%2F09%2FDP%2B%E5%9B%9E%E6%BA%AF-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20II%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，在字符串中增加空格来构建一个句子，使得句子中所有的单词都在词典中。返回所有这些可能的句子。说明：分隔时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入:s = “catsanddog”wordDict = [“cat”, “cats”, “and”, “sand”, “dog”]输出:[“cats and dog”,“cat sand dog”] 示例 2：输入:s = “pineapplepenapple”wordDict = [“apple”, “pen”, “applepen”, “pine”, “pineapple”]输出:[“pine apple pen apple”,“pineapple pen apple”,“pine applepen apple”]解释: 注意你可以重复使用字典中的单词。 示例 3：输入:s = “catsandog”wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出:[] 算法思路这道题类似 Word Break I 判断是否能把字符串拆分为字典里的单词 @LeetCode 只不过要求计算的并不仅仅是是否能拆分，而是要求出所有的拆分方案。因此用递归。但是直接递归做会超时，原因是LeetCode里有几个很长但是无法拆分的情况，所以就先跑一遍Word Break I，先判断能否拆分，然后再进行拆分。 DP实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 class Solution &#123; boolean isBreak(String s, List&lt;String&gt; wordDict) &#123; boolean[] canBreak = new boolean[s.length()+1]; canBreak[0] = true; for(int i=1; i&lt;=s.length(); i++) &#123; boolean flag = false; for(int j=0; j&lt;i; j++) &#123; if(canBreak[j] &amp;&amp; wordDict.contains(s.substring(j,i))) &#123; flag = true; break; &#125; &#125; canBreak[i] = flag; &#125; return canBreak[s.length()]; &#125; void dfs(String s, List&lt;String&gt; wordDict, String str, int index)&#123; String result = str; //记录字符串状态 int len = s.length(); String tmp = s.substring(index, len); if(wordDict.contains(tmp))&#123; //最后一段存在于字典中，则保存结果 str += tmp; res.add(str); &#125; List&lt;Integer&gt; listIndex = new ArrayList(); List&lt;String&gt; listStr = new ArrayList(); for(int i = index; i &lt; len; i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; listIndex.add(i+1); listStr.add(temp); &#125; &#125; String temp = result; //保存递归前的字符串状态，以便回溯 for(int i = 0; i &lt; listIndex.size(); i++)&#123; result += listStr.get(i) + " "; dfs(s, wordDict, result, listIndex.get(i)); result = temp; &#125; &#125; List&lt;String&gt; res = new ArrayList(); public List&lt;String&gt; wordBreak(String s, List&lt;String&gt; wordDict) &#123; if(!isBreak(s, wordDict)) return res; String str = ""; dfs(s, wordDict, str, 0); return res; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫]]></title>
    <url>%2F2018%2F06%2F09%2FPython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[文本爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#encoding:utf-8import requests,urllib3.request,time,osimport random,csv,socket,http.clientfrom bs4 import BeautifulSoupdef get_contend(url, data = None): #获取网页中html代码 header=&#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'zh-CN,zh;q=0.9', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; timeout = random.choice(range(80,180)) while True: try: rep = requests.get(url,headers = header,timeout=timeout) rep.encoding = 'utf-8' break except socket.timeout as e: print ('3',e) time.sleep(random.choice.range(8,15)) except socket.error as e: print ('4',e) time.sleep(random.choice.range(20,60)) except http.client.BadStatusLine as e: print ('5',e) time.sleep(random.choice.range(30,80)) except http.client.IncompleteRead as e: print ('6',e) time.sleep(random.choice.range(5,15)) return rep.textdef get_data(html_text): final = [] bs = BeautifulSoup(html_text,'html.parser') #创建BeautifulSoup对象 body = bs.body #获取body部分 data = body.find('div',&#123;'id':'7d'&#125;) #找到需要爬取部分的div ul = data.find('ul') #获取ul部分 li = ul.find_all('li') #获取所有的li for day in li: #对li标签中内容进行遍历 temp = [] date =day.find('h1').string #找到日期 temp.append(date) #将日期添加到temp中 p = day.find_all('p') #找到每个li中的所有p标签 temp.append(p[0].string,) #第一个p标签中的天气状况添加到temp if p[1].find('span') == None: t_highest = None else: t_highest = p[1].find('span').string #找到最高温 t_highest = t_highest.replace('C','') t_lowest = p[1].find('i').string # 找到最低温 t_lowest = t_lowest.replace('C','') temp.append(t_highest) temp.append(t_lowest) final.append(temp) return finaldef write_data(data,name): #将数据写入文件 file_name = name with open(file_name, 'a', errors='ignore', newline='') as f: f_csv = csv.writer(f) f_csv.writerows(data)if __name__ == '__main__': url = 'http://www.weather.com.cn/weather/101190401.shtml' html = get_contend(url) result = get_data(html) print(result) write_data(result,'weather.csv') 图虫图片爬取按标签爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273__author__ = 'Result_Lv'#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_album_name(json_dict): album_name = [] postlist = json_dict['postList'] for i in range(len(postlist)): if postlist[i]['title'] == '': #图集标题为空时，命名为默认 album_name.append('Default' + str(i)) else: album_name.append(postlist[i]['title']) return album_namedef get_photo_id(json_dict): #获得所有照片的ID author_id = [] album_id = [] post_list = json_dict['postList'] for i in range(len(post_list)): #获取每个图集的照片ID photo_id = [] author_id.append(post_list[i]['author_id']) # 获取每个图集作者ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list album_id.append(photo_id) return author_id,album_iddef download_album(path,album_name,author_id,album_id): #下载图集 for i in range(len(album_id)): if not os.path.exists(path + album_name[i]): #若不存在对应图集的文件夹 try: os.makedirs(path + album_name[i]) #以图集名创建文件夹 except OSError as e: print(e) continue print('正在下载第' + str(i + 1) + '个图册:' + album_name[i]) for j in range(len(album_id[i])): fileurl = 'https://photo.tuchong.com/' + str(author_id[i]) +'/f/' + str(album_id[i][j]) + '.jpg' #生成每张照片Url filename = path + album_name[i] + '/' + str(j+1) + '.jpg' #命名照片 print(' 正在下载第' + str(j+1) + '张照片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 #爬取页数 path = 'F:/少女/' #存放路径 for i in range(page): url = 'https://tuchong.com/rest/tags/少女/posts?page=' + str(i+1) + '&amp;count=20&amp;order=weekly' #tag的json地址 json_dict = get_json(url) album_name = get_album_name(json_dict) para = get_photo_id(json_dict) author_id = para[0] album_id = para[1] download_album(path,album_name,author_id,album_id) 按作者爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): #解析json header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_all_photo_id(json_dict): #获得所有照片的ID #post_id = [] photo_id = [] post_list = json_dict['post_list'] author_id = post_list[0]['author_id'] #获取作者ID author_name = post_list[0]['site']['name'] #获取作者姓名 # for i in range(len(post_list)): #获取所有图集ID # post_id.append(post_list[i]['post_id']) for i in range(len(post_list)): #获取每个图集的照片ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list return author_name,author_id,photo_iddef download_photo(path,author_id,photo_id): #下载全部照片 if not os.path.exists(path): os.makedirs(path) for i in range(len(photo_id)): filename = path + '/' + str(i+1) + '.jpg' fileurl = 'https://photo.tuchong.com/' + str(author_id) + '/f/' + str(photo_id[i]) + '.jpg' print(' 第' + str(i + 1) + '张图片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 for i in range(page): url = 'https://thomaskksj.tuchong.com/rest/2/sites/395013/posts?count=20&amp;page=' + str(i + 1) #作者主页的json地址 print('正在下载第' + str(i+1) + '页:' + url) json_dict = get_json(url) para = get_all_photo_id(json_dict) author_name = para[0] author_id = para[1] photo_id = para[2] path = 'F:/' + author_name + '/page' + str(i + 1) download_photo(path,author_id,photo_id)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查准率precision和查全率recall、F1]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9F%A5%E5%87%86%E7%8E%87precision%E5%92%8C%E6%9F%A5%E5%85%A8%E7%8E%87recall%E3%80%81F1%2F</url>
    <content type="text"><![CDATA[真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），分别用TP、FP、TN、FN表示相应样例数，样例总数=TP+FP+TN+FN；分类结果混淆矩阵： 真实情况 预测结果 正例 反例 正例 TP（真正例） FN（假反例） 反例 FP（假正例） TN（真反例） 查准率（precision）：被认定为正例的里面，判断正确的比例。 查全率（recall）：真实正例里，被判断出为正例的比例。 查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低,可以通过P-R曲线来取两者的平衡值 若一个学习器P-R曲线被另一个学习器的曲线完全”包住“,则可断言后者的性能优于前者， 例如图中学习器A 的性能优于学习器C; 如果两个学习器的P-R 曲线发生了交叉7,例如图中的A 与B ，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较然而，在很多情形下，人们往往仍希望把学习器A 与B 比出个高低. 这时一个比较合理的判据是比较P-R 曲线节面积的大小。 “平衡点“是”查准率=查全率“时的取值。 但更常用的使用F1来衡量查准率与查全率； F1基于查准率与查全率的调和平均： ，sum为样例总数， 具体应用中可能对P和R有不同的倚重。比如商品推荐中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，这时候查准率更重要。而在逃犯检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。 F1度量的一般形式F**β（加权调和平均**）就可以表达这种偏好。 即 ß = 1时退化为标准的F1,当β&gt;1意味着P占比重更大，反之则是R。 ROC、AUCROC:全称“受试者工作特征”，表达了模型的泛化能力。其纵坐标为“TPR真正例率”；横坐标为“FPR假正例率”。 ROC曲线根据模型的排序结果，一个个划分正负，每次得出两个值TPR,FPR。很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。为啥在ROC曲线画一个y=x呢，那表示随机猜测的ROC。 AUC:ROC曲线下的面积.AUC=1,是完美分类器(并不存在)；0.5。AUC 越大，意味着辨别能力越强。 与P-R图相似，如果一条ROC曲线包含另一条ROC曲线，则前者的学习器性能更优越。如果曲线有交叉，则可以通过计算AUC大小得到。 代价敏感错误率、代价曲线代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP-单词拆分 I]]></title>
    <url>%2F2018%2F06%2F09%2FDP-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20I%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。说明：拆分时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入: s = “leetcode”, wordDict = [“leet”, “code”]输出: true解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 示例 2：输入: s = “applepenapple”, wordDict = [“apple”, “pen”]输出: true解释: 返回 true 因为 “applepenapple” 可以被拆分成 “apple pen apple”。注意你可以重复使用字典中的单词。 示例 3：输入: s = “catsandog”, wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出: false 算法思路动态规划的思路：将问题拆分成更小的子问题。用dp[i]表示0到i的子字符串是否可以拆分成满足条件的单词，在计算dp[i]的时候，我们已经知道dp[0],dp[1],…,dp[i-1],如果以i为结尾的j~i子串是满足条件的，并且0~j的子串也是在字典中的，那么dp[i]就是true。用公式表示就是： dp[j]&amp;&amp;s.substring[j,i+1]∈dict DP实现123456789101112131415class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123;; boolean [] dp = new boolean[s.length()+1]； dp[0] = true; for(int i = 0; i &lt; s.length(); i++)&#123; for(int j = 0; j &lt;= i; j++)&#123; if(dp[j] &amp;&amp; wordDict.contains(s.substring(j, i+1)))&#123; dp[i+1] = true; break; &#125; &#125; &#125; return dp[s.length()]; &#125;&#125; DFS解法，超时1234567891011121314151617181920212223242526class Solution &#123; boolean dfs(String s, List&lt;String&gt; wordDict, int index)&#123; // 超时 String left = s.substring(index, s.length()); if(wordDict.contains(left))&#123; return true; &#125; List&lt;Integer&gt; list = new ArrayList(); for(int i = index; i &lt; s.length(); i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; list.add(i+1); &#125; &#125; for(Integer each:list) &#123; if(dfs(s, wordDict, each))&#123; return true; &#125; &#125; return false; &#125; boolean flag = false; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; return dfs(s, wordDict, 0); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偏差、方差、噪声]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%81%8F%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。 偏差（Bias）和方差（Variance）偏差（Bias）：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。 方差（Variance）：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。 偏差：形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与指标； 方差：形容数据分散程度的，算是“无监督的”，客观的指标。 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度. 偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的. 给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小. 一般来说偏差与方差是有冲突的，这称为偏差—方差窘境，给定学习任务，假定我们能控制学习算法的训练程度（例如决策树可控制层数，神经网络可控制训练轮数，集成学习方法可控制基学习器个数），则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以便学习器产生显著变化，此时偏差主导了泛化错误率;随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率;在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合. 为什么KNN（k最近邻k-Nearest Neighbor）算法在增大k时，偏差会变大；但RF（Random Forest随机森林）增大树的数目时偏差却保持不变；GBDT（Gradient Boosting）在增大树的数目时偏差却又能变小。 对于KNN算法，k值越大，表示模型的学习能力越弱，因为k越大，它越倾向于从“面”上考虑做出判断，而不是具体地考虑一个样本近身的情况来做出判断，所以，它的偏差会越来越大。 对于RF，我们实际上是部分实现了多次训练取均值的效果，每次训练得到的树都是一个很强的学习者，每一个的方差都比较大，但综合起来就会比较小。好比一个很强的学习者学习时，刮着西风，它会据此调整自己的瞄准方法，另一个很强的学习者学习时刮着东风，（西风、东风可以理解为不同训练集中的噪声）它也会据此调整自己的瞄准方法，在测试样本时，一个误差向西，一个误差向东，刚好起到互相抵消的作用，所以方差会比较小。但是由于每棵树的偏差都差不多，所以，我们取平均时，偏差不会怎么变化。 为什么说是部分实现了多次训练取均值的效果而不是全部呢？因为我们在训练各棵树时，是通过抽样样本集来实现多次训练的，不同的训练集中不可避免地会有重合的情况，此时，就不能认为是独立的多次训练了，各个训练得到的树之间的方差会产生一定的相关性，训练集中重合的样本越多，则两棵树之间的方差的相关性越强，就越难达成方差互相抵消的效果。 对于GBDT，N棵树之间根本就不是一种多次训练取均值的关系，而是N棵树组成了相关关联，层层递进的超级学习者，可想而知，它的方差一定是比较大的。但由于它的学习能力比较强，所以，它的偏差是很小的，而且树的棵树越多，学习能力就越强，偏差就越小。也就是说，只要学习次数够多，预测的均值会无限接近于目标。简单讲就是GBDT的N棵树实际上是一个有机关联的模型，不能认为是N个模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子串-最长公共子序列]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[最长公共子串DP实现1234567891011121314151617181920212223242526272829public static int maxSubStr(String str1, String str2) &#123; int result = 0; int index = 0; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int [len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; dp[i][j] = dp[i-1][j-1] + 1; // 状态转移 if(dp[i][j] &gt; result) &#123; result = dp[i][j]; index = i; // 记录最大子串的最后一个下标 &#125; // result = result &gt; dp[i][j] ? result : dp[i][j]; &#125;else &#123; dp[i][j] = 1; result = result &gt; dp[i][j] ? result : dp[i][j]; &#125; &#125; &#125; &#125; System.out.println(result); for(int i = index - result + 1; i &lt;= index; i++) &#123; System.out.print(str1.charAt(i) + " "); &#125; return result;&#125; 最长公共子序列####动态规划假设Z=&lt;z1,z2,⋯,zk&gt;是X与Y的LCS， 我们观察到如果Xm=Yn，则Zk=Xm=Yn，有Zk−1是Xm−1与Yn−1的LCS；如果Xm≠Yn，则Zk是Xm与Yn−1的LCS，或者是Xm−1与Yn的LCS。因此，求解LCS的问题则变成递归求解的两个子问题。但是，上述的递归求解的办法中，重复的子问题多，效率低下。改进的办法——用空间换时间，用数组保存中间状态，方便后面的计算。这就是动态规划（DP)的核心思想了。DP求解LCS用二维数组c[i][j]记录串x1x2⋯xi与y1y2⋯yj的LCS长度，则可得到状态转移方程 DP实现1234567891011121314151617181920public static int maxSubSequence(String str1, String str2) &#123; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int[len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else &#123; dp[i][j] = dp[i-1][j] &gt; dp[i][j-1] ? dp[i-1][j] : dp[i][j-1]; &#125; &#125;else if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = 1; &#125; &#125; &#125; System.out.println(dp[len1 - 1][len2 - 1]); return dp[len1-1][len2-1];&#125;]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分糖果]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%88%86%E7%B3%96%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[题目描述o 分糖果：科大讯飞第二道编程题 o 小明和小红是好朋友，但最近遇到一个棘手的问题，有一盒糖果要分成两份但是每颗糖果质量都不尽相同， o 但为了分配的公平每份糖的糖果数量相差不得超过1，在此条件下两份糖果的质量差距尽可能小。 o 输入一行数，包含一个数n，代表糖果数量，后面一次是n个整数一次表示每个糖果的质量，每个糖果的质量都是1到450 o 之间的一个整数，每盒最多有20个糖果。 o 输出：每个样例输出两个数字分别为两堆糖果的质量，如不相同，先小后大。 o 样例：输入：5 9 6 5 8 7 o 输出：17 18 算法思想o 回溯，在数量差值为1的结果中找出最小的质量差 Python实现1234567891011121314151617def divide(candies, num, select, sum, total, index): global min global res if(abs(total-sum*2) &lt; min): res = sum min = abs(total-sum*2) result.append(select) for i in range(len(candies)): if(index == num-1): return3 select.append(candies[index]) sum += candies[index] temp = select.copy() if(len(select) &lt;= int(num/2)+1): index += 1 divide(candies, num, temp, sum, total, index) sum -= select[len(select) - 1] select.remove(select[len(select)-1])]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回溯法思想]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[子集树与排列树当所给问题是从n个元素的集合S中找出满足某种性质的子集时，解空间为子集树。例如：0-1背包问题 (选或不选问题)当所给问题是从n个元素的集合S中找出满足某种性质的排列时，解空间为排列树。例如：旅行售货员问题（选择顺序问题） 算法结构 深度优先搜索与广度优先搜索算法有何区别深度优先搜索法不全部保留结点，扩展完的结点从数据存储结构栈中弹出删去，在栈中存储的结点数就是解空间树的深度，因此它占用空间较少。所以，当搜索树的结点较多，用其它方法易产生内存溢出时，深度优先搜索不失为一种有效的求解方法。广度优先搜索算法，一般需存储产生所有结点，占用的存储空间要比深度优先搜索大得多，因此，程序设计中，必须考虑溢出和节省内存空间的问题。但广度优先搜索法一般无回溯操作（即入栈和出栈的操作），所以运行速度比深度优先搜索要快些。 回溯与分支限界区别回溯法以深度优先的方式搜索解空间树T，而分支限界法则以广度优先或以最小耗费优先的方式搜索解空间树T。它们在问题的解空间树T上搜索的方法不同，适合解决的问题也就不同。一般情况下，回溯法的求解目标是找出T中满足约束条件的所有解的方案，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。相对而言，分支限界算法的解空间比回溯法大得多，因此当内存容量有限时，回溯法成功的可能性更大。 最优化处理问题在处理最优问题时，采用穷举法、回溯法或分支限界法都可以通过利用当前最优解和上界函数加速。仅就对限界剪支的效率而言，优先队列的分支限界法显然要更充分一些。在穷举法中通过上界函数与当前情况下函数值的比较可以直接略过不合要求的情况而省去了更进一步的枚举和判断；回溯法则因为层次的划分，可以在上界函数值小于当前最优解时，剪去以该结点为根的子树，也就是节省了搜索范围；分支限界法在这方面除了可以做到回溯法能做到的之外，同时若采用优先队列的分支限界法，用上界函数作为活结点的优先级，一旦有叶结点成为当前扩展结点，就意味着该叶结点所对应的解即为最优解，可以立即终止其余的过程。在前面的例题中曾说明，优先队列的分支限界法更象是有选择、有目的地进行搜索，时间效率、空间效率都是比较高的。 算法总结一个问题是该用递推、贪心、搜索还是动态规划，完全是由这个问题本身阶段间状态的转移方式决定的！每个阶段只有一个状态-&gt;递推；每个阶段的最优状态都是由上一个阶段的最优状态得到的-&gt;贪心；每个阶段的最优状态是由之前所有阶段的状态的组合得到的-&gt;搜索；每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到而不管之前这个状态是如何得到的-&gt;动态规划。 动态规划1.求最优解问题2.整体问题的最优解依赖于各个子问题的最优解3.把大问题分解成小问题，小问题之间还有相互重叠的更小的子问题4.从上往下分析，从下往上求解，避免重复求解小问题]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
