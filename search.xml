<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[10大经典算法总结]]></title>
    <url>%2F2018%2F08%2F21%2F10%E5%A4%A7%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[0、算法概述0.1 算法分类①：十种常见排序算法可以分为两大类： 非线性时间比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此称为非线性时间比较类排序。 线性时间非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此称为线性时间非比较类排序。 ②：另外一种分类排序方法——我们通常所说的排序算法往往指的是内部排序算法，即数据记录在内存中进行排序。 排序算法大体可分为两种： 一种是比较排序，时间复杂度O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。 另一种是非比较排序，时间复杂度可以达到O(n)，主要有：计数排序，基数排序，桶排序等。 0.2 算法复杂度 0.3 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。通俗地讲就是保证排序前后两个相等的数的相对顺序不变。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数。 ​ ​ 对于不稳定的排序算法，只要举出一个实例，即可说明它的不稳定性；而对于稳定的排序算法，必须对算法进行分析从而得到稳定的特性。需要注意的是，排序算法是否为稳定的是由具体算法决定的，不稳定的算法在某种条件下可以变为稳定的算法，而稳定的算法在某种条件下也可以变为不稳定的算法。 例如，对于冒泡排序，原本是稳定的排序算法，如果将记录交换的条件改成A[i] &gt;= A[i + 1]，则两个相等的记录就会交换位置，从而变成不稳定的排序算法。 其次，说一下排序算法稳定性的好处。排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，前一个键排序的结果可以为后一个键排序所用。基数排序就是这样，先按低位排序，逐次按高位排序，低位排序后元素的顺序在高位也相同时是不会改变的。 0.4 技巧记忆口诀： 不稳定排序算法口诀：快些选队（快希选堆）其余为稳定的。 算法复杂度和关键字顺序无关的有： 顺口溜：一堆（堆排序）海龟（归并排序）选（选择排序）基（基数排序）友 快些以 nlog2 n 的速度归队 快=快速排序，些=希尔排序，归=归并排序，队=堆排序 这四种排序算法，时间都是 n log2 n 的，除了这四个之外，其他的排序算法平均时间都为 n^2 一趟排序，保证一个元素为最终位置的有两类排序算法：交换类（冒泡和快速）排序和选择类排序（简单和堆） 元素比较次数和原始序列无关的算法：简单选择排序，折半插入排序 排序趟数和原序列有关的算法：交换类，其余类无关 借助于比较进行排序的算法，在最坏的时候，最好的时间复杂度为 n log2 n 堆排序和简单选择排序的时间复杂度和初始序列无关 0.5 总结：（1）在比较类排序中，归并排序号称最快，其次是快速排序和堆排序，两者不相伯仲，但是有一点需要注意，数据初始排序状态对堆排序不会产生太大的影响，而快速排序却恰恰相反。 （2）线性时间非比较类排序一般要优于非线性时间比较类排序，但前者对待排序元素的要求较为严格，比如计数排序要求待排序数的最大值不能太大，桶排序要求元素按照hash分桶后桶内元素的数量要均匀。线性时间非比较类排序的典型特点是以空间换时间。 1、冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 1.1 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 1.2 动图演示 1.3 代码实现1234567891011121314151617181920212223// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n^2)// 最优时间复杂度 ---- 如果能在内部循环第一次运行时,使用一个旗标来表示有无需要交换的可能,可以把最优时间复杂度降低到O(n)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 稳定void BubbleSort(RecordType arr[]) &#123; var len = arr.length; change = TRUE; for (var i = 0; i &lt; len - 1 &amp;&amp; change; i++) &#123; change = FALSE; for (var j = 0; j &lt; len - 1 - i; j++) &#123;// 依次比较相邻的两个元素,使较大的那个向后移 if (arr[j].key &gt; arr[j+1].key) &#123; // 相邻元素两两对比。 如果条件改成arr[j].key &gt;= arr[j+1].key,则变为不稳定的排序算法 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; change = TRUE; &#125; &#125; &#125; return arr;&#125; 1.1、冒泡排序的改进：鸡尾酒排序（定向/双向冒泡排序）​ 鸡尾酒排序，也叫定向冒泡排序，是冒泡排序的一种改进，即排序过程中交替改变扫描方向。 先从底向上冒一个最小元素，再从上向低冒一个最大元素。 此算法与冒泡排序的不同处在于从低到高然后从高到低，而冒泡排序则仅从低到高去比较序列里的每个元素。他可以得到比冒泡排序稍微好一点的效能。 鸡尾酒排序的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt;// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n^2)// 最优时间复杂度 ---- 如果序列在一开始已经大部分排序过的话,会接近O(n)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 稳定void Swap(int A[], int i, int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125;void CocktailSort(int A[], int n)&#123; int left = 0; // 初始化边界 int right = n - 1; bool flag = TRUE; while (flag) &#123; flag = FLASE; for (int i = left; i &lt; right; i++) // 前半轮,从左到右扫描，将最大元素放到最右边 &#123; if (A[i] &gt; A[i + 1]) &#123; Swap(A, i, i + 1); flag = TRUE; &#125; &#125; right--; for (int i = right; i &gt; left; i--) // 后半轮,从右到左扫描,将最小元素放到最左边 &#123; if (A[i - 1] &gt; A[i]) &#123; Swap(A, i - 1, i); &#125; &#125; left++; &#125;&#125;int main()&#123; int A[] = &#123; 6, 5, 3, 1, 8, 7, 2, 4 &#125;; // 从小到大定向冒泡排序 int n = sizeof(A) / sizeof(int); CocktailSort(A, n); printf("鸡尾酒排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; ​ 使用鸡尾酒排序为一列数字进行排序的过程如右图所示： 以序列(2,3,4,5,1)为例，鸡尾酒排序只需要访问一次序列就可以完成排序，但如果使用冒泡排序则需要四次。但是在乱数序列的状态下，鸡尾酒排序与冒泡排序的效率都很差劲。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer（一）：二维数组中的查找]]></title>
    <url>%2F2018%2F07%2F12%2F%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[剑指Offer（一）：二维数组中的查找摘要 在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 一、前言本系列文章为《剑指offer》刷题笔记。 刷题平台：牛客网 二、题目在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 1、思路​ 首先选取数组中右上角的数字。如果该数字等于要查找的数字，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数字，剔除这个数字所在的行。也就是说如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 2、举例如果在一个二维数组中找到数字7，则返回true，如果没有找到，则返回false。 查找过程如下： 3、编程实现C++： 1234567891011121314151617181920212223class Solution &#123;public: bool Find(int target, vector&lt;vector&lt;int&gt; &gt; array) &#123; int rows = array.size(); int cols = array[0].size(); if(!array.empty() &amp;&amp; rows &gt; 0 &amp;&amp; cols &gt; 0)&#123; int row = 0; int col = cols - 1; while(row &lt; rows &amp;&amp; col &gt;= 0)&#123; if(array[row][col] == target)&#123; return true; &#125; else if(array[row][col] &gt; target)&#123; --col; &#125; else&#123; ++row; &#125; &#125; &#125; return false; &#125;&#125;; Python2.7： 12345678910111213141516171819# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here rows = len(array) cols = len(array[0]) if rows &gt; 0 and cols &gt; 0: row = 0 col = cols - 1 while row &lt; rows and col &gt;= 0: if target == array[row][col]: return True elif target &lt; array[row][col]: col -= 1 else: row += 1 return False]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
        <tag>数组</tag>
        <tag>笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解朴素贝叶斯]]></title>
    <url>%2F2018%2F07%2F06%2F%E7%90%86%E8%A7%A3%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[理解朴素贝叶斯（1）先验概率、后验概率、联合概率、全概率如果我对这个西瓜没有任何了解，包括瓜的颜色、形状、瓜蒂是否脱落。按常理来说，西瓜成熟的概率大概是 60%。那么，这个概率 P(瓜熟) 就被称为先验概率。 即，先验概率是根据以往经验和分析得到的概率，先验概率无需样本数据，不受任何条件的影响。就像只根据常识而不根据西瓜状态来判断西瓜是否熟，这就是先验概率。 一个判断西瓜是否成熟的常识，就是看瓜蒂是否脱落。一般来说，瓜蒂脱落的情况下，西瓜成熟的概率大一些，大概是 75%。如果把瓜蒂脱落当作一种结果，然后去推测西瓜成熟的概率，这个概率 P(瓜熟 | 瓜蒂脱落) 就被称为后验概率。后验概率类似于条件概率。 知道了先验概率和后验概率，我们再来看看什么是联合概率。P(瓜熟，瓜蒂脱落) 称之为联合分布，它表示瓜熟了且瓜蒂脱落的概率。关于联合概率，满足下列乘法等式： P(瓜熟，瓜蒂脱落) = P(瓜熟**|瓜蒂脱落)**·P(瓜蒂脱落)=P(瓜蒂脱落|瓜熟)·P(瓜熟) 其中，P(瓜熟 | 瓜蒂脱落) 就是刚刚介绍的后验概率，表示在“瓜蒂脱落”的条件下，“瓜熟”的概率。P(瓜蒂脱落 | 瓜熟) 表示在“瓜熟”的情况下，“瓜蒂脱落”的概率。 如何计算瓜蒂脱落的概率呢？实际上可以分成两种情况：一种是瓜熟状态下瓜蒂脱落的概率，另一种是瓜生状态下瓜蒂脱落的概率。瓜蒂脱落的概率就是这两种情况之和。因此，我们就推导出了全概率公式： P(瓜蒂脱落)=P(瓜蒂脱落|瓜熟)·P(瓜熟)+P(瓜蒂脱落|瓜生)·P(瓜生) （2）单个特征判断瓜熟好了，介绍完先验概率、后验概率、联合概率、全概率后，我们来看这样一个问题：西瓜的状态分成两种：瓜熟与瓜生，概率分别为 0.6 与 0.4，且瓜熟里面瓜蒂脱落的概率是 0.8，瓜生里面瓜蒂脱落的概率是 0.4。那么，如果我现在挑到了一个瓜蒂脱落的瓜，则该瓜是好瓜的概率多大？ 显然，这是一个计算后验概率的问题，根据我们上面推导的联合概率和全概率公式，可以求出： 一项一项来看： 条件概率 P(瓜蒂脱落 | 瓜熟) = 0.8 先验概率 P(瓜熟) = 0.6 条件概率 P(瓜蒂脱落 | 瓜生) = 0.4 先验概率 P(瓜生) = 0.4 将以上数值带入上式，得： 注意，以上这种计算后验概率的公式就是利用贝叶斯定理。 （3）多个特征判断瓜熟判断一个瓜是否熟了，除了要看瓜蒂是否脱落，还要看瓜的形状和颜色。形状有圆和尖之分，颜色有深绿、浅绿、青色之分。我们可以使用刚刚引入的贝叶斯定理思想来尝试解决这个问题。 现在，特征由原来的 1 个，变成现在的 3 个，我们用 X 表示特征，用 Y 表示瓜的类型（瓜熟还是瓜生）。则根据贝叶斯定理，后验概率 P(Y=ck | X=x) 的表达式为： 其中，ck 表示类别，k 为类别个数。本例中，k = 1，2，c1 表示瓜熟，c2 表示瓜生。上面的公式看似有点复杂，但其实与上一节单特征（瓜蒂是否脱落）的形式是一致的。 有一点需要注意，这里的特征 X 不再是单一的，而是包含了 3 个特征。因此，条件概率 P(X=x | Y=ck) 假设各个条件相互独立，也就是说假设不同特征之间是相互独立的。这样，P(X=x | Y=ck) 就可以写成： 其中，n 为特征个数，j 表示当前所属特征。针对这个例子，P(X=x | Y=ck) 可以写成： 这种条件独立性的假设就是朴素贝叶斯法“朴素”二字的由来。这一假设让朴素贝叶斯法变得简单，但是有时候会牺牲一定的分类准确率。 ​ 利用朴素贝叶斯思想，我们就可以把后验概率写成： 上面的公式看上去比较复杂，其实只是样本特征增加了，形式上与上一节 P(瓜熟 | 瓜蒂脱落) 是一致的。 现在，一个西瓜，观察了它的瓜蒂、形状、颜色三个特征，就能根据上面的朴素贝叶斯公式，分别计算 c1（瓜熟）和 c2（瓜生）的概率，即 P(Y=c1 | X=x) 和 P(Y=c2 | X=x)。然后再比较 P(Y=c1 | X=x) 和 P(Y=c2 | X=x) 值的大小： 若 P(Y=c1 | X=x) &gt; P(Y=c2 | X=x)，则判断瓜熟； 若 P(Y=c1 | X=x) &lt; P(Y=c2 | X=x)，则判断瓜生。 值得注意的是上式中的分母部分，对于所有的 ck 来说，都是一样的。因此，分母可以省略，不同的 ck，仅比较 P(Y=ck | X=x) 的分子即可： （2）朴素贝叶斯分类买瓜之前，还有一件事情要做，就是搜集样本数据。通过网上资料和查阅，获得了一组包含 10 组样本的数据。这组数据是不同瓜蒂、形状、颜色对应的西瓜是生是熟。我把这组数据当成是历史经验数据，以它为标准。 其中，瓜蒂分为脱落和未脱，形状分为圆形和尖形，颜色分为深绿、浅绿、青色。不同特征组合对应着瓜熟或者瓜生。 现在，挑了一个西瓜，它的瓜蒂脱落、形状圆形、颜色青色。这时候，就完全可以根据样本数据和朴素贝叶斯法来计算后验概率。 首先，对于瓜熟的情况： 瓜熟的先验概率： P(瓜熟) = 6 / 10 = 0.6。 条件概率： P(脱落 | 瓜熟) = 4 / 6 = 2 / 3。 条件概率： P(圆形 | 瓜熟) = 4 / 6 = 2 / 3。 条件概率： P(青色 | 瓜熟) = 2 / 6 = 1 / 3。 计算后验概率分子部分：P(瓜熟) × P(脱落 | 瓜熟) × P(圆形 | 瓜熟) × P(青色 | 瓜熟) = 0.6 × (2 / 3) × (2 / 3) × (1 / 3) = 4 / 45。 然后，对于瓜生的情况： 瓜生的先验概率： P(瓜生) = 4 / 10 = 0.4。 条件概率： P(脱落 | 瓜生) = 1 / 4 = 0.25。 条件概率： P(圆形 | 瓜生) = 1 / 4 = 0.25。 条件概率： P(青色 | 瓜生) = 1 / 4 = 0.25。 计算后验概率分子部分：P(瓜生) × P(脱落 | 瓜生) × P(圆形 | 瓜生) × P(青色 | 瓜生) = 0.4 × 0.25 × 0.25 × 0.25 = 1 / 160。 因为 4 / 45 &gt; 1 / 160，所以预测为瓜熟。终于计算完了，很肯定这个西瓜瓜蒂脱落、形状圆形、颜色青色，应该是熟瓜。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名解析]]></title>
    <url>%2F2018%2F07%2F03%2F%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[域名解析域名有钱最好买.com的，不要买.cn的，我自己买的.cn的好多坑，.top的相对就比较便宜（low）。 此次在腾讯云买的dadavision.cn。 之前.cn的一直要我备案，否则DNS解析不了，而且还证书审核失败。 在github项目的setting里面设置域名 添加域名解析 这里注意 在添加绑定域名后需要注意以下问题（未绑定可以不用） 需要将github上CNAME里面内容在文件工程中GitBlog\source\CNAME 这里注意新建的CNAME不要有后缀名。]]></content>
      <categories>
        <category>域名解析</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>next</tag>
        <tag>教程</tag>
        <tag>域名</tag>
        <tag>域名解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch0.4.0升级概述新手教程]]></title>
    <url>%2F2018%2F06%2F20%2FPytorch0-4-0%E5%8D%87%E7%BA%A7%E6%A6%82%E8%BF%B0%E6%96%B0%E6%89%8B%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[pytorch0.4支持了Windows系统的开发，在首页即可使用pip安装pytorch和torchvision。 说白了，以下文字就是来自官方文档60分钟入门的简要翻译. pytorch是啥python的科学计算库，使得NumPy可用于GPU计算，并提供了一个深度学习平台使得灵活性和速度最大化 入门Tensors(张量) Tensors与NumPy的ndarrays类似，另外可以使用GPU加速计算 未初始化的5*3的矩阵:x = torch.empty(5, 3) 随机初始化的矩阵:x = torch.rand(5, 3) 全零矩阵,定义数据类型:x = torch.zeros(5, 3, dtype=torch.long) 由数据构造矩阵:x = torch.tensor([5.5, 3]) 由已存在张量构造矩阵，性质与之前张量一致: 12x = x.new_ones(5, 3, dtype=torch.double) x = torch.randn_like(x, dtype=torch.float) 获取维度:print(x.size()) Operations有多种operation的格式，这里考虑加法 12y = torch.rand(5, 3)print(x + y) 1print(torch.add(x, y)) 123result = torch.empty(5, 3)torch.add(x, y, out=result)print(result) 123# adds x to yy.add_(x)print(y) operations中需要改变张量本身的值，可以在operation后加,比如`x.copy(y), x.t_()` 索引:print(x[:, 1]) 改变维度:x.view(-1, 8) 和Numpy的联系torch tensor 和 numpy array之间可以进行相互转换，他们会共享内存位置，改变一个，另一个会跟着改变。 tensor to array1234a = torch.ones(5)b = a.numpy()a.add_(1)print(a,b) array to tensor123456import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b) CUDA Tensorstensor可以使用.to方法将其移动到任何设备。 123456789# let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # a CUDA device object y = torch.ones_like(x, device=device) # directly create a tensor on GPU x = x.to(device) # or just use strings ``.to(&quot;cuda&quot;)`` z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double)) # ``.to`` can also change dtype together! Autograd(自动求导)pytorch神经网络的核心模块就是autograd，autograd模块对Tensors上的所有operations提供了自动求导。 Tensortorch.Tensor是模块中的核心类，如果设置属性.requires_grad = True,开始追踪张量上的所有节点操作，指定其是否计算梯度。使用.backward()方法进行所有梯度的自动求导，张量的梯度会累积到.grad属性中。 .detach()停止张量的追踪，从梯度计算中分离出来；另外在评估模型时一般使用代码块with torch.no_grad():,因为模型中通常训练的参数也会有.requires_grad = True,这样写可以停止全部张量的梯度更新。 Function类是autograd的变体，Tensor和Function相互交错构建成无环图，编码了完整的计算过程，每个Variable(变量)都有.grad_fn属性，引用一个已经创建了的Tensor的Function. 如上，使用.backward()计算梯度。如果张量是一个标量(只有一个元素),不需要对.backward()指定参数；如果张量不止一个元素，需要指定.backward()的参数，其匹配张量的维度。 1234567891011121314151617import torchx = torch.ones(2, 2, requires_grad=True)print(x)y = x + 2print(y)print(y.grad_fn)z = y * y * 3out = z.mean()print(z, out)a = torch.randn(2, 2)a = ((a * 3) / (a - 1))print(a.requires_grad)a.requires_grad_(True) # 改变a张量内在的属性print(a.requires_grad)b = (a * a).sum()print(b.grad_fn) Gradients反向传播时，由于out是一个标量，out.backward()等效于out.backward(torch.tensor(1)) 123456789101112131415161718192021out.backward()print(x.grad)x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000: y = y * 2print(y)gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(gradients)print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): print((x ** 2).requires_grad) 神经网络神经网络可以用torch.nn构建。nn依赖于autograd定义模型和求导，nn.Module定义网络层，方法forward(input)返回网络输出。 举例说明，如下是对数字图片分类的卷积网络架构。 这是一个简单的前馈神经网络，将输入数据依次通过几层网络层后最终得到输出。 神经网络典型的训练步骤如下： 定义神经网络及学习的参数(权重) 迭代输入数据 将输入数据输入到网络结构中 计算代价函数 误差向后传播 更新网络权重 weight = weight - learning_rate * gradient 定义网络123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) out: 1234567Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 可以仅定义forward()函数，当使用autograd时backward()被自动定义。可以在forward()函数中使用任何operation操作。 net.parameters()返回模型中的可学习参数。 123params = list(net.parameters())print(len(params))print(params[0].size()) # conv1&apos;s .weight 使所有参数的梯度归零然后开始计算梯度 12net.zero_grad()out.backward(torch.randn(1, 10)) 代价函数代价函数将(output,target)作为输入，计算output与target之间的距离。 nn模块中有几种不同的代价函数选择，最简单的是nn.MSELoss，计算均方误差 eg： 1234567output = net(input)target = torch.arange(1, 11) # a dummy target, for exampletarget = target.view(1, -1) # make it the same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target)print(loss) 按照向后传播的方向传播loss，使用grad_fn可以查看整个流程的计算图 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 使用loss.backward()，流程中所有requres_grad=True的张量累积它的梯度至.grad 123print(loss.grad_fn) # MSELossprint(loss.grad_fn.next_functions[0][0]) # Linearprint(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 向后传播loss.backward()传播误差， 123456789net.zero_grad() # zeroes the gradient buffers of all parametersprint(&apos;conv1.bias.grad before backward&apos;)print(net.conv1.bias.grad)loss.backward()print(&apos;conv1.bias.grad after backward&apos;)print(net.conv1.bias.grad) 更新权重误差每次传播后，需要对权重进行更新，简单的更新方式如下： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) torch.optim实现了这一过程，并有着不同的更新规则GD, Nesterov-SGD, Adam, RMSProp， 1234567891011import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update note: 每次迭代时由于梯度的累积，需要手动将梯度归零optimizer.zero_grad()]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
        <tag>Pytorch0.4.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[唐老师研究生毕业演讲]]></title>
    <url>%2F2018%2F06%2F20%2F%E5%94%90%E8%80%81%E5%B8%88%E7%A0%94%E7%A9%B6%E7%94%9F%E6%AF%95%E4%B8%9A%E6%BC%94%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[尊敬的何院士、各位嘉宾、各位老师、同学们： 大家下午好！]]></content>
      <categories>
        <category>毕业演讲</category>
      </categories>
      <tags>
        <tag>毕业演讲</tag>
        <tag>唐老师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10+Anaconda+tensorflow1.8+pytorch0.4.0注意事项]]></title>
    <url>%2F2018%2F06%2F12%2Fwin10-Anaconda-tensorflow1-8-pytorch0-4-0%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[注意事项 Anaconda会自带python3.6，所以为了避免环境配置乱套，删除本机独立安装的python。 不要按网上配置虚拟环境，比如TensorFlow配置一个虚拟环境，pytorch配置一个虚拟环境……keras……cafee……只要一个环境即可。 win10+Anaconda3-5.2.0+tensorflow1.8+pytorch0.4.0 在windows PowerShell中确定python环境在Anaconda中 Anaconda注意：自动添加到环境变量其它事项一路安装，普通套路。 TensorFlow1、TensorFlow或者pytorch，无论什么框架，越新越好，新的功能多bug少，这里用的TensorFlow1.8。 2、TensorFlow和pytorch最好都用conda装，但是这里conda install tensorflow只会给你比较低的版本，所以TensorFlow安装在这里使用了：1conda install --channel https://conda.anaconda.org/conda-forge tensorflow 可能会下载很慢，如果有部分下载未成功，重新执行命令即可，之前下载的会保留。 Pytorch同样遵循使用最新框架原则。我自己电脑用的pytorch-CPU版本，所以执行：12conda install conda install pytorch-cpu -c pytorchpip install torchvision 在实验室服务器用的是cuda9.0版本12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 安装成功之后效果： PyCharm注意事项在不用虚拟幻境情况下，好处就是不用切换环境配置了，直接一个 Anaconda\python.exe全部搞定，如图： 附录鄙视链：caffe-pytorch-tensorflow-caffe2- caffetorch-slim-tensorflow-keras]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Anaconda</tag>
        <tag>tensorflow</tag>
        <tag>pytorch</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch0.4.0重大更新]]></title>
    <url>%2F2018%2F06%2F11%2FPyTorch0.4.0%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[一、重大核心变化包括 Tensor/Variable 合并 零维张量 数据类型 迁移指南 二、现添加的新特征包括 Tensors： 全面支持高级索引 快速傅立叶变换 神经网络： 计算时的存储权衡 bottleneck-识别代码中热点（hotspots）的工具 torch.distributions 24 个基础的概率分布 增加cdf、方差、信息熵、困惑度等 分布式训练 易于使用的 Launcher utility NCCL2 后端 C++拓展 Windows 支持 ONNX 改进 RNN 支持 三、性能改进 四、Bug 修复 五、torchvision的一些变化1.torchvision.transform中函数torchvision.transforms.`Scale(*args, **kwargs)即将被函数torchvision.transforms.Resize`(size, interpolation=2)代替。（参考官方文档：点击打开链接） 2.torchvision.transform中函数torchvision.transforms.`RandomSizedCrop(*args, **kwargs)即将被函数torchvision.transforms.RandomResizedCrop`(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)代替。（参考官方文档：点击打开链接） 六、安装方式——Windows安装【方法一】pip直接安装。官网（点击打开链接）给出的安装步骤如下图所示（根据CUDA版本以及Python版本选择）。 Run this command: 12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 这里我是下载好torch-0.4.0-cp36-cp36m-win_amd64.whl文件到本地之后才安装的。进入Anaconda Prompt，然后进入文件所在目录： 打开cmd命令提示符，先利用anaconda创建一个虚拟环境，命名为pytorch4 1conda create -n pytorch4 python=3.6 激活刚才创建好的虚拟环境 1activate pytorch4 安装pytorch0.4.0 1pip install torch-0.4.0-cp35-cp35m-win_amd64.whl 注：根据自己的配置选择whl下载来链接 安装torchvision1pip install torchvision 简单测试安装是否成功123pythonimport torchprint(torch.__version__) 如果输出0.4.0，那么恭喜Windows下的PyTorch0.4.0安装成功！]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>PyTorch</tag>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mnist手写数字识别]]></title>
    <url>%2F2018%2F06%2F09%2FMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[minist_mpl.py1234567891011121314151617181920212223242526272829303132333435 #encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense,Activation,Dropoutfrom keras.optimizers import SGD,Adadeltafrom keras.models import save_modelimport matplotlib.pyplot as plt(x_train,y_train),(x_test,y_test) = mnist.load_data()x_train = x_train.reshape(60000,28*28).astype('float32') #转换数据格式x_test = x_test.reshape(10000,28*28).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train,10) #one-hot编码y_test = keras.utils.to_categorical(y_test,10)print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)modle = Sequential()#第一层隐层，64个神经元modle.add(Dense(256,activation='relu',input_dim=28*28))#第二层隐层，64个神经元modle.add(Dense(256,activation='relu'))modle.add(Dropout(0.5))#输出层，10个神经元modle.add(Dense(10,activation='softmax'))sgd = SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)modle.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy'])modle.fit(x_train,y_train,epochs=10,batch_size=128)score = modle.evaluate(x_test,y_test,batch_size=128)print(score)modle.save('MLP_minist.h5') 123456789101112131415161718192021222324252627282930313233343536373839#encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequential,save_modelfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2Dfrom keras.optimizers import SGD,Adadelta(x_train,y_train),(x_test,y_test) = mnist.load_data() #加载数据print(x_train.shape,x_test.shape)x_train = x_train.reshape(60000,28,28,1).astype('float32') #二维数据x_test = x_test.reshape(10000,28,28,1).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train) #one-hot编码y_test = keras.utils.to_categorical(y_test)num_classes = y_test.shape[1]model = Sequential() #创建序列模型model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1))) #第一层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Conv2D(64,(3,3),activation='relu')) #第二层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Flatten()) #铺平当前节点model.add(Dense(128,activation='relu')) #全连接层model.add(Dropout(0.5)) #随机失活model.add(Dense(num_classes,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) #编译模型model.fit(x_train,y_train,batch_size=128,epochs=10) #训练模型score = model.evaluate(x_test,y_test,batch_size=128) #评价模型print(score) #打印分类准确率model.save('CNN_minist.h5')]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Keras</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交叉验证集、测试集]]></title>
    <url>%2F2018%2F06%2F09%2F%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%2F</url>
    <content type="text"><![CDATA[什么有交叉验证：主要是因为训练集较小。无法直接像前面那样只分出训练集，验证集，测试就可以了（简单交叉验证）。 最佳的数据分类情况是把数据集分为三部分，分别为：训练集(train set)，验证集(validation set)和测试集(test set) 验证集和测试集两者的主要区别是：验证集用于进一步确定模型中的超参数(例如正则项系数、ANN{Artificial Neural Network}中隐含层的节点个数、网络层数、迭代次数、学习率)而测试集只是用于评估模型的精确度(即泛化能力)！ 举个例子：假设建立一个BP神经网络，对于隐含层的节点数目，我们并没有很好的方法去确定。一般将节点数设定为某一具体的值，通过训练集训练出相应的参数后，再由交叉验证集去检测该模型的误差；然后再改变节点数，重复上述过程，直到交叉验证误差最小。此时的节点数可以认为是最优节点数，即该节点数(这个参数)是通过交叉验证集得到的。 而测试集是在确定了所有参数之后，根据测试误差来评判这个学习模型的；也可以说是用来评估模型的泛化能力。所以，验证集主要主要是用于模型的调参。 “交叉验证法“ (cross validation)先将数据集D 划分为k 个大小相似的互斥子集， 即D = D**1 ∪ D**2 ∪**… ∪ Dk, Di ∩ Dj = ø (i≠j) . 每个子集Di 都尽可能保持数据分布的一致性，即从D 中通过分层采样得到. 然后，每次用k-1 个子集的并集作为训练集，余下的那个子集作为测试集;这样就可获得k组训练/测试集，从而可进行k 次训练和测试，最终返回的是这k 个测试结果的均值**。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为” k 折**/倍**交叉验证“ (k-fold cross validation). k 最常用的取值是10 ，此时称为10折交叉验证; 其他常用的k 值有5、20 等. 为了避免其他属性携带的信息被训练集中未出现的属性值”抹去”，在估计概率值时通常要进行”平滑” (smoothing) ，常用”拉普拉斯修正“，所以，P(c)和P(xi|c)修正为： 其中N 表示训练集D 中可能的类别数，Ni表示第 与留出法相似，将数据集D 划分为k 个子集同样存在多种划分方式.为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p 次。最终的评估结果是这p 次k 折交叉验证结果的均值，例如常见的有”10 次10 折交叉验证。 假定数据集D中包含m个样本，若令k=m ，则得到了交叉验证法的一个特例:留一法(Leave- One-Out比，简称LOO) . 显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集**—**每个子集包含一个样本;留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D 训练出的模型很相似.因此，留一法的评估结果往往被认为比较准确.然而，留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1 百万个样本，则需训练1 百万个模型)，而这还是在未考虑算法调参的情况下.另外，留一法的估计结果也未必永远比其他评估方法准确;”没有免费的午餐”定理对实验评估方法同样适用.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP-单词拆分 I]]></title>
    <url>%2F2018%2F06%2F09%2FDP-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20I%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。说明：拆分时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入: s = “leetcode”, wordDict = [“leet”, “code”]输出: true解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 示例 2：输入: s = “applepenapple”, wordDict = [“apple”, “pen”]输出: true解释: 返回 true 因为 “applepenapple” 可以被拆分成 “apple pen apple”。注意你可以重复使用字典中的单词。 示例 3：输入: s = “catsandog”, wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出: false 算法思路动态规划的思路：将问题拆分成更小的子问题。用dp[i]表示0到i的子字符串是否可以拆分成满足条件的单词，在计算dp[i]的时候，我们已经知道dp[0],dp[1],…,dp[i-1],如果以i为结尾的j~i子串是满足条件的，并且0~j的子串也是在字典中的，那么dp[i]就是true。用公式表示就是： dp[j]&amp;&amp;s.substring[j,i+1]∈dict DP实现123456789101112131415class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123;; boolean [] dp = new boolean[s.length()+1]； dp[0] = true; for(int i = 0; i &lt; s.length(); i++)&#123; for(int j = 0; j &lt;= i; j++)&#123; if(dp[j] &amp;&amp; wordDict.contains(s.substring(j, i+1)))&#123; dp[i+1] = true; break; &#125; &#125; &#125; return dp[s.length()]; &#125;&#125; DFS解法，超时1234567891011121314151617181920212223242526class Solution &#123; boolean dfs(String s, List&lt;String&gt; wordDict, int index)&#123; // 超时 String left = s.substring(index, s.length()); if(wordDict.contains(left))&#123; return true; &#125; List&lt;Integer&gt; list = new ArrayList(); for(int i = index; i &lt; s.length(); i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; list.add(i+1); &#125; &#125; for(Integer each:list) &#123; if(dfs(s, wordDict, each))&#123; return true; &#125; &#125; return false; &#125; boolean flag = false; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; return dfs(s, wordDict, 0); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>动态规划</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫]]></title>
    <url>%2F2018%2F06%2F09%2FPython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[文本爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#encoding:utf-8import requests,urllib3.request,time,osimport random,csv,socket,http.clientfrom bs4 import BeautifulSoupdef get_contend(url, data = None): #获取网页中html代码 header=&#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'zh-CN,zh;q=0.9', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; timeout = random.choice(range(80,180)) while True: try: rep = requests.get(url,headers = header,timeout=timeout) rep.encoding = 'utf-8' break except socket.timeout as e: print ('3',e) time.sleep(random.choice.range(8,15)) except socket.error as e: print ('4',e) time.sleep(random.choice.range(20,60)) except http.client.BadStatusLine as e: print ('5',e) time.sleep(random.choice.range(30,80)) except http.client.IncompleteRead as e: print ('6',e) time.sleep(random.choice.range(5,15)) return rep.textdef get_data(html_text): final = [] bs = BeautifulSoup(html_text,'html.parser') #创建BeautifulSoup对象 body = bs.body #获取body部分 data = body.find('div',&#123;'id':'7d'&#125;) #找到需要爬取部分的div ul = data.find('ul') #获取ul部分 li = ul.find_all('li') #获取所有的li for day in li: #对li标签中内容进行遍历 temp = [] date =day.find('h1').string #找到日期 temp.append(date) #将日期添加到temp中 p = day.find_all('p') #找到每个li中的所有p标签 temp.append(p[0].string,) #第一个p标签中的天气状况添加到temp if p[1].find('span') == None: t_highest = None else: t_highest = p[1].find('span').string #找到最高温 t_highest = t_highest.replace('C','') t_lowest = p[1].find('i').string # 找到最低温 t_lowest = t_lowest.replace('C','') temp.append(t_highest) temp.append(t_lowest) final.append(temp) return finaldef write_data(data,name): #将数据写入文件 file_name = name with open(file_name, 'a', errors='ignore', newline='') as f: f_csv = csv.writer(f) f_csv.writerows(data)if __name__ == '__main__': url = 'http://www.weather.com.cn/weather/101190401.shtml' html = get_contend(url) result = get_data(html) print(result) write_data(result,'weather.csv') 图虫图片爬取按标签爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273__author__ = 'Result_Lv'#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_album_name(json_dict): album_name = [] postlist = json_dict['postList'] for i in range(len(postlist)): if postlist[i]['title'] == '': #图集标题为空时，命名为默认 album_name.append('Default' + str(i)) else: album_name.append(postlist[i]['title']) return album_namedef get_photo_id(json_dict): #获得所有照片的ID author_id = [] album_id = [] post_list = json_dict['postList'] for i in range(len(post_list)): #获取每个图集的照片ID photo_id = [] author_id.append(post_list[i]['author_id']) # 获取每个图集作者ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list album_id.append(photo_id) return author_id,album_iddef download_album(path,album_name,author_id,album_id): #下载图集 for i in range(len(album_id)): if not os.path.exists(path + album_name[i]): #若不存在对应图集的文件夹 try: os.makedirs(path + album_name[i]) #以图集名创建文件夹 except OSError as e: print(e) continue print('正在下载第' + str(i + 1) + '个图册:' + album_name[i]) for j in range(len(album_id[i])): fileurl = 'https://photo.tuchong.com/' + str(author_id[i]) +'/f/' + str(album_id[i][j]) + '.jpg' #生成每张照片Url filename = path + album_name[i] + '/' + str(j+1) + '.jpg' #命名照片 print(' 正在下载第' + str(j+1) + '张照片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 #爬取页数 path = 'F:/少女/' #存放路径 for i in range(page): url = 'https://tuchong.com/rest/tags/少女/posts?page=' + str(i+1) + '&amp;count=20&amp;order=weekly' #tag的json地址 json_dict = get_json(url) album_name = get_album_name(json_dict) para = get_photo_id(json_dict) author_id = para[0] album_id = para[1] download_album(path,album_name,author_id,album_id) 按作者爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): #解析json header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_all_photo_id(json_dict): #获得所有照片的ID #post_id = [] photo_id = [] post_list = json_dict['post_list'] author_id = post_list[0]['author_id'] #获取作者ID author_name = post_list[0]['site']['name'] #获取作者姓名 # for i in range(len(post_list)): #获取所有图集ID # post_id.append(post_list[i]['post_id']) for i in range(len(post_list)): #获取每个图集的照片ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list return author_name,author_id,photo_iddef download_photo(path,author_id,photo_id): #下载全部照片 if not os.path.exists(path): os.makedirs(path) for i in range(len(photo_id)): filename = path + '/' + str(i+1) + '.jpg' fileurl = 'https://photo.tuchong.com/' + str(author_id) + '/f/' + str(photo_id[i]) + '.jpg' print(' 第' + str(i + 1) + '张图片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 for i in range(page): url = 'https://thomaskksj.tuchong.com/rest/2/sites/395013/posts?count=20&amp;page=' + str(i + 1) #作者主页的json地址 print('正在下载第' + str(i+1) + '页:' + url) json_dict = get_json(url) para = get_all_photo_id(json_dict) author_name = para[0] author_id = para[1] photo_id = para[2] path = 'F:/' + author_name + '/page' + str(i + 1) download_photo(path,author_id,photo_id)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP+回溯-单词拆分 II]]></title>
    <url>%2F2018%2F06%2F09%2FDP%2B%E5%9B%9E%E6%BA%AF-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20II%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，在字符串中增加空格来构建一个句子，使得句子中所有的单词都在词典中。返回所有这些可能的句子。说明：分隔时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入:s = “catsanddog”wordDict = [“cat”, “cats”, “and”, “sand”, “dog”]输出:[“cats and dog”,“cat sand dog”] 示例 2：输入:s = “pineapplepenapple”wordDict = [“apple”, “pen”, “applepen”, “pine”, “pineapple”]输出:[“pine apple pen apple”,“pineapple pen apple”,“pine applepen apple”]解释: 注意你可以重复使用字典中的单词。 示例 3：输入:s = “catsandog”wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出:[] 算法思路这道题类似 Word Break I 判断是否能把字符串拆分为字典里的单词 @LeetCode 只不过要求计算的并不仅仅是是否能拆分，而是要求出所有的拆分方案。因此用递归。但是直接递归做会超时，原因是LeetCode里有几个很长但是无法拆分的情况，所以就先跑一遍Word Break I，先判断能否拆分，然后再进行拆分。 DP实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 class Solution &#123; boolean isBreak(String s, List&lt;String&gt; wordDict) &#123; boolean[] canBreak = new boolean[s.length()+1]; canBreak[0] = true; for(int i=1; i&lt;=s.length(); i++) &#123; boolean flag = false; for(int j=0; j&lt;i; j++) &#123; if(canBreak[j] &amp;&amp; wordDict.contains(s.substring(j,i))) &#123; flag = true; break; &#125; &#125; canBreak[i] = flag; &#125; return canBreak[s.length()]; &#125; void dfs(String s, List&lt;String&gt; wordDict, String str, int index)&#123; String result = str; //记录字符串状态 int len = s.length(); String tmp = s.substring(index, len); if(wordDict.contains(tmp))&#123; //最后一段存在于字典中，则保存结果 str += tmp; res.add(str); &#125; List&lt;Integer&gt; listIndex = new ArrayList(); List&lt;String&gt; listStr = new ArrayList(); for(int i = index; i &lt; len; i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; listIndex.add(i+1); listStr.add(temp); &#125; &#125; String temp = result; //保存递归前的字符串状态，以便回溯 for(int i = 0; i &lt; listIndex.size(); i++)&#123; result += listStr.get(i) + " "; dfs(s, wordDict, result, listIndex.get(i)); result = temp; &#125; &#125; List&lt;String&gt; res = new ArrayList(); public List&lt;String&gt; wordBreak(String s, List&lt;String&gt; wordDict) &#123; if(!isBreak(s, wordDict)) return res; String str = ""; dfs(s, wordDict, str, 0); return res; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>动态规划</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查准率precision和查全率recall、F1]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9F%A5%E5%87%86%E7%8E%87precision%E5%92%8C%E6%9F%A5%E5%85%A8%E7%8E%87recall%E3%80%81F1%2F</url>
    <content type="text"><![CDATA[真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），分别用TP、FP、TN、FN表示相应样例数，样例总数=TP+FP+TN+FN；分类结果混淆矩阵： 真实情况 预测结果 正例 反例 正例 TP（真正例） FN（假反例） 反例 FP（假正例） TN（真反例） 查准率（precision）：被认定为正例的里面，判断正确的比例。 查全率（recall）：真实正例里，被判断出为正例的比例。 查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低,可以通过P-R曲线来取两者的平衡值 若一个学习器P-R曲线被另一个学习器的曲线完全”包住“,则可断言后者的性能优于前者， 例如图中学习器A 的性能优于学习器C; 如果两个学习器的P-R 曲线发生了交叉7,例如图中的A 与B ，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较然而，在很多情形下，人们往往仍希望把学习器A 与B 比出个高低. 这时一个比较合理的判据是比较P-R 曲线节面积的大小。 “平衡点“是”查准率=查全率“时的取值。 但更常用的使用F1来衡量查准率与查全率； F1基于查准率与查全率的调和平均： ，sum为样例总数， 具体应用中可能对P和R有不同的倚重。比如商品推荐中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，这时候查准率更重要。而在逃犯检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。 F1度量的一般形式F**β（加权调和平均**）就可以表达这种偏好。 即 ß = 1时退化为标准的F1,当β&gt;1意味着P占比重更大，反之则是R。 ROC、AUCROC:全称“受试者工作特征”，表达了模型的泛化能力。其纵坐标为“TPR真正例率”；横坐标为“FPR假正例率”。 ROC曲线根据模型的排序结果，一个个划分正负，每次得出两个值TPR,FPR。很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。为啥在ROC曲线画一个y=x呢，那表示随机猜测的ROC。 AUC:ROC曲线下的面积.AUC=1,是完美分类器(并不存在)；0.5。AUC 越大，意味着辨别能力越强。 与P-R图相似，如果一条ROC曲线包含另一条ROC曲线，则前者的学习器性能更优越。如果曲线有交叉，则可以通过计算AUC大小得到。 代价敏感错误率、代价曲线代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偏差、方差、噪声]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%81%8F%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。 偏差（Bias）和方差（Variance）偏差（Bias）：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。 方差（Variance）：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。 偏差：形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与指标； 方差：形容数据分散程度的，算是“无监督的”，客观的指标。 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度. 偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的. 给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小. 一般来说偏差与方差是有冲突的，这称为偏差—方差窘境，给定学习任务，假定我们能控制学习算法的训练程度（例如决策树可控制层数，神经网络可控制训练轮数，集成学习方法可控制基学习器个数），则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以便学习器产生显著变化，此时偏差主导了泛化错误率;随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率;在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合. 为什么KNN（k最近邻k-Nearest Neighbor）算法在增大k时，偏差会变大；但RF（Random Forest随机森林）增大树的数目时偏差却保持不变；GBDT（Gradient Boosting）在增大树的数目时偏差却又能变小。 对于KNN算法，k值越大，表示模型的学习能力越弱，因为k越大，它越倾向于从“面”上考虑做出判断，而不是具体地考虑一个样本近身的情况来做出判断，所以，它的偏差会越来越大。 对于RF，我们实际上是部分实现了多次训练取均值的效果，每次训练得到的树都是一个很强的学习者，每一个的方差都比较大，但综合起来就会比较小。好比一个很强的学习者学习时，刮着西风，它会据此调整自己的瞄准方法，另一个很强的学习者学习时刮着东风，（西风、东风可以理解为不同训练集中的噪声）它也会据此调整自己的瞄准方法，在测试样本时，一个误差向西，一个误差向东，刚好起到互相抵消的作用，所以方差会比较小。但是由于每棵树的偏差都差不多，所以，我们取平均时，偏差不会怎么变化。 为什么说是部分实现了多次训练取均值的效果而不是全部呢？因为我们在训练各棵树时，是通过抽样样本集来实现多次训练的，不同的训练集中不可避免地会有重合的情况，此时，就不能认为是独立的多次训练了，各个训练得到的树之间的方差会产生一定的相关性，训练集中重合的样本越多，则两棵树之间的方差的相关性越强，就越难达成方差互相抵消的效果。 对于GBDT，N棵树之间根本就不是一种多次训练取均值的关系，而是N棵树组成了相关关联，层层递进的超级学习者，可想而知，它的方差一定是比较大的。但由于它的学习能力比较强，所以，它的偏差是很小的，而且树的棵树越多，学习能力就越强，偏差就越小。也就是说，只要学习次数够多，预测的均值会无限接近于目标。简单讲就是GBDT的N棵树实际上是一个有机关联的模型，不能认为是N个模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子串-最长公共子序列]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[最长公共子串DP实现1234567891011121314151617181920212223242526272829public static int maxSubStr(String str1, String str2) &#123; int result = 0; int index = 0; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int [len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; dp[i][j] = dp[i-1][j-1] + 1; // 状态转移 if(dp[i][j] &gt; result) &#123; result = dp[i][j]; index = i; // 记录最大子串的最后一个下标 &#125; // result = result &gt; dp[i][j] ? result : dp[i][j]; &#125;else &#123; dp[i][j] = 1; result = result &gt; dp[i][j] ? result : dp[i][j]; &#125; &#125; &#125; &#125; System.out.println(result); for(int i = index - result + 1; i &lt;= index; i++) &#123; System.out.print(str1.charAt(i) + " "); &#125; return result;&#125; 最长公共子序列####动态规划假设Z=&lt;z1,z2,⋯,zk&gt;是X与Y的LCS， 我们观察到如果Xm=Yn，则Zk=Xm=Yn，有Zk−1是Xm−1与Yn−1的LCS；如果Xm≠Yn，则Zk是Xm与Yn−1的LCS，或者是Xm−1与Yn的LCS。因此，求解LCS的问题则变成递归求解的两个子问题。但是，上述的递归求解的办法中，重复的子问题多，效率低下。改进的办法——用空间换时间，用数组保存中间状态，方便后面的计算。这就是动态规划（DP)的核心思想了。DP求解LCS用二维数组c[i][j]记录串x1x2⋯xi与y1y2⋯yj的LCS长度，则可得到状态转移方程 DP实现1234567891011121314151617181920public static int maxSubSequence(String str1, String str2) &#123; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int[len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else &#123; dp[i][j] = dp[i-1][j] &gt; dp[i][j-1] ? dp[i-1][j] : dp[i][j-1]; &#125; &#125;else if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = 1; &#125; &#125; &#125; System.out.println(dp[len1 - 1][len2 - 1]); return dp[len1-1][len2-1];&#125;]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub搭建个人博客]]></title>
    <url>%2F2018%2F06%2F09%2FHexo%2BGitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hexo+GitHub搭建个人博客目录： 前言： ​ 准备工作 搭建github博客 ​ 创建仓库 ​ 绑定域名 配置SSH key ​ 测试是否成功 使用hexo写博客 ​ hexo简介 ​ 原理 注意事项 安装 初始化 ​ 修改主题 ​ 上传到github 保留CNAME、README.md等文件 常用hexo命令 ​ _config.yml 写博客 最终效果 https://dadavision.cn/ 技巧 1.前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台； 等等； 1.1. 准备工作在开始一切之前，你必须已经： 有一个github账号，没有的话去注册一个； 安装了node.js、npm，并了解相关基础知识； 安装了git for windows（或者其它git客户端） 本文所使用的环境： Windows10企业版 node.js@8.11.2 git@2.17.1 hexo@3.7.1 2.搭建github博客2.1. 创建仓库新建一个名为你的用户名.github.io的仓库，比如说，如果你的github用户名是test，那么你就新建test.github.io的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是 http://test.github.io 了，是不是很方便？ 由此可见，每一个github账户最多只能创建一个这样可以直接使用域名访问的仓库。 几个注意的地方： 注册的邮箱一定要验证，否则不会成功； 仓库名字必须是：username.github.io，其中username是你的用户名； 仓库创建成功不会立即生效，需要过一段时间，大概10-30分钟，或者更久，我的等了半个小时才生效； 创建成功后，默认会在你这个仓库里生成一些示例页面，以后你的网站所有代码都是放在这个仓库里啦。 2.2. 绑定域名当然，你不绑定域名肯定也是可以的，就用默认的 xxx.github.io 来访问，如果你想更个性一点，想拥有一个属于自己的域名，那也是OK的。 首先你要注册一个域名，域名注册以前总是推荐去godaddy，现在觉得其实国内的阿里云、腾讯云也挺不错的，价格也不贵，毕竟是大公司，放心！或者说万网，本质上都是在万网上。 绑定域名分2种情况：带www和不带www的。 域名配置最常见有2种方式，CNAME和A记录，CNAME填写域名，A记录填写IP，由于不带www方式只能采用A记录，所以必须先ping一下你的用户名.github.io的IP，然后到你的域名DNS设置页，将A记录指向你ping出来的IP，将CNAME指向你的用户名.github.io，这样可以保证无论是否添加www都可以访问，如下： 然后到你的github项目根目录新建一个名为CNAME的文件（无后缀），里面填写你的域名，加不加www看你自己喜好，因为经测试： 如果你填写的是没有www的，比如 mygit.me，那么无论是访问 http://www.mygit.me 还是 http://mygit.me ，都会自动跳转到 http://mygit.me 如果你填写的是带www的，比如 www.mygit.me ，那么无论是访问 http://www.mygit.me 还是 http://mygit.me ，都会自动跳转到 http://www.mygit.me 如果你填写的是其它子域名，比如 abc.mygit.me，那么访问 http://abc.mygit.me 没问题，但是访问 http://mygit.me ，不会自动跳转到 http://abc.mygit.me 另外说一句，在你绑定了新域名之后，原来的你的用户名.github.io并没有失效，而是会自动跳转到你的新域名。 3.配置SSH key为什么要配置这个呢？因为你提交代码肯定要拥有你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。 1$ cd ~/. ssh #检查本机已存在的ssh密钥 如果提示：No such file or directory 说明你是第一次使用git。 1ssh-keygen -t rsa -C &quot;邮件地址&quot; 然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到.ssh\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key： 将刚复制的内容粘贴到key那里，title随便填，保存。 3.1. 测试是否成功1$ ssh -T git@github.com # 注意邮箱地址不用改 如果提示Are you sure you want to continue connecting (yes/no)?，输入yes，然后会看到： Hi liuxianan! You’ve successfully authenticated, but GitHub does not provide shell access. 看到这个信息说明SSH已配置成功！ 此时你还需要配置： 12$ git config --global user.name &quot;dadavision&quot;// 你的github用户名，非昵称$ git config --global user.email &quot;dadavision@qq.com&quot;// 填写你的github注册邮箱 具体这个配置是干嘛的我没仔细深究。 4.使用hexo写博客4.1. hexo简介Hexo是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。 官网： http://hexo.iogithub: https://github.com/hexojs/hexo 4.2. 原理由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 4.3. 注意事项安装之前先来说几个注意事项： 很多命令既可以用Windows的cmd来完成，也可以使用git bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行； hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导； hexo有2种_config.yml文件，一个是根目录下的全局的_config.yml，一个是各个theme下的； 4.4. 安装1$ npm install -g hexo 4.5. 初始化在电脑的某个地方新建一个名为hexo的文件夹（名字可以随便取），比如我的是F:\Workspaces\hexo，由于这个文件夹将来就作为你存放代码的地方，所以最好不要随便放。 12$ cd /f/GitBlog/hexo/$ hexo init hexo会自动下载一些文件到这个目录，包括node_modules，目录结构如下图： 这里有个’更新博客’，这里涉及一个技巧，后面讲。 12$ hexo g # 生成$ hexo s # 启动服务 执行以上命令之后，hexo就会在public文件夹生成相关html文件，这些文件将来都是要提交到github去的： hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容，很多人会碰到浏览器一直在转圈但是就是加载不出来的问题，一般情况下是因为端口占用的缘故，因为4000这个端口太常见了，可以用hexo clean。 第一次初始化的时候hexo已经帮我们写了一篇名为 Hello World 的文章，默认的主题比较丑。 4.6. 修改主题既然默认主题很丑，那我们别的不做，首先来替换一个好看点的主题。这是 官方主题。 个人比较喜欢的2个主题：hexo-theme-jekyll 和 hexo-theme-yilia。 首先下载这个主题： 12$ cd /f/Workspaces/hexo/$ git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 下载后的主题都在这里： 另外一种更方便的方式是： 我用的next主题。 直接从官网github上，clone down下来，解压后直接放进文件夹更快。 修改_config.yml中的theme: landscape改为theme: next，然后重新执行hexo g来重新生成。 如果出现一些莫名其妙的问题，可以先执行hexo clean来清理一下public的内容，然后再来重新生成和发布。 4.7. 上传之前在上传代码到github之前，一定要记得先把你以前所有代码下载下来（虽然github有版本管理，但备份一下总是好的），因为从hexo提交代码时会把你以前的所有代码都删掉。 4.8. 上传到github如果你一切都配置好了，发布上传很容易，一句hexo d就搞定，当然关键还是你要把所有东西配置好。 首先，ssh key肯定要配置好。 其次，配置_config.yml中有关deploy的部分： 正确写法： 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:DaDaVision/DaDaVision.github.io.git branch: master 错误写法： 1234deploy: type: github repository: https://github.com:DaDaVision/DaDaVision.github.io.git branch: master 后面一种写法是hexo2.x的写法，现在已经不行了，无论是哪种写法，此时直接执行hexo d的话一般会报如下错误： 1Deployer not found: github 或者 Deployer not found: git 原因是还需要安装一个插件： 1npm install hexo-deployer-git --save 其它命令不确定，部署这个命令一定要用git bash，否则会提示Permission denied (publickey). 打开你的git bash，输入hexo d就会将本次有改动的代码全部提交，没有改动的不会. 4.9. 保留CNAME、README.md等文件提交之后网页上一看，发现以前其它代码都没了，此时不要慌，一些非md文件可以把他们放到source文件夹下，这里的所有文件都会原样复制（除了md文件）到public目录的： 由于hexo默认会把所有md文件都转换成html，包括README.md，所有需要每次生成之后、上传之前，手动将README.md复制到public目录，并删除README.html。 这里需要注意： 在添加绑定域名后需要注意以下问题（未绑定可以不用） 需要将github上CNAME里面内容在文件工程中GitBlog\source\CNAME 这里注意新建的CNAME不要有后缀名。 4.10. 常用hexo命令常见命令 1234567hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 缩写： 1234hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令： 12hexo s -g #生成并本地预览hexo d -g #生成并上传 4.11. _config.yml这里面都是一些全局配置，每个参数的意思都比较简单明了，所以就不作详细介绍了。 需要特别注意的地方是，冒号后面必须有一个空格，否则可能会出问题。 4.12. 写博客定位到我们的hexo根目录，执行命令： 1hexo new &apos;哈喽&apos; hexo会帮我们在_posts下生成相关md文件，我们只需要打开这个文件就可以开始写博客了，默认生成如下内容： 当然你也可以直接自己新建md文件，用这个命令的好处是帮我们自动生成了时间。 一般完整格式如下： 123456789---title: postName #文章页面上的显示名称，一般是中文date: 2018-7-02 15:30:16 #文章生成时间，一般不改，当然也可以任意修改categories: 默认分类 #分类tags: [tag1,tag2,tag3] #文章标签，可空，多标签请用格式，注意:后面有个空格description: 附加一段文章摘要，字数最好在140字以内，会出现在meta的description里面---以下是正文 那么hexo new page &#39;postName&#39;命令和hexo new &#39;postName&#39;有什么区别呢？ 1hexo new page &quot;my-second-blog&quot; 生成如下： 最终部署时生成：GitBlog\public\my-second-blog\index.html，但是它不会作为文章出现在博文目录。 4.12.1. 写博客工具那么用什么工具写博客呢？ 推荐用Typora和Hbuilder X。 4.12.2. 如何让博文列表不显示全部内容默认情况下，生成的博文目录会显示全部的文章内容，如何设置文章摘要的长度呢？ 答案是在合适的位置加上&lt;!--more--&gt;即可。 5.最终效果可以访问我的git博客来查看效果：https://dadavision.cn 6.技巧其中生成，上传github有很多命令，以下方法可以减少繁琐，就是创建.bat批处理文件： 123f:cd GitBloghexo clean &amp; hexo g &amp;&amp; gulp &amp; hexo d &amp; hexo s 还可以设置全局快捷键：]]></content>
      <categories>
        <category>个人博客</category>
      </categories>
      <tags>
        <tag>个人博客</tag>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>next</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分糖果]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%88%86%E7%B3%96%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[题目描述o 分糖果：科大讯飞第二道编程题 o 小明和小红是好朋友，但最近遇到一个棘手的问题，有一盒糖果要分成两份但是每颗糖果质量都不尽相同， o 但为了分配的公平每份糖的糖果数量相差不得超过1，在此条件下两份糖果的质量差距尽可能小。 o 输入一行数，包含一个数n，代表糖果数量，后面一次是n个整数一次表示每个糖果的质量，每个糖果的质量都是1到450 o 之间的一个整数，每盒最多有20个糖果。 o 输出：每个样例输出两个数字分别为两堆糖果的质量，如不相同，先小后大。 o 样例：输入：5 9 6 5 8 7 o 输出：17 18 算法思想o 回溯，在数量差值为1的结果中找出最小的质量差 Python实现1234567891011121314151617def divide(candies, num, select, sum, total, index): global min global res if(abs(total-sum*2) &lt; min): res = sum min = abs(total-sum*2) result.append(select) for i in range(len(candies)): if(index == num-1): return3 select.append(candies[index]) sum += candies[index] temp = select.copy() if(len(select) &lt;= int(num/2)+1): index += 1 divide(candies, num, temp, sum, total, index) sum -= select[len(select) - 1] select.remove(select[len(select)-1])]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回溯法思想]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[子集树与排列树当所给问题是从n个元素的集合S中找出满足某种性质的子集时，解空间为子集树。例如：0-1背包问题 (选或不选问题)当所给问题是从n个元素的集合S中找出满足某种性质的排列时，解空间为排列树。例如：旅行售货员问题（选择顺序问题） 算法结构 深度优先搜索与广度优先搜索算法有何区别深度优先搜索法不全部保留结点，扩展完的结点从数据存储结构栈中弹出删去，在栈中存储的结点数就是解空间树的深度，因此它占用空间较少。所以，当搜索树的结点较多，用其它方法易产生内存溢出时，深度优先搜索不失为一种有效的求解方法。广度优先搜索算法，一般需存储产生所有结点，占用的存储空间要比深度优先搜索大得多，因此，程序设计中，必须考虑溢出和节省内存空间的问题。但广度优先搜索法一般无回溯操作（即入栈和出栈的操作），所以运行速度比深度优先搜索要快些。 回溯与分支限界区别回溯法以深度优先的方式搜索解空间树T，而分支限界法则以广度优先或以最小耗费优先的方式搜索解空间树T。它们在问题的解空间树T上搜索的方法不同，适合解决的问题也就不同。一般情况下，回溯法的求解目标是找出T中满足约束条件的所有解的方案，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。相对而言，分支限界算法的解空间比回溯法大得多，因此当内存容量有限时，回溯法成功的可能性更大。 最优化处理问题在处理最优问题时，采用穷举法、回溯法或分支限界法都可以通过利用当前最优解和上界函数加速。仅就对限界剪支的效率而言，优先队列的分支限界法显然要更充分一些。在穷举法中通过上界函数与当前情况下函数值的比较可以直接略过不合要求的情况而省去了更进一步的枚举和判断；回溯法则因为层次的划分，可以在上界函数值小于当前最优解时，剪去以该结点为根的子树，也就是节省了搜索范围；分支限界法在这方面除了可以做到回溯法能做到的之外，同时若采用优先队列的分支限界法，用上界函数作为活结点的优先级，一旦有叶结点成为当前扩展结点，就意味着该叶结点所对应的解即为最优解，可以立即终止其余的过程。在前面的例题中曾说明，优先队列的分支限界法更象是有选择、有目的地进行搜索，时间效率、空间效率都是比较高的。 算法总结一个问题是该用递推、贪心、搜索还是动态规划，完全是由这个问题本身阶段间状态的转移方式决定的！每个阶段只有一个状态-&gt;递推；每个阶段的最优状态都是由上一个阶段的最优状态得到的-&gt;贪心；每个阶段的最优状态是由之前所有阶段的状态的组合得到的-&gt;搜索；每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到而不管之前这个状态是如何得到的-&gt;动态规划。 动态规划1.求最优解问题2.整体问题的最优解依赖于各个子问题的最优解3.把大问题分解成小问题，小问题之间还有相互重叠的更小的子问题4.从上往下分析，从下往上求解，避免重复求解小问题]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
