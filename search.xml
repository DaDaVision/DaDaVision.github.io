<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[协方差与相关系数]]></title>
    <url>%2F2018%2F08%2F25%2F%E5%8D%8F%E6%96%B9%E5%B7%AE%E4%B8%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[什么是协方差（Covariance）？ 协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。 如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。 以上是某百科的解释。 1、协方差是怎么来的？ ​ 简单地来说，协方差就是反映两个变量 X 和 Y 的相互关系。这种相互关系大致分为三种：正相关、负相关、不相关。 ​ 什么是正相关呢？例如房屋面积（X）越大，房屋总价（Y）越高，则房屋面积与房屋总价是正相关的； ​ 什么是负相关呢？例如一个学生打游戏的时间（X）越多，学习成绩（Y）越差，则打游戏时间与学习成绩是负相关的； ​ 什么是不相关呢？例如一个人皮肤的黑白程度（X）与他的身体健康程度（Y）并无明显关系，所以是不相关的。 我们先来看第一种情况，令变量 X 和变量 Y 分别为： X = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30] Y = [12 15 17 21 22 21 18 23 26 25 22 28 24 28 30 33 28 34 36 35] 在坐标上描绘出 X 和 Y 的联合分布： 显然，Y 在整体趋势上是随着 X 的增加而增加的，即 Y 与 X 的变化是同向的。这种情况，我们就称 X 与 Y 是正相关的。 我们再来看第二种情况，令变量 X 和变量 Y 分别为： X = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30] Y = [35 35 29 29 28 28 27 26 26 23 21 22 25 19 16 19 20 16 15 16] 在坐标上描绘出 X 和 Y 的联合分布： 显然，Y 在整体趋势上是随着 X 的增加而减少的，即 Y 与 X 的变化是反向的。这种情况，我们就称 X 与 Y 是负相关的。 我们再来看第三种情况，令变量 X 和变量 Y 分别为： X = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30] Y = [16 16 28 17 20 26 20 17 21 15 12 29 24 25 16 15 21 13 17 25] 在坐标上描绘出 X 和 Y 的联合分布： 显然，Y 在整体趋势上与 X 的并无正相关或者负相关的关系。这种情况，我们就称 X 与 Y 是不相关的。 回过头来，我们来看 X 与 Y 正相关的情况，令 EX、EY 分别是 X 和 Y 的期望值。什么是期望呢？在这里我们可以把它看成是平均值，即 EX 是变量 X 的平均值，EY 是变量 Y 的平均值。把 EX 和 EY 在图中表示出来得到下面的图形： 上图中，整个区域被 EX 和 EY 分割成 I、II、III、IV 四个区域，且 X 和 Y 大部分分布在 I、III 区域内，只有少部分分布在 II、IV 区域内。 在区域 I 中，满足 X&gt;EX，Y&gt;EY，则有 (X-EX)(Y-EY)&gt;0； 在区域 II 中，满足 X&lt;EX，Y&gt;EY，则有 (X-EX)(Y-EY)&lt;0； 在区域 III 中，满足 X&lt;EX，Y&lt;EY，则有 (X-EX)(Y-EY)&gt;0； 在区域 IV 中，满足 X&gt;EX，Y&lt;EY，则有 (X-EX)(Y-EY)&lt;0。 显然，在区域 I、III 中，(X-EX)(Y-EY)&gt;0；在区域 II、IV 中，(X-EX)(Y-EY)&lt;0。而 X 和 Y 正相关时，数据大部分是分布在 I、III 区域内，只有少部分分布在 II、IV 区域。因此，从平均角度来看，正相关满足： $E(X-EX)(Y-EY)&gt;0$ 上式表示的是 (X-EX)(Y-EY) 的期望大于零，即 (X-EX)(Y-EY) 的平均值大于零。 然后，再来看 X 和 Y 负相关的情况： 上图中，X 和 Y 大部分分布在 II、IV 区域内，只有少部分分布在 I、III 区域内。 同样，在区域 I、III 中，(X-EX)(Y-EY)&gt;0；在区域 II、IV 中，(X-EX)(Y-EY)&lt;0。而 X 和 Y 负相关时，数据大部分是分布在 II、IV 区域内，只有少部分分布在 I、III 区域。因此，从平均角度来看，负相关满足： $E(X-EX)(Y-EY)&lt;0$ 上式表示的是 (X-EX)(Y-EY) 的期望小于零，即 (X-EX)(Y-EY) 的平均值小于零。 最后，再来看 X 和 Y 不相关的情况： 上图中，X 和 Y 在 I、II、III、IV 区域内近似均匀分布。 同样，在区域 I、III 中，(X-EX)(Y-EY)&gt;0；在区域 II、IV 中，(X-EX)(Y-EY)&lt;0。而 X 和 Y 不相关时，数据在各区域内均匀分布，从平均角度来看，不相关满足： $E(X-EX)(Y-EY)=0$ 上式表示的是 (X-EX)(Y-EY) 的期望等于零，即 (X-EX)(Y-EY) 的平均值等于零。 综上所述，我们得到以下结论： 当 X 和 Y 正相关时：$E(X-EX)(Y-EY)&gt;0$ 当 X 和 Y 负相关时： $E(X-EX)(Y-EY)&lt;0$ 当 X 和 Y 不相关时： $E(X-EX)(Y-EY)=0$ 因此，我们就引出了协方差的概念，它是表示 X 和 Y 之间相互关系的数字特征。我们定义协方差为： $Cov(X,Y)=E(X-EX)(Y-EY)$ 根据之前讨论的结果， 当 Cov(X,Y) &gt; 0 时，X 与 Y 正相关； 当 Cov(X,Y) &lt; 0 时，X 与 Y 负相关； 当 Cov(X,Y) = 0 时，X 与 Y 不相关。 值得一提的是，E 代表求期望值。也可以用平均值来计算协方差： $Cov(X,Y)=\frac{1}{N-1}\sum^{N}{i=1}(X{i}-\overline{X_{i}})(Y_{i}-\overline{Y_{i}})$ 这里，之所以除以 N-1 而不是 N 的原因是对总体样本期望的无偏估计。顺便提一下，如果令 Y = X，则协方差表示的正是 X 的方差。 下面，我们根据协方差的公式，分别计算上面三种情况下 X 与 Y 的协方差。 X 与 Y 正相关时，Cov(X,Y) = 37.3684； X 与 Y 负相关时，Cov(X,Y) = -34.0789； X 与 Y 不相关时，Cov(X,Y) = -1.0263。 2、相关系数与协方差有什么关系？ 我们已经知道了什么是协方差以及协方差公式是怎么来的，如果知道两个变量 X 与 Y 的协方差与零的关系，我们就能推断出 X 与 Y 是正相关、负相关还是不相关。那么有一个问题：协方差数值大小是否代表了相关程度呢？也就是说如果协方差为 100 是否一定比协方差为 10 的正相关性强呢？ 请看下面这个例子！ 变量 X1 与 Y1 分别为： X1 = [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30] Y1 = [12 12 13 15 16 16 17 19 21 22 22 23 23 26 25 28 29 29 31 32] 变量 X2 和 Y2 分别为： X2 = [110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300] Y2 = [113 172 202 206 180 184 242 180 256 209 288 255 240 278 319 322 345 289 333 372] X1、Y1 和 X2、Y2 分别联合分布图，如下所示： 显然，从图中可以看出，X1、Y1 和 X2、Y2 都呈正相关，而且 X1 与 Y1 正相关的程度明显比 X2 与 Y2 更大一些。接下来，我们计算两幅图的协方差看看是不是这样。 Cov(X1,Y1) = 37.5526 Cov(X2,Y2) = 3730.26 意外！X2 与 Y2 的协方差竟然比 X1 与 Y1 的协方差还大 100 倍。看来并不是协方差越大，正相关程度越高。这到底是为什么呢？ 其实，出现这种情况的原因是两种情况数值变化的幅值不同（或者量纲不同）。计算协方差的时候我们并没有把不同变量幅值差异性考虑进来，在比较协方差的时候也就没有一个统一的量纲标准。 所以，为了消除这一影响，为了准确得到变量之间的相似程度，我们需要把协方差除以各自变量的标准差。这样就得到了相关系数的表达式： $\rho=\frac{Cov(X,Y)}{\sigma X\sigma Y}$ 可见，相关系数就是在协方差的基础上除以变量 X 和 Y 的标准差。其中标准差的计算公式为： $\sigma X = \sqrt{\frac{1}{N-1}\sum^{N}{i=1}(X{i}-\overline{X_{i}})}$ $\sigma Y = \sqrt{\frac{1}{N-1}\sum^{N}{i=1}(Y{i}-\overline{Y_{i}})}$ ​ 为什么除以各自变量的标准差就能消除幅值影响呢？这是因为标准差本身反映了变量的幅值变化程度，除以标准差正好能起到抵消的作用，让协方差标准化。这样，相关系数的范围就被归一化到 [-1,1] 之间了。 下面，我们就来分别计算上面这个例子中 X1、Y1 和 X2、Y2 的相关系数。 ρ(X1,Y1) = 0.9939 ρ(X2,Y2) = 0.9180 好了，我们得到 X1 与 Y1 的相关系数大于 X2 与 Y2 的相关系数。这符合实际情况。也就是说，根据相关系数，我们就能判定两个变量的相关程度，得到以下结论： 相关系数大于零，则表示两个变量正相关，且相关系数越大，正相关性越高； 相关系数小于零，则表示两个变量负相关，且相关系数越小，负相关性越高； 相关系数等于零，则表示两个变量不相关。 ​ 回过头来看一下协方差与相关系数的关系，其实，相关系数是协方差的标准化、归一化形式，消除了量纲、幅值变化不一的影响。实际应用中，在比较不同变量之间相关性时，使用相关系数更为科学和准确。但是协方差在机器学习的很多领域都有应用，而且非常重要！]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>机器学习</tag>
        <tag>协方差</tag>
        <tag>相关系数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepIOT神经网络压缩]]></title>
    <url>%2F2018%2F08%2F24%2FDeepIOT%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework Shuochao YaoUniversity of Illinois Urbana ChampaignYiran ZhaoUniversity of Illinois Urbana ChampaignAston ZhangUniversity of Illinois Urbana ChampaignLu SuState University of New York at BualoTarek AbdelzaherUniversity of Illinois Urbana Champaign ​ 物联网在我们的日常感知场景中越来越受欢迎，有很多应用，比如智能家居/城市，在我们的日常生活中有大量的嵌入式/移动设备，下一个自然的步骤是构建能够集体学习的智能感知系统 。另一方面，DL最近在复杂的传感和识别任务上取得了很大进展 。 ​ 然而,挑战仍然存在,当我们部署先进的dl模型在低端很多设备参考这个数字从一篇分析先进的图像识别cnn模型,这些模型通常需要大约10 Giga-operations和100 mb的大小会导致记忆的问题,时间,能量消耗物联网设备上部署这些模型时。 ​ 深度学习的最新进展激发了在传感应用中使用深度中立网络的动力，但是在受限的嵌入式设备上的过度的资源需求仍然是一个重要的障碍。最近探索的解决方案空间是在设备使用前，以某种方式压缩(近似或简化)深层神经网络。一种新的压缩解决方案，叫做DeepIoT，它在这个空间中做出了两个关键的贡献。 首先，与目前针对压缩特殊性型神经网络的解决方案不同的是，DeepIoT提出了一种统一普适的方法，它压缩了所有常用的用于传感应用的深度学习结构，包括完全连接的、卷积的和递归的神经网络，以及它们的组合。 其次，与稀疏加权矩阵或假定加权矩阵内的线性结构的解决方案不同，DeepIoT通过寻找最少数量的非冗余隐藏元素（例如每层所需的滤波器和维度）将神经网络结构压缩为更小的密集矩阵，同时保留传感应用的性能相同。即不降低准确率。重要的是，它使用一种方法来获得参数冗余的全局视图，该方法被证明可以产生更好的压缩。 由DeepIoT生成的压缩模型可以直接使用现有的深度学习库，这些库在嵌入式和移动系统上运行，无需进一步的修改。结果表明了深度神经网络在资源受限的嵌入式设备上的开发潜力。 一、已有方法介绍 ​ 1、大密度参数矩阵——&gt;稀疏矩阵 ​ 有研究试图解决这个问题，现有的最先进的压缩算法是一种基于量级的修剪方法如上所示，该算法查看权重矩阵，并迭代地删除小于3列的实体。最终，它将把这个大而密集的矩阵转换成一个大而稀疏的矩阵。通常这种方法只能将参数的数量减少到10个。 基于幅值 1、基于幅度的fine-tuning压缩方法。 ​ 该方法是迭代地去除低幅度的重量连接;然而，它需要更多的资源消耗实现稀疏矩阵。 2、剪枝 ​ 积极的修剪方法增加了不可挽回的网络损害的潜在风险。虽然有一种连接拼接的压缩算法，为一定的阈值提供了恢复的机会然而，该算法仍然侧重于权重水平而不是结构水平。 基于因子分解 除了基于幅度的方法之外，另一系列作品着重于基于因子分解的方法，其通过利用参数中的低秩结构来降低神经网络的复杂性。 3、利用各种矩阵分解方法和fine-tunning近似卷积运算，以减少神经网络的执行时间。 4、应用稀疏编码和基于矩阵分解的方法分别降低全连接层和卷积层的复杂度。 ​ 主要原因是稀疏矩阵实现效率低。一方面，稀疏矩阵需要记录，这导致了2倍的内存消耗。另一方面，稀疏矩阵相乘很难优化 。 ​ 然而，基于分解的方法与基于量值的方法相比通常获得较低的压缩比，并且低等级假设可能损害最终网络性能。 5、应用频域信息进行模型压缩。然而，需要额外的实现来加速频域表示，并且该方法不适用于具有小卷积滤波器大小的现代CNN。 6、Hinton等人提出了一个teacher-student框架，将整个模型中的知识提炼成一个模型。然而，框架更侧重于将模型集合压缩为单一模型而不是结构压缩。 二、系统介绍 ​ DeepIoT通过决定每层中最小元素的数量来压缩常用的深度神经网络结构以用于传感应用。 先前关于神经网络压缩的照明研究使疏密密集将参数矩阵转换为大型稀疏矩阵。 相比之下，DeepIoT最大限度地减少了每层中的元素数量，从而将参数转化为一组小密集矩阵。小密集矩阵不需要为元素索引额外存储，并且对处理进行了有效优化。 ​ DeepIoT借用了广泛使用的被称为Dropout的深度学习正则化方法隐藏元素的想法。 Dropout操作给每个隐藏元素一个Dropout概率。 在dropout过程中，隐藏元素可根据其dropout概率进行修剪。 可以生成“细化”的网络结构。 但是，这些dropout概率通常设置为预定义值，例如0.5。 这种预定义的值不是最优的概率，从而导致对解空间的有效探索较少。如果我们能够获得每个隐藏元素的最优dropout概率，那么我们就有可能生成 最优的薄网络结构，在保留了传感应用的精度，同时最大限度地降低了传感系统的资源消耗。DeepIoT的一个重要目的是为神经网络中的每个隐藏元素找到最优的dropout概率。 ​ 注意，Dropout可以很容易地应用于所有常用的神经网络结构。 ​ 1、在全连接的神经网络中，神经元在每一层都被丢弃; 直接删除隐藏的元素/神经元，而不是权重。最后，我们将一个大的稠密权矩阵转化为一个细网络结构的小密度权矩阵。因此，我们可以绕过稀疏矩阵的表示 . 一开始没有最优压缩决策，所以前期是计算梯度变化，抑制dropout概率，并逐渐压缩，而不是直接删除。 ​ 2、在卷积神经网络中，卷积核被丢弃在每一层; ​ 对于CNN的每一层，都有一组卷积核来学习输入图像的特征。DeepIoT尝试删除冗余卷积核，以减少CNN层的卷积核数量。 ​ 3、并且在递归神经网络中，减小每层的维度。 对于RNN层，DeepIoT直接降低了RNN的维度，用于模型压缩。 ​ 意味着DeepIoT可以应用于所有常用的神经网络结构及其组合。 ​ DeepIoT的贡献在于利用新颖的压缩器神经网络将每个层的模型参数作为输入，学习参数冗余，并相应地生成Dropout概率。由于不同层之间存在参数互联，因此我们设计压缩神经网络作为递归神经网络，能够全局共享冗余信息并逐层生成dropout概率。 压缩机神经网络与原神经网络协同优化，通过compressor-critic框架进行压缩，尽量减小原始传感应用程序的损失功能。actor-critic框架模拟了著名的基于增强学习的actor-critic算法的思想，以迭代的方式优化了两个网络。 评估实验。 1、首先由三个任务组成，这三个任务使嵌入式系统能够以基本模式（包括手写文本，视觉和语音）与人类进行交互，与其他类似尺寸的神经网络相比，我们生成的神经网络具有更高的准确性。 2、第二组提供了两个应用压缩神经网络来解决以人为中心的情境感知任务的例子;即资源受限情景中的人类活动识别和用户识别。 DeepIOT特性： 由于压缩过程是迭代进行的，必须具有恢复能力以提高最终性能。 以前的方法主要依赖于一些特定的压缩度量，我们希望DeepIoT基于参数冗余进行压缩。 深度神经网络是一种深层结构，因此深度神经网络应该具有全局性，必须了解全局冗余度。 上图我们使用一个压缩3层全连接神经网络结构的例子来说明DeepIoT的整个流程。 （绿色方框表示原始神经网络的参数。橙色代表dropout操作）所示。使用DeepIoT压缩传感应用的神经网络结构的基本步骤可概括如下： （1） 插入操作。将概率$p^{(l)}$随机置零的隐藏元素(图1中的红色方框)插入到原始神经网络的内部层中。 （2） 构建压缩神经网络。它将图层中的原始神经网络权重矩阵$W^{l}​$ （图1中的绿框）作为输入进行压缩，在不同层之间学习和共享参数冗余，并生成最优dropout概率$p^{(l)}​$，然后送回到原始神经网络中的dropout操作。 （3） 用compressor-critic框架对压缩神经网络和原始神经网络进行迭代优化。 对压缩神经网络进行迭代优化，使其产生更好的退出概率，从而为原神经网络生成更有效的网络结构。 ​ ​ 学习参数冗余 （橙色） ​ 基于参数冗余的DeepIOT。在理想的情况下，DeepIoT可以学习一个带参数的最优压缩机$\mu$，函数从神经网络中获取权重矩阵W。学习参数冗余，并生成可用于压缩的隐藏神经元的最优Dropout概率 。 ​ 参数冗余的全局视图 ​ 但我们如何设计这个最优压缩函数与参数冗余的全局视图。我们知道深神经网络本身就是完美函数逼近器。只要有足够的容量，它就可以逼近所有已有的连续函数。因此，我们将压缩函数设计成一种递归神经网络。它从原始神经网络中逐层提取权重矩阵，了解全局冗余信息，生成压缩dropout概率。这里我们称压缩函数、压缩器和要压缩的原始神经网络为“critic”，因为critic向压缩器提供反馈，让它知道根据“监督”得出的dropout概率是好是坏 。 Compressor-Critic Framework ​ DeepIoT的目标函数是根据compressor学习到的损失概率来学习压缩模型的期望损失。我们反复训练，这意味着当训练compressor时，我们把所有的东西都固定在critic，反之亦然 。 ​ 对原神经网络和压缩神经网络进行迭代优化，使压缩神经网络以软删除的方式逐渐压缩原神经网络。 ​ 我们把最初的神经网络称为$F_{W}(x|z)$，我们称之为Critic。它将x作为输入，并基于二进制dropout 记为Z和模型参数w生成预测，这些参数w指的是一组权重$W=w^{(l)}$。 。我们假设$F_{W}(x|z)$是一个预先训练好的模型。$z~\mu_{\phi}(W)$表示压缩神经网络。它将Critic的权重作为输入，并根据其自身参数生成mask向量Z的概率分布。为了优化压缩器，使其在Critic中去掉隐藏元素，DeepIoT遵循了上图的目标函数。 ​ 其中$l$是Critic的目标函数。目标函数可以解释为原始神经网络的期望损失乘以压缩机产生的dropout概率。 ​ DeeploT以迭代的方式优化Compressor和Critic。通过对Compressor采用梯度下降法，迭代求解，降低了目标函数中定义的期望损失。如何去做？由于存在离散采样操作，在计算图中，反向传播不直接适用。因此，我们使用无偏似然比估计量来计算。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>笔记</tag>
        <tag>神经网络压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HAN]]></title>
    <url>%2F2018%2F08%2F24%2FHAN%2F</url>
    <content type="text"><![CDATA[论文：Hierarchical Attention Networks for Document Classification 发表会议：*NAACL2016* 前言：本文针对文本分类任务提出了一个层次化attention机制模型(HAN)，有两个显著的特点：(1)采用“词-句子-文章”的层次化结构来表示一篇文本。(2)该模型有两个层次的attention机制，分别存在于词层次(word level)和句子层次(sentence level)。从而使该模型具有对文本中重要性不同的句子和词的能力给予不同“注意力”的能力。作者在6个数据集合上进行了测试，并且相较以往的模型效果提升显著。最后，通过可视化说明该模型可以选择出含有丰富信息的词语和句子。 一 、写作动机 ​ 文本分类是一项基础的NLP任务，在主题分类，情感分析，垃圾邮件检测等应用上有广泛地应用。其目标是给每篇文本分配一个类别标签。本文中模型的直觉是，不同的词和句子对文本信息的表达有不同的影响，词和句子的重要性是严重依赖于上下文的，即使是相同的词和句子，在不同的上下文中重要性也不一样。就像人在阅读一篇文本时，对文本不同的内容是有着不同的注意度的。而本文在attention机制的基础上，联想到文本是一个层次化的结构，提出用词向量来表示句子向量，再由句子向量表示文档向量，并且在词层次和句子层次分别引入attention操作的模型。 ​ 分层构建只不过加上了两个Attention层，用于分别对句子和文档中的单词、句子的重要性进行建模。其主要思想是，首先考虑文档的分层结构：单词构成句子，句子构成文档，所以建模时也分这两部分进行。其次，不同的单词和句子具有不同的信息量，不能单纯的统一对待所以引入Attention机制。而且引入Attention机制除了提高模型的精确度之外还可以进行单词、句子重要性的分析和可视化，让我们对文本分类的内部有一定了解。模型主要可以分为下面四个部分，如下图所示： a word sequence encoder, a word-level attention layer, a sentence encoder a sentence-level attention layer. 这里对Attention机制进行一个补充介绍，可以参考论文“FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG -TERM MEMORY PROBLEMS”里面的插图： 1. 基于GRU的词序列编码器 GRU是RNN的一个变种，使用门机制来记录序列当前的状态。隐藏层有两个门(gate)，重置门(reset gate)rt和更新门(update gate)!zt。这两个门一起来控制当前状态有多少信息要更新。在时刻t，隐藏层状态的计算公式： $h_{t} =(1-z_{t})\odot h_{t-1} +z_{t}\odot \tilde{h_t }$ 更新门(update gate)zt是用来决定有多少过去的信息被保留，以及有多少新信息被加进来： $z_{t} =\sigma (W_{z}x_{t} +U_{z}h_{t-1} +b_{z} )$ 这里xt是在时刻t输入的单词的词向量，候选状态ht~的计算方法和普通的RNN相似： $\tilde{h_{t} }=tanh(W_{h}x_{t}+r_{t}\odot (U_{h}h_{t-1} ) +b_{h} ) $ 重置门rt决定有多少过去的信息作用于候选状态，如果rt是0，即忘记之前的所有状态： $r_{t}=\sigma(W_{r}x_{t}+U_{r}h_{t-1}+b_{r} )$ 2 层次化attention 2.1 词编码器（Word Encoder） 由词序列组成$w_{it}$,$t\in [0,T]$组成的句子,首先把词转化成词向量，$x_{it}=W_{e}w_{it}$,然后用双向的GRU网络,可以将正向和反向的上下文信息结合起来，获得隐藏层输出。 $x_{it}=W_{e}w_{it},t\in[1,T]$ $\overrightarrow{h_{it}}=\overrightarrow{GRU}({x_{it}})$ $\overleftarrow{h_{it}}=\overleftarrow{GRU}({x_{it}})$ 对于一个给定的词语$w_{it}$,经过GRU网络后，我们获得了一种新的表示：$h_{it}=[\overrightarrow{h_{it}},\overleftarrow{h_{it}}]$ $h_{it}$包含了$w_{it}$周围两个方向的信息。 2.2 词级别的attention机制 attention机制的目的是要把一个句子中，对句子的含义最重要，贡献最大的词语找出来。我们通过将$h_{it}$输入到一个单层的感知机(MLP)中得到的结果$u_{it}$作为$h_{it}$的隐含表示。 $u_{it}=tanh(W_{w}h_{it}+b_{w})$ 为了衡量单词的重要性,我们用$u_{it}$和一个随机初始化的上下文向量$u_{w}$的相似度来表示，然后经过softmax操作获得了一个归一化的attention权重矩阵$\alpha _{it}$，代表句子i中第t个词的权重。 $\alpha_{i}=\frac{exp\left( u_{i}^\top u_{s} \right) }{\sum_{t}^{}{exp\left( u_{t}^\top u_{s} \right) } } $ 有了attention权重矩阵以后，我们可以将句子向量$s_{i}$看作组成这些句子的词向量的加权求和。这里的上下文向量$u_{w}$是在训练网络的过程中学习获得的。我们可以把$u_{w}$当作一种询问的高级表示，比如“哪些词含有比较重要的信息？” $s_{i}=\sum_{t}{\alpha_{it}h_{it}} $ 2.3 语句编码器(Sentence Encoder) 得到了句子向量表示$s_{i}$以后，我们用类似的办法获得文档向量： $\overrightarrow{h_{i}}=\overrightarrow{GRU}(s_{i}),i\in[1,L]$ $\overleftarrow{h_{i}}=\overleftarrow{GRU}(s_{i}),i\in[L,1]$ 对于给定的句子$s_{i}$我们得到了相应的句子表示$h_{i}=[\overrightarrow{h_{i}},\overleftarrow{h_{i}}]$。这样获得的表示可以包含两个方向的上下文信息。 2.4 句子级别的attention 和词级别的attention类似，我们也提出了一个句子级别的上下文向量$u_{s}$,来衡量一个句子在整篇文本的重要性。 $u_{i}=tanh(W_{s}h_{i}+b_{s})$ $\alpha_{i}=\frac{exp\left( u_{i}^\top u_{s} \right) }{\sum_{t}^{}{exp\left( u_{t}^\top u_{s} \right) } } $ $v=\sum_{i}{\alpha_{i}h_{i}}$ 我们获得了整篇文章的向量表示v,最后可以使用全链接的softmax层进行分类。 总结 ​ 本文提出了一种基于层次化attention的文本分类模型，可以利用attention机制识别出一句话中比较重要的词语，利用重要的词语形成句子的表示，同样识别出重要的句子，利用重要句子表示来形成整篇文本的表示。实验证明，该模型确实比基准模型获得了更好的效果，可视化分析也表明，该模型能很好地识别出重要的句子和词语。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>笔记</tag>
        <tag>Attention</tag>
        <tag>Hierarchical Attention Networks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中Attention注意力机制]]></title>
    <url>%2F2018%2F08%2F24%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%ADAttention%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[​ 最近两年，注意力模型（Attention Model）被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，是深度学习技术中最值得关注与深入了解的核心技术之一。 ​ 本文以机器翻译为例，深入浅出地介绍了深度学习中注意力机制的原理及关键计算机制，同时也抽象出其本质思想，并介绍了注意力模型在图像及语音等领域的典型应用场景。 ​ 注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。 人类的视觉注意力 ​ 从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。 图1 人类的视觉注意力 ​ 视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。 ​ 这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。 ​ 图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。 ​ 深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。 Encoder-Decoder框架 ​ 要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。 ​ Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。 图2 抽象的文本处理领域的Encoder-Decoder框架 ​ 文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成： Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C： 对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息y1,y2,……,y i-1!来生成i时刻要生成的单词yi： ​ 每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。 ​ Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。 Attention模型 ​ 本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。 Soft Attention模型 ​ 图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下： ​ 其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。 ​ 而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。 ​ 如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。 ​ 在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。 ​ 没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。 ​ 上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值： （Tom,0.3）(Chase,0.2) (Jerry,0.5) ​ 每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。 ​ 同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。 图3 引入注意力模型的Encoder-Decoder框架 即生成目标句子单词的过程成了下面的形式： ​ 而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下： ​ 其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入Xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式： ​ 其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设Ci下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。 ​ 这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？ ​ 为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。 图5 RNN作为具体模型的Encoder-Decoder框架 那么用图6可以较为便捷地说明注意力分配概率分布值的通用计算过程。 ​ 图6 注意力分配概率计算 ​ 对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成yj时输入句子中的单词“Tom”、“Chase”、“Jerry”对yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。 ​ 绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。 图7 英语-德语翻译的注意力概率分布 上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。 ​ 目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的 步骤，而注意力模型其实起的是相同的作用。 图8 Google 神经网络机器翻译系统结构图 ​ 图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。 Attention机制的本质思想 ​ 如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。 图9 Attention机制的本质思想 ​ 我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式： ​ 其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。 ​ 当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。 ​ 从图9可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。 ​ 至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。 图10 三阶段计算Attention过程 ​ 在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Keyi，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式： ​ 第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算： ​ 第二阶段的计算结果!ai即为Valuei对应的权重系数，然后进行加权求和即可得到Attention数值： ​ 通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。 Self Attention模型 ​ 通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。 ​ 在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。 ​ 如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。 图11 可视化Self Attention实例 图12 可视化Self Attention实例 ​ 从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。 ​ 很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。 ​ 但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。 Attention机制的应用 ​ 前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。 图13 图片-描述任务的Encoder-Decoder框架 图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考图13）。 ​ 此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。 图14 图片生成句子中每个单词时的注意力聚焦区域 ​ 图15给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。 图15 图像描述任务中Attention机制的聚焦作用 图16 语音识别中音频序列和输出字符之间的Attention ​ 语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。 ​ 图16可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。 ​ 上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。 目前主流attention主流方法 首先是Object Recognition。 然后是Image Caption。 NLP中的Machine Translation. 不使用RNN结构。在特征图上生成空间分布的权重，然后再对特征图加权求和，试图让网络学出来对不同物体区域加以不同关注度。 总结与泛化。划重点：attention机制听起来高达上，其实就是学出一个权重分布，再拿这个权重分布施加在原来的特征之上，就可以叫做attention。 简单来说： （1）这个加权可以是保留所有分量均做加权（即soft attention）；也可以是在分布中以某种采样策略选取部分分量（即hard attention）。 （2）这个加权可以作用在原图上，也就是RAM和DRAM；也可以作用在特征图上，如后续的好多文章（例如image caption)。 （3）这个加权可以作用在空间尺度上，给不同空间区域加权；也可以作用在channel尺度上，给不同通道特征加权；甚至特征图上每个元素加权。 （4）这个加权还可以作用在不同时刻历史特征上，如Machine Translation，以及我前段时间做的视频相关的工作。 所以说，Attention是啥啊？就是一个权重分布！]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>笔记</tag>
        <tag>Attention</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM原理与实现]]></title>
    <url>%2F2018%2F08%2F24%2FLSTM%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[[RNN] [LSTM网络] [LSTM核心思想] [逐步理解LSTM] [遗忘门] [输入门] [输出门] [LSTM变体] [多层LSTM] LSTM实现手写数字 [设置LSTM参数] [初始化权值参数] Recurrent Neural Networks​ 人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。 传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。 RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。 RNN 包含循环 ​ 在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。 这些循环使得 RNN 看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开： 展开的 RNN ​ 链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。 并且 RNN 也已经被人们应用了！在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。建议参考 Andrej Karpathy 的博客文章——The Unreasonable Effectiveness of Recurrent Neural Networks 来看看更丰富有趣的 RNN 的成功应用。 而这些成功应用的关键之处就是 LSTM 的使用，这是一种特别的 RNN，比标准的 RNN 在很多的任务上都表现得更好。几乎所有的令人振奋的关于 RNN 的结果都是通过 LSTM 达到的。 长期依赖（Long-Term Dependencies）问题​ RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。 ​ 不太长的相关信息和位置间隔 ​ 但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。 相当长的相关信息和位置间隔* ​ 在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。 然而，幸运的是，LSTM 并没有这个问题！ LSTM 网络​ Long Short Term Memory 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。 LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！ 所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。 标准 RNN 中的重复模块包含单一的层 ​ LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。 LSTM 中的重复模块包含四个交互的层 ​ 不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。 LSTM 中的图标 ​ 在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。 LSTM 的核心思想​ LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。 细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。 ​ LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作(向量或者矩阵的pointwise 乘法指按元素相乘,点乘（不改变向量长度、大小） a = [a1 a2 a3] b = [b1 b2 b3] a和b的pointwise product a • b = a1b1+a2\b2+a3*b3 )。 ​ Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！ ​ LSTM 拥有三个门，来保护和控制细胞状态。这三个门分别输入门、遗忘门和输出门。 LSTM通过三个这样的本结构来实现信息的保护和控制。 逐步理解LSTM现在我们就开始通过三个门逐步的了解LSTM的原理 遗忘门​ 在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取h_{t-1}和x_t，输出一个在 0 到 1 之间的数值给每个在细胞状态C_{t-1}中的数字。1 表示“完全保留”，0 表示“完全舍弃”。 ​ 让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。 决定丢弃信息 其中ht−1表示的是上一个cell的输出，xt表示的是当前细胞的输入。σ表示sigmod函数。 输入门​ 下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，C^t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。 在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。 确定更新的信息 ​ 现在是更新旧细胞状态的时间了，Ct−1更新为Ct。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。 ​ 我们把旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上it∗C~t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。 ​ 在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。 更新细胞状态 输出门​ 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。 ​ 在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。 输出信息 LSTM 的变体​ 我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。 其中一个流形的 LSTM 变体，就是由 Gers &amp; Schmidhuber (2000) 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。 peephole 连接 ​ 上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。 ​ 另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。 coupled 忘记门和输入门 ​ 另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。 ​ 其中， rt表示重置门，zt表示更新门。重置门决定是否将之前的状态忘记。(作用相当于合并了 LSTM 中的遗忘门和传入门）当rt趋于0的时候，前一个时刻的状态信息ht−1会被忘掉，隐藏状态h^t会被重置为当前输入的信息。更新门决定是否要将隐藏状态更新为新的状态h^t（作用相当于 LSTM 中的输出门） 。 ​ 和 LSTM 比较一下：- GRU 少一个门，同时少了细胞状态Ct。- 在 LSTM 中，通过遗忘门和传入门控制信息的保留和传入；GRU 则通过重置门来控制是否要保留原来隐藏状态的信息，但是不再限制当前信息的传入。- 在 LSTM 中，虽然得到了新的细胞状态 Ct，但是还不能直接输出，而是需要经过一个过滤的处理:ht=ot∗tanh(Ct)；同样，在 GRU 中, 虽然我们也得到了新的隐藏状态h^t， 但是还不能直接输出，而是通过更新门来控制最后的输出：ht=(1−zt)∗ht−1+zt∗h^t。 多层LSTM多层LSTM是将LSTM进行叠加，其优点是能够在高层更抽象的表达特征，并且减少神经元的个数，增加识别准确率并且降低训练时间。 结论​ LSTM 是我们在 RNN 中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢？在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是注意力！” 这个想法是让 RNN 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，Xu, et al.(2015)已经这么做了——如果你希望深入探索注意力可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索…… 注意力也不是 RNN 研究领域中唯一的发展方向。例如，Kalchbrenner, et al. (2015) 提出的 Grid LSTM 看起来也是很有前途。使用生成模型的 RNN，诸如Gregor, et al. (2015) Chung, et al. (2015) 和 Bayer &amp; Osendorfer (2015) 提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！ LSTM实现手写数字这里我们利用的数据集是tensorflow提供的一个手写数字数据集。该数据集是一个包含n张28*28的数据集。 设置LSTM参数1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-import tensorflow as tffrom tensorflow.contrib import rnnimport numpy as npimport input_data# configuration# O * W + b -&gt; 10 labels for each image, O[? 28], W[28 10], B[10]# ^ (O: output 28 vec from 28 vec input)# |# +-+ +-+ +--+# |1|-&gt;|2|-&gt; ... |28| time_step_size = 28# +-+ +-+ +--+# ^ ^ ... ^# | | |# img1:[28] [28] ... [28]# img2:[28] [28] ... [28]# img3:[28] [28] ... [28]# ...# img128 or img256 (batch_size or test_size 256)# each input size = input_vec_size=lstm_size=28# configuration variablesinput_vec_size = lstm_size = 28 # 输入向量的维度time_step_size = 28 # 循环层长度batch_size = 128test_size = 256 这里设置将batch_size设置为128，time_step_size表示的是lstm神经元的个数，这里设置为28个，input_vec_size表示一次输入的像素数。 初始化权值参数12345678910111213141516171819202122def init_weights(shape): return tf.Variable(tf.random_normal(shape, stddev=0.01))def model(X, W, B, lstm_size): # X, input shape: (batch_size, time_step_size, input_vec_size) # XT shape: (time_step_size, batch_size, input_vec_size) #对这一步操作还不是太理解，为什么需要将第一行和第二行置换 XT = tf.transpose(X, [1, 0, 2]) # permute time_step_size and batch_size,[28, 128, 28] # XR shape: (time_step_size * batch_size, input_vec_size) XR = tf.reshape(XT, [-1, lstm_size]) # each row has input for each lstm cell (lstm_size=input_vec_size) # Each array shape: (batch_size, input_vec_size) X_split = tf.split(XR, time_step_size, 0) # split them to time_step_size (28 arrays),shape = [(128, 28),(128, 28)...] # Make lstm with lstm_size (each input vector size). num_units=lstm_size; forget_bias=1.0 lstm = rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, state_is_tuple=True) # Get lstm cell output, time_step_size (28) arrays with lstm_size output: (batch_size, lstm_size) # rnn..static_rnn()的输出对应于每一个timestep，如果只关心最后一步的输出，取outputs[-1]即可 outputs, _states = rnn.static_rnn(lstm, X_split, dtype=tf.float32) # 时间序列上每个Cell的输出:[... shape=(128, 28)..] # tanh activation # Get the last output return tf.matmul(outputs[-1], W) + B, lstm.state_size # State size to initialize the state ​ init_weigths函数利用正态分布随机生成参数的初始值，model的四个参数分别为：X为输入的数据，W表示的是2810的权值(标签为0-9)，B表示的是偏置，维度和W一样。这里首先将一批128（28*28）的图片放进神经网络。然后进行相关的操作(注释已经写得很明白了，这里就不再赘述)，然后利用WX+B求出预测结果，同时返回lstm的尺寸 训练1234py_x, state_size = model(X, W, B, lstm_size)cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost) 然后通过交叉熵计算误差，反复训练得到最优值。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>笔记</tag>
        <tag>LSTM</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树表查找]]></title>
    <url>%2F2018%2F08%2F23%2F%E6%A0%91%E8%A1%A8%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[​ 线性表的查找的顺序查找和折半查找作为查找表的组织形式，其中折半查找效率较高。但由于折半查找要求表中记录按关键字有序排列，且不能用链表做存储结构，因此当表的插入或删除操作频繁时，为维护表的有序性，需要移动表中很多记录。这种由移动记录引用的额外时间开销，就会抵消折半查找的优点。 ​ 所以，线性表的查找更适用于静态查找表，若要对动态查找表进行高效率的查找，可采用几种特殊的二叉树作为查找表的组织形式，在此将它们统称为树表。 一、二叉排序树 ​ 二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树： 1）若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 2）若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 3）任意节点的左、右子树也分别为二叉查找树。 二叉查找树性质：对二叉查找树进行中序遍历，即可得到有序的数列。 在讨论二叉排序树上的运算之前,定义其节点的类型如下：1234567​ typedef struct node //记录类型 &#123; KeyType key; //关键字项 InfoType data; //其他数据域 struct node *lchild,*rchild; //左右孩子指针 &#125; BSTNode; 不同形态的二叉查找树如下图所示： 如下图，这个是普通的二叉树： 在此基础上，加上节点之间的大小关系，就是二叉查找树： 1. 二叉排序树上的查找 因为二叉排序树可看做是一个有序表，所以在二叉排序树上进行查找，和二分查找类似，也是一个逐步缩小查找范围的过程。 递归查找算法SearchBST()如下（在二叉排序树bt上查找关键字为k的记录,成功时返回该节点指针，否则返回NULL）： 12345678910BSTNode *SearchBST(BSTNode *bt,KeyType k)&#123; if (bt==NULL || bt-&gt;key==k) //递归终结条件​ return bt;​ if (k&lt;bt-&gt;key) ​ return SearchBST(bt-&gt;lchild,k); //在左子树中递归查找 ​ else return SearchBST(bt-&gt;rchild,k); //在右子树中递归查找 &#125; 也可以采用如下非递归算法：12345678910111213BSTNode *SearchBST1(BSTNode *bt,KeyType k)&#123; while (bt!=NULL) &#123; if (k==bt-&gt;key) return bt; else if (k&lt;bt-&gt;key) bt=bt-&gt;lchild; //在左子树中查找 else bt=bt-&gt;rchild; //在左子树中查找 &#125; else //没有找到返回NULL return NULL; &#125; 2. 二叉排序树的插入和生成 ​ 在二叉排序树中插入一个关键字为k的新记录，要保证插入后仍满足BST性质。 插入过程： （1）若二叉排序树T为空，则创建一个key域为k的节点，将它作为根节点； （2）否则将k和根节点的关键字比较，若两者相等，则说明树中已有此关键字k，无须插入，直接返回0； （3）若k 小于T-&gt;key，则将k插入根节点的左子树中。 （4）否则将它插入右子树中。 对应的递归算法InsertBST()如下：123456789101112131415int InsertBST(BSTNode *&amp;p,KeyType k) //在以*p为根节点的BST中插入一个关键字为k的节点。插入成功返回1,否则返回0&#123; if (p==NULL) //原树为空, 新插入的记录为根节点 &#123; p=(BSTNode *)malloc(sizeof(BSTNode)); p-&gt;key=k;p-&gt;lchild=p-&gt;rchild=NULL; return 1; &#125; else if (k==p-&gt;key) //存在相同关键字的节点,返回0 return 0; else if (k&lt;p-&gt;key) return InsertBST(p-&gt;lchild,k); //插入到左子树中 else return InsertBST(p-&gt;rchild,k); //插入到右子树中 &#125; 插入操作图示如下： 最大最小值 二叉排序树的生成，是从一个空树开始，每插入一个关键字，就调用一次插入算法将它插入到当前已生成的二叉排序树中。 从关键字数组A[0..n-1]生成二叉排序树的算法CreatBST()如下：123456789101112BSTNode *CreatBST(KeyType A[],int n) //返回树根指针 &#123; BSTNode *bt=NULL; //初始时bt为空树 int i=0; while (i&lt;n) &#123; InsertBST(bt,A[i]); //将A[i]插入二叉排序树T中 i++; &#125; return bt; //返回建立的二叉排序树的根指针 &#125; 3. 二叉排序树的节点删除 （1）被删除的节点是叶子节点：直接删去该节点。 （2）被删除的节点只有左子树或者只有右子树，用其左子树或者右子树代替它。 （3）被删除的节点既有左子树，也有右子树：以其前驱替代之，然后再删除该前驱节点。前驱是左子树中最大的节点。也可以用其后继替代之，然后再删除该后继节点。后继是右子树中最小的节点。1234567891011121314151617181920212223242526272829303132333435int DeleteBST(BSTNode *&amp;bt,KeyType k) //在bt中删除关键字为k的节点&#123; if (bt==NULL) return 0; //空树删除失败 else &#123; if (k&lt;bt-&gt;key) return DeleteBST(bt-&gt;lchild,k); //递归在左子树中删除为k的节点 else if (k&gt;bt-&gt;key) return DeleteBST(bt-&gt;rchild,k); //递归在右子树中删除为k的节点 else &#123; Delete(bt); //调用Delete(bt)函数删除*bt节点 return 1; &#125; &#125;&#125; void Delete(BSTNode *&amp;p) //从二叉排序树中删除*p节点&#123; BSTNode *q; if (p-&gt;rchild==NULL) //*p节点没有右子树的情况 &#123; q=p; p=p-&gt;lchild; //其右子树的根节点放在被删节点的位置上 free(q); &#125; else if (p-&gt;lchild==NULL) //*p节点没有左子树 &#123; q=p; p=p-&gt;rchild; //将*p节点的右子树作为双亲节点的相应子树/ free(q); &#125; else Delete1(p,p-&gt;lchild); //*p节点既没有左子树又没有右子树的情况&#125; 删除二叉排序树节点的算法DeleteBST()如下（指针变量p指向待删除的节点，指针变量q指向待删除节点*p的双亲节点）：123456789101112void Delete1(BSTNode *p,BSTNode *&amp;r) //当被删*p节点有左右子树时的删除过程 &#123; BSTNode *q; if (r-&gt;rchild!=NULL) Delete1(p,r-&gt;rchild); //递归找最右下节点 else //找到了最右下节点*r &#123; p-&gt;key=r-&gt;key; //将*r的关键字值赋给*p q=r; r=r-&gt;lchild; //将左子树的根节点放在被删节点的位置上 free(q); //释放原*r的空间 &#125; &#125; 当删除的节点有2个子节点时，问题就变复杂了。 ​ 假设我们删除的节点t具有两个子节点。因为t具有右子节点，所以我们需要找到其右子节点中的最小节点，替换t节点的位置。这里有四个步骤： 保存带删除的节点到临时变量t 将t的右节点的最小节点min(t.right)保存到临时节点x 将x的右节点设置为deleteMin(t.right)，该右节点是删除后，所有比x.key最大的节点。 将x的做节点设置为t的左节点。 整个过程如下图： 分析 ​ 二叉查找树的运行时间和树的形状有关，树的形状又和插入元素的顺序有关。在最好的情况下，节点完全平衡，从根节点到最底层叶子节点只有lgN个节点。在最差的情况下，根节点到最底层叶子节点会有N各节点。在一般情况下，树的形状和最好的情况接近。 ​ 在分析二叉查找树的时候，我们通常会假设插入的元素顺序是随机的。对BST的分析类似与快速排序中的查找。 复杂度分析：它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。 下图为二叉树查找和顺序查找以及二分查找性能的对比图： 二、平衡二叉树(AVL) ​ 若一棵二叉树中每个节点的左、右子树的高度至多相差1，则称此二叉树为平衡二叉树。 ​ 在算法中，通过平衡因子（balancd factor，用bf表示）来具体实现上述平衡二叉树的定义。 ​ 平衡因子：平衡二叉树中每个节点有一个平衡因子域，每个节点的平衡因子是该节点左子树的高度减去右子树的高度。从平衡因子的角度可以说，若一棵二叉树中所有节点的平衡因子的绝对值小于或等于1，即平衡因子取值为1、0或-1，则该二叉树称为平衡二叉树。 定义AVL树的节点的类型如下：123456typedef struct node //记录类型&#123; KeyType key; //关键字项 int bf; //增加的平衡因子 InfoType data; //其他数据域 struct node *lchild,*rchild; //左右孩子指针&#125; BSTNode;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
        <tag>查找算法</tag>
        <tag>树表查找，二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见查找算法总结]]></title>
    <url>%2F2018%2F08%2F22%2F%E5%B8%B8%E8%A7%81%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[阅读目录 顺序查找 二分查找 插值查找 斐波那契查找 树表查找 分块查找 哈希查找 ​ 查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。本文简单概括性的介绍了常见的七种查找算法，说是七种，其实二分查找、插值查找以及斐波那契查找都可以归为一类——插值查找。插值查找和斐波那契查找是在二分查找的基础上的优化查找算法。 查找定义：根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 查找算法分类： 1）静态查找和动态查找； 注：静态或者动态都是针对查找表而言的。动态表指查找表中有删除和插入操作的表。 2）无序查找和有序查找。 无序查找：被查找数列有序无序均可； 有序查找：被查找数列必须为有序数列。 平均查找长度（Average Search Length，ASL）：需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度。 对于含有n个数据元素的查找表，查找成功的平均查找长度为：ASL = Pi*Ci的和。 Pi：查找表中第i个数据元素的概率。 Ci：找到第i个数据元素时已经比较过的次数。 1. 顺序查找 说明：顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想：顺序查找也称为线形查找，属于无序查找算法。从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 复杂度分析： 查找成功时的平均查找长度为：（假设每个数据元素的概率相等） ASL = 1/n(1+2+3+…+n) = (n+1)/2 ; 当查找不成功时，需要n+1次比较，时间复杂度为O(n); 所以，顺序查找的时间复杂度为O(n)。 （1） C++实现：123456789//顺序查找int Sequence_Search(int a[], int value, int n)&#123; int i; for(i=0; i&lt;n; i++) if(a[i]==value) return i; return -1;&#125; （2）python实现： 1234567def Sequence_Search(nums, target): if len(nums)==0: return False for i in range(len(nums)): if nums[i]==target: return True return False 2. 二分查找/折半查找 说明：元素必须是有序的，如果是无序的则要先进行排序操作。 基本思想：也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下，关键词比较次数为log2(n+1)，且期望时间复杂度为O(log2n)； 注：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。 （1） C++实现：123456789101112131415161718192021222324252627282930//二分查找，递归版本int BinarySearch2(int a[], int value, int low, int high)&#123; int mid = low+(high-low)/2; if(a[mid]==value) return mid; if(a[mid]&gt;value) return BinarySearch2(a, value, low, mid-1); if(a[mid]&lt;value) return BinarySearch2(a, value, mid+1, high);&#125;//二分查找（折半查找），非递归版本int BinarySearch1(int a[], int value, int n)&#123; int low, high, mid; low = 0; high = n-1; while(low&lt;=high) &#123; mid = (low+high)/2; if(a[mid]==value) return mid; if(a[mid]&gt;value) high = mid-1; if(a[mid]&lt;value) low = mid+1; &#125; return -1;&#125; （2） python版本123456789101112131415161718192021222324252627282930313233343536373839#-----------------递归二分查找------------------def binary_Search(nums,target,left,right): if len(nums)==0: return False mid=(left+right)//2 while left&lt;=right: if nums[mid]&gt;target: return binary_Search(nums, target, left, mid-1) elif nums[mid]&lt;target: return binary_Search(nums, target, mid+1, right) else: return midif __name__ == '__main__': list = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] low=0 high=len(list)-1 result = binary_Search(list,444,low,high) print(result)#-------------------非递归查找------------------------def binary_Search(nums,target): if len(nums)==0: return False low = 0 high = len(nums)-1 while low &lt;= high: mid = (low+high)//2 if nums[mid] == target: return mid elif nums[mid] &lt; target: low = mid + 1 else: high = mid - 1 return Falseif __name__ == '__main__': list = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_Search(list,444) print(result) ​ 时间复杂度分析 最坏情况下，关键词比较次数为⌊log2n⌋+1，最好情况就是1，所以二分查找的时间复杂度为O(logn)。它显然好于顺序查找的O(n)。 3. 插值查找 在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。 同样的，比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。 经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下： mid=(low+high)/2, 即mid=low+1/2*(high-low); 通过类比，我们可以将查找的点改进为如下： mid=low+(key-a[low])/(a[high]-a[low])*(high-low)， 也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 基本思想：基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为O(log2(log2n))。 （1） C++实现：1234567891011//插值查找int InsertionSearch(int a[], int value, int low, int high)&#123; int mid = low+(value-a[low])/(a[high]-a[low])*(high-low); if(a[mid]==value) return mid; if(a[mid]&gt;value) return InsertionSearch(a, value, low, mid-1); if(a[mid]&lt;value) return InsertionSearch(a, value, mid+1, high);&#125; （2）python代码 1234567891011121314151617181920212223242526# 插值查找算法# 时间复杂度O(log(n))def insert_search(lis, key): low = 0 high = len(lis) - 1 time = 0 while low &lt; high: time += 1 # 计算mid值是插值算法的核心代码 mid = low + int((high - low) * (key - lis[low])/(lis[high] - lis[low])) print("mid=%s, low=%s, high=%s" % (mid, low, high)) if key &lt; lis[mid]: high = mid - 1 elif key &gt; lis[mid]: low = mid + 1 else: # 打印查找的次数 print("times: %s" % time) return mid print("times: %s" % time) return Falseif __name__ == '__main__': LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = insert_search(LIST, 444) print(result) ​ 时间复杂度分析 ​ 它的时间复杂度跟二分查找的时间复杂度一样，为O(logn)。需要注意的是对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 4. 斐波那契查找 在介绍斐波那契查找算法之前，我们先介绍一下很它紧密相连并且大家都熟知的一个概念——黄金分割。 黄金比例又称黄金分割，是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1:0.618或1.618:1。 0.618被公认为最具有审美意义的比例数字，这个数值的作用不仅仅体现在诸如绘画、雕塑、音乐、建筑等艺术领域，而且在管理、工程设计等方面也有着不可忽视的作用。因此被称为黄金分割。 大家记不记得斐波那契数列：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89…….（从第三个数开始，后边每一个数都是前两个数的和）。然后我们会发现，随着斐波那契数列的递增，前后两个数的比值会越来越接近0.618，利用这个特性，我们就可以将黄金比例运用到查找技术中。 基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素比较，比较结果分三种情况： 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1; ​ 3）&lt;，high=mid-1。 斐波那契查找与折半查找很相似，他是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契数小1，及n=F(k)-1; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1),比较结果也分为三种 1）相等，mid位置的元素即为所求 2）&gt;，low=mid+1,k-=2; 说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为n-(F(k-1))= Fk-1-F(k-1)=Fk-F(k-1)-1=F(k-2)-1个，所以可以递归的应用斐波那契查找。 3）&lt;，high=mid-1,k-=1。 说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归 的应用斐波那契查找。 复杂度分析：最坏情况下，时间复杂度为O(log2n)，且其期望复杂度也为O(log2n)。 （1）C++实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 斐波那契查找#include "stdafx.h"#include &lt;memory&gt;#include &lt;iostream&gt;using namespace std;const int max_size=20;//斐波那契数组的长度/*构造一个斐波那契数组*/ void Fibonacci(int * F)&#123; F[0]=0; F[1]=1; for(int i=2;i&lt;max_size;++i) F[i]=F[i-1]+F[i-2];&#125;/*定义斐波那契查找法*/ int FibonacciSearch(int *a, int n, int key) //a为要查找的数组,n为要查找的数组长度,key为要查找的关键字&#123; int low=0; int high=n-1; int F[max_size]; Fibonacci(F);//构造一个斐波那契数组F int k=0; while(n&gt;F[k]-1)//计算n位于斐波那契数列的位置 ++k; int * temp;//将数组a扩展到F[k]-1的长度 temp=new int [F[k]-1]; memcpy(temp,a,n*sizeof(int)); for(int i=n;i&lt;F[k]-1;++i) temp[i]=a[n-1]; while(low&lt;=high) &#123; int mid=low+F[k-1]-1; if(key&lt;temp[mid]) &#123; high=mid-1; k-=1; &#125; else if(key&gt;temp[mid]) &#123; low=mid+1; k-=2; &#125; else &#123; if(mid&lt;n) return mid; //若相等则说明mid即为查找到的位置 else return n-1; //若mid&gt;=n则说明是扩展的数值,返回n-1 &#125; &#125; delete [] temp; return -1;&#125;int main()&#123; int a[] = &#123;0,16,24,35,47,59,62,73,88,99&#125;; int key=100; int index=FibonacciSearch(a,sizeof(a)/sizeof(int),key); cout&lt;&lt;key&lt;&lt;" is located at:"&lt;&lt;index; return 0;&#125; （2）python实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import random#source为待查找数组，key为要查找的数def fibonacciSearch(source,key): #生成裴波那契数列 fib = [0,1] for i in range(1,36): fib.append(fib[-1]+fib[-2]) #确定待查找数组在裴波那契数列的位置 k = 0 n = len(source) #此处 n&gt;fib[k]-1 也是别有深意的 #若n恰好是裴波那契数列上某一项，且要查找的元素正好在最后一位，此时必须将数组长度填充到数列下一项的数字 while(n &gt; fib[k]-1): k = k + 1 #将待查找数组填充到指定的长度 for i in range(n,fib[k]): a.append(a[-1]) low,high = 0,n-1 while(low &lt;= high): #获取黄金分割位置元素下标 mid = low + fib[k-1] - 1 if(key &lt; a[mid]): #若key比这个元素小,则key值应该在low至mid-1之间，剩下的范围个数为F(k-1)-1 high = mid - 1 k = k -1 elif(key &gt; a[mid]): #若key比这个元素大,则key至应该在mid+1至high之间，剩下的元素个数为F(k)-F(k-1)-1=F(k-2)-1 low = mid + 1 k = k - 2 else: if(mid &lt; n): return mid else: return n-1 return -1### 函数测试 ####生成待查找的数组a = [random.randint(1,100000) for x in range(0,33)]a.append(673990)a.sort()#待查找的数key = 673990#输出查找到的位置下标print(fibonacciSearch(a,key)) 时间复杂度分析 ​ 斐波那契查找的整体时间复杂度也为O(log(n))。但就平均性能，要优于二分查找。但是在最坏情况下，比如这里如果key为1，则始终处于左侧半区查找，此时其效率要低于二分查找。 总结 ​ 二分查找的mid运算是加法与除法，插值查找则是复杂的四则运算，而斐波那契查找只是最简单的加减运算。在海量数据的查找中，这种细微的差别可能会影响最终的查找效率。因此，三种有序表的查找方法本质上是分割点的选择不同，各有优劣，应根据实际情况进行选择。 5. 树表查找6. 线性索引查找6.1 简介 ​ 前面讲的几种比较高效的查找方法是基于有序的基础之上的，而事实上，数据集可能增长非常快，例如，某些微博网站或大型论坛的帖子和回复总数每天都是成百万上千万条，或者一些服务器的日志信息记录也可能是海量数据，要保证记录全部是按照当中的某个关键字有序，其时间代价是非常高昂的，所以这种数据都是按先后顺序存储的。 ​ 对于这样的查找表，我们如何能够快速查找到需要的数据呢？常常使用的方法就是—-索引。索引是为了加快查找速度而设计的一种数据结构。它是把一个关键字与它对应的记录相关联的过程，一个索引由若干个索引项构成，每个索引项至少应包含关键字和其对应的记录在存储器中的位置等信息。 ​ 索引按照结构可以分为线性索引、树形索引和多级索引。我们这里就只介绍线性索引技术。所谓线性索引就是将索引项集合组织为线性结构，也称为索引表。我们重点介绍三个线性索引：稠密索引、分块索引、和倒排索引。 6.2 稠密索引 ​ 稠密索引如下图所示，它是指在线性索引中，将数据集中的每个记录对应一个索引项。 ​ 上图中，左边的图像为索引序列，它是是按照关键码有序排列的。索引项有序也就意味着，我们要查找关键字时，可以用到折半、插值、斐波那契等有序查找算法，大大提高效率。比如查找上表中的18。如果不用索引表，需要6次。而用左侧的索引表，折半两次就可以找到18对应的指针。 ​ 这显然是稠密索引优点，但是如果数据集非常大，比如上亿，那也就意味着索引也得同样的数据集长度规模，对于内存有限的计算机来说，可能就需要反复去访问磁盘，查找性能反而大大下降了。 6.3 分块索引 ​ 稠密索引因为索引项与数据集的记录个数相同，所以空间代价很大。为了减少索引的个数，我们可以对数据集进行分块，使其分块有序，然后再对每一块建立一个索引项，从而减少索引项的个数。 分块查找又称索引顺序查找，它是顺序查找的一种改进方法。 算法思想：将n个数据元素”按块有序”划分为m块（m ≤ n）。每一块中的结点不必有序，但块与块之间必须”按块有序”；即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字；而第2块中任一元素又都必须小于第3块中的任一元素，…… 算法流程： step1 先选取各块中的最大关键字构成一个索引表； step2 查找分两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 ​ 分块有序，是把数据集的记录分成若干块，并且这些块需要满足两个条件： ​ （1）块内无序，即每一块内的记录不要求有序。当然，你如果能够让块内有序对查找来说更理想，不过这就要付出大量时间和空间代价，因此通常我们不要求块内有序 ​ （2）块间有序，例如要求第二块所有记录的关键字均要大于第一块中所有记录的关键字，第三块的所有记录的关键字均要大于第二块的所有记录关键字….因为只有块间有序，才有可能在查找时带来效率。 对于分块有序的数据集，将每块对应一个索引项，这种索引方法叫做分块索引。我们定义的分块索引项由三个数据项组成，如下图所示： ​ 这三个数据项分别为最大关键码（它存储每一块中的最大关键字，这样的好处就是可以使得在它之后的下一块中最小关键字也能比这一块最大的关键字要大）、存储了块中的记录个数（以便于循环时使用）和指向块首数据元素的指针（便于开始对这一块中的记录进行遍历）。 ​ 由上面的分析我们可以大概明白分块索引的步骤： ​ （1）在分块索引表中查找要查关键字所在的块。由于分块索引表是块间有序的，因此很容易利用折半、插值等算法得到结果。 ​ （2） 根据块首指针找到相应的块，并在块中顺序查找关键码。因为块中可以是无序的，因此只能顺序查获。 6.4 倒排索引 ​ 不知道你对搜索引擎好奇过没，无论你查找什么样的信息，它都可以在极短的时间内给你一些结果，是什么算法技术达到这样的高效查找呢？这里介绍一种最基础的搜索技术—-倒排索引。 ​ 我们来看一个例子，假设有以下两篇文章： ​ (1) Books and friends should be few but good . ​ (2) A good book is a good friend. ​ 假设我们忽略掉如“books”，“friends”中的复数”s”以及如“A”这样的大小写差异。我们可以整理出这样一张单词表，如下图所示，并将单词做了排序，也就是表格显示了每个不同的单词分别出现在哪篇文章中，比如“good”它在两篇文章中都有出现，而is只有在文章2中才有。 ​ 在这里这张单词表就是索引表，索引项的通用结构是次关键码和记录号表。 其中记录号表存储具有相同次字关键字的所有记录的记录号（可以指向记录的指针或者是该记录的主关键字）。因为这种查找方法是通过属性值来确定记录的位置，而不是通过记录来确定属性值，所以我们称其为倒排索引。 ​ 7. 哈希查找 什么是哈希表（Hash）？ ​ 散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。 中文名 哈希表 ​ 外文名 Hash table ​ 别 名 散列表 ​ 作 用 直接进行访问的数据结构 我们使用一个下标范围比较大的数组来存储元素。可以设计一个函数（哈希函数， 也叫做散列函数），使得每个元素的关键字都与一个函数值（即数组下标）相对应，于是用这个数组单元来存储这个元素；也可以简单的理解为，按照关键字为每一个元素”分类”，然后将这个元素存储在相应”类”所对应的地方。但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素，却计算出了相同的函数值，这样就产生了”冲突”，换句话说，就是把不同的元素分在了相同的”类”之中。后面我们将看到一种解决”冲突”的简便做法。 总的来说，”直接定址”与”解决冲突”是哈希表的两大特点。 什么是哈希函数？ 哈希函数的规则是：通过某种转换关系，使关键字适度的分散到指定大小的的顺序结构中，越分散，则以后查找的时间复杂度越小，空间复杂度越高。 算法思想：哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。 算法流程： 1）用给定的哈希函数构造哈希表； 2）根据选择的冲突处理方法解决地址冲突； 常见的解决冲突的方法：拉链法和线性探测法。 3）在哈希表的基础上执行哈希查找。 哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 复杂度分析： 单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(1)（注意，在查找之前我们需要构建相应的Hash表）。 使用Hash，我们付出了什么？ 我们在实际编程中存储一个大规模的数据，最先想到的存储结构可能就是map，也就是我们常说的KV(key value) pair，经常使用Python可能更有这种体会。使用map的好处就是，我们在后续处理数据处理时，可以根据数据的key快速的查找到对应的value值。map的本质就是Hash表，那我们在获取了超高查找效率的基础上，我们付出了什么？ Hash是一种典型以空间换时间的算法，比如原来一个长度为100的数组，对其查找，只需要遍历且匹配相应记录即可，从空间复杂度上来看，假如数组存储的是byte类型数据，那么该数组占用100byte空间。现在我们采用Hash算法，我们前面说的Hash必须有一个规则，约束键与存储位置的关系，那么就需要一个固定长度的hash表，此时，仍然是100byte的数组，假设我们需要的100byte用来记录键与位置的关系，那么总的空间为200byte,而且用于记录规则的表大小会根据规则，大小可能是不定的。 Hash算法和其他查找算法的性能对比：]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
        <tag>查找算法</tag>
        <tag>查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10大经典排序算法总结]]></title>
    <url>%2F2018%2F08%2F21%2F10%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[0、算法概述0.1 算法分类①：十种常见排序算法可以分为两大类： 非线性时间比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此称为非线性时间比较类排序。 线性时间非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此称为线性时间非比较类排序。 ②：另外一种分类排序方法——我们通常所说的排序算法往往指的是内部排序算法，即数据记录在内存中进行排序。 排序算法大体可分为两种： 一种是比较排序，时间复杂度O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。 另一种是非比较排序，时间复杂度可以达到O(n)，主要有：计数排序，基数排序，桶排序等。 0.2 算法复杂度 0.3 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。通俗地讲就是保证排序前后两个相等的数的相对顺序不变。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数。 ​ ​ 对于不稳定的排序算法，只要举出一个实例，即可说明它的不稳定性；而对于稳定的排序算法，必须对算法进行分析从而得到稳定的特性。需要注意的是，排序算法是否为稳定的是由具体算法决定的，不稳定的算法在某种条件下可以变为稳定的算法，而稳定的算法在某种条件下也可以变为不稳定的算法。 例如，对于冒泡排序，原本是稳定的排序算法，如果将记录交换的条件改成A[i] &gt;= A[i + 1]，则两个相等的记录就会交换位置，从而变成不稳定的排序算法。 其次，说一下排序算法稳定性的好处。排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，前一个键排序的结果可以为后一个键排序所用。基数排序就是这样，先按低位排序，逐次按高位排序，低位排序后元素的顺序在高位也相同时是不会改变的。 0.4 技巧记忆口诀： 不稳定排序算法口诀：快些选队（快希选堆）其余为稳定的。 算法复杂度和关键字顺序无关的有： 顺口溜：一堆（堆排序）海龟（归并排序）选（选择排序）基（基数排序）友 快些以 nlog2 n 的速度归队 快=快速排序，些=希尔排序，归=归并排序，队=堆排序 这四种排序算法，时间都是 n log2 n 的，除了这四个之外，其他的排序算法平均时间都为 n^2 一趟排序，保证一个元素为最终位置的有两类排序算法：交换类（冒泡和快速）排序和选择类排序（简单和堆） 元素比较次数和原始序列无关的算法：简单选择排序，折半插入排序 排序趟数和原序列有关的算法：交换类，其余类无关 借助于比较进行排序的算法，在最坏的时候，最好的时间复杂度为 n log2 n 堆排序和简单选择排序的时间复杂度和初始序列无关 0.5 总结：（1）在比较类排序中，归并排序号称最快，其次是快速排序和堆排序，两者不相伯仲，但是有一点需要注意，数据初始排序状态对堆排序不会产生太大的影响，而快速排序却恰恰相反。 （2）线性时间非比较类排序一般要优于非线性时间比较类排序，但前者对待排序元素的要求较为严格，比如计数排序要求待排序数的最大值不能太大，桶排序要求元素按照hash分桶后桶内元素的数量要均匀。线性时间非比较类排序的典型特点是以空间换时间。 1、冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 1.1 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 1.2 动图演示 1.3 代码实现1234567891011121314151617181920212223// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n^2)// 最优时间复杂度 ---- 如果能在内部循环第一次运行时,使用一个旗标来表示有无需要交换的可能,可以把最优时间复杂度降低到O(n)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 稳定void BubbleSort(RecordType arr[]) &#123; var len = arr.length; change = TRUE; for (var i = 0; i &lt; len - 1 &amp;&amp; change; i++) &#123; change = FALSE; for (var j = 0; j &lt; len - 1 - i; j++) &#123;// 依次比较相邻的两个元素,使较大的那个向后移 if (arr[j].key &gt; arr[j+1].key) &#123; // 相邻元素两两对比。 如果条件改成arr[j].key &gt;= arr[j+1].key,则变为不稳定的排序算法 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; change = TRUE; &#125; &#125; &#125; return arr;&#125; ​ 假如说上述代码对序列{ 6, 5, 3, 1, 8, 7, 2, 4 }进行冒泡排序，则实现过程如下： ​ 使用冒泡排序为一列数字进行排序的过程如右图所示: 尽管冒泡排序是最容易了解和实现的排序算法之一，但它对于少数元素之外的数列排序是很没有效率的。 1.1、冒泡排序的改进：鸡尾酒排序（定向/双向冒泡排序）​ 鸡尾酒排序，也叫定向冒泡排序，是冒泡排序的一种改进，即排序过程中交替改变扫描方向。 先从底向上冒一个最小元素，再从上向低冒一个最大元素。 此算法与冒泡排序的不同处在于从低到高然后从高到低，而冒泡排序则仅从低到高去比较序列里的每个元素。他可以得到比冒泡排序稍微好一点的效能。 鸡尾酒排序的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;stdio.h&gt;// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n^2)// 最优时间复杂度 ---- 如果序列在一开始已经大部分排序过的话,会接近O(n)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 稳定void Swap(int A[], int i, int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125;void CocktailSort(int A[], int n)&#123; int left = 0; // 初始化边界 int right = n - 1; bool flag = TRUE; while (flag) &#123; flag = FLASE; for (int i = left; i &lt; right; i++) // 前半轮,从左到右扫描，将最大元素放到最右边 &#123; if (A[i] &gt; A[i + 1]) &#123; Swap(A, i, i + 1); flag = TRUE; &#125; &#125; right--; for (int i = right; i &gt; left; i--) // 后半轮,从右到左扫描,将最小元素放到最左边 &#123; if (A[i - 1] &gt; A[i]) &#123; Swap(A, i - 1, i); flag = TRUE; &#125; &#125; left++; &#125;&#125;int main()&#123; int A[] = &#123; 6, 5, 3, 1, 8, 7, 2, 4 &#125;; // 从小到大定向冒泡排序 int n = sizeof(A) / sizeof(int); CocktailSort(A, n); printf("鸡尾酒排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; ​ 使用鸡尾酒排序为一列数字进行排序的过程如右图所示： 以序列(2,3,4,5,1)为例，鸡尾酒排序只需要访问一次序列就可以完成排序，但如果使用冒泡排序则需要四次。但是在乱数序列的状态下，鸡尾酒排序与冒泡排序的效率都很差劲。 2、选择排序（Selection Sort）选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 2.1 算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 2.2 动图演示 2.3 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;stdio.h&gt;// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n^2)// 最优时间复杂度 ---- O(n^2)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 不稳定void Swap(int A[], int i, int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125;void SelectionSort(int A[], int n)&#123; for (int i = 0; i &lt; n - 1; i++) // i为已排序序列的末尾 &#123; int min = i; for (int j = i + 1; j &lt; n; j++) // 未排序序列 &#123; if (A[j] &lt; A[min]) // 找出未排序序列中的最小值 &#123; min = j; &#125; &#125; if (min != i) &#123; Swap(A, min, i); // 放到已排序序列的末尾，该操作很有可能把稳定性打乱，所以选择排序是不稳定的排序算法 &#125; &#125;&#125;int main()&#123; int A[] = &#123; 8, 5, 2, 6, 9, 3, 1, 4, 0, 7 &#125;; // 从小到大选择排序 int n = sizeof(A) / sizeof(int); SelectionSort(A, n); printf("选择排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 上述代码对序列{ 8, 5, 2, 6, 9, 3, 1, 4, 0, 7 }进行选择排序的实现过程如右图: 使用选择排序为一列数字进行排序的宏观过程： 选择排序是不稳定的排序算法，不稳定发生在最小元素与A[i]交换的时刻。 比如序列：{ 5, 8, 5, 2, 9 }，一次选择的最小元素是2，然后把2和第一个5进行交换，从而改变了两个元素5的相对次序。 3、插入排序（Insertion Sort）​ 插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。它的工作原理非常类似于我们抓扑克牌 。 ​ 对于未排序数据(右手抓到的牌)，在已排序序列(左手已经排好序的手牌)中从后向前扫描，找到相应位置并插入。 插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 3.1 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 3.2 动图演示 3.2 代码实现1234567891011121314151617181920212223242526272829303132333435363738#include &lt;stdio.h&gt;// 分类 ------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- 最坏情况为输入序列是降序排列的,此时时间复杂度O(n^2)// 最优时间复杂度 ---- 最好情况为输入序列是升序排列的,此时时间复杂度O(n)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 稳定void InsertionSort(int A[], int n)&#123; for (int i = 1; i &lt; n; i++) // 类似抓扑克牌排序 &#123; int get = A[i]; // 右手抓到一张扑克牌 int j = i - 1; // 拿在左手上的牌总是排序好的 while (j &gt;= 0 &amp;&amp; A[j] &gt; get) // 将抓到的牌与手牌从右向左进行比较 &#123; A[j + 1] = A[j]; // 如果该手牌比抓到的牌大，就将其右移 j--; &#125; A[j + 1] = get; // 直到该手牌比抓到的牌小(或二者相等)，将抓到的牌插入到该手牌右边(相等元素的相对次序未变，所以插入排序是稳定的) &#125;&#125;int main()&#123; int A[] = &#123; 6, 5, 3, 1, 8, 7, 2, 4 &#125;;// 从小到大插入排序 int n = sizeof(A) / sizeof(int); InsertionSort(A, n); printf("插入排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 上述代码对序列{ 6, 5, 3, 1, 8, 7, 2, 4 }进行插入排序的实现过程如下 使用插入排序为一列数字进行排序的宏观过程： 插入排序不适合对于数据量比较大的排序应用。但是，如果需要排序的数据量很小，比如量级小于千，那么插入排序还是一个不错的选择。 插入排序在工业级库中也有着广泛的应用，在STL的sort算法和stdlib的qsort算法中，都将插入排序作为快速排序的补充，用于少量元素的排序（通常为8个或以下）。 3.1、插入排序的改进：二分插入排序​ 对于插入排序，如果比较操作的代价比交换操作大的话，可以采用二分查找法来减少比较操作的次数，我们称为二分插入排序，代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n^2)// 最优时间复杂度 ---- O(nlogn)// 平均时间复杂度 ---- O(n^2)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 稳定void InsertionSortDichotomy(int A[], int n)&#123; for (int i = 1; i &lt; n; i++) &#123; int get = A[i]; // 右手抓到一张扑克牌 int left = 0; // 拿在左手上的牌总是排序好的，所以可以用二分法 int right = i - 1; // 手牌左右边界进行初始化 while (left &lt;= right) // 采用二分法定位新牌的位置 &#123; int mid = (left + right) / 2; if (A[mid] &gt; get) right = mid - 1; else left = mid + 1; &#125; for (int j = i - 1; j &gt;= left; j--) // 将欲插入新牌位置右边的牌整体向右移动一个单位 &#123; A[j + 1] = A[j]; &#125; A[left] = get; // 将抓到的牌插入手牌 &#125;&#125;int main()&#123; int A[] = &#123; 5, 2, 9, 4, 7, 6, 1, 3, 8 &#125;;// 从小到大二分插入排序 int n = sizeof(A) / sizeof(int); InsertionSortDichotomy(A, n); printf("二分插入排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 当n较大时，二分插入排序的比较次数比直接插入排序的最差情况好得多，但比直接插入排序的最好情况要差，所当以元素初始序列已经接近升序时，直接插入排序比二分插入排序比较次数少。二分插入排序元素移动次数与直接插入排序相同，依赖于元素初始序列。 4、希尔排序（Shell Sort）-插入排序的更高效改进​ 1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫递减增量排序。希尔排序是不稳定的排序算法。 ​ 希尔排序是基于插入排序的以下两点性质而提出改进方法的： 插入排序在对几乎已经排好序的数据操作时，效率高，即可以达到线性排序的效率 但插入排序一般来说是低效的，因为插入排序每次只能将数据移动一位 4.1 算法描述先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 4.2 动图演示 4.3 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt; // 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- 根据步长序列的不同而不同。已知最好的为O(n(logn)^2)// 最优时间复杂度 ---- O(n)// 平均时间复杂度 ---- 根据步长序列的不同而不同。// 所需辅助空间 ------ O(1)// 稳定性 ------------ 不稳定void ShellSort(int A[], int n)&#123; int h = 0; while (h &lt;= n) // 生成初始增量 &#123; h = 3 * h + 1; &#125; while (h &gt;= 1) &#123; for (int i = h; i &lt; n; i++) &#123; int j = i - h; int get = A[i]; while (j &gt;= 0 &amp;&amp; A[j] &gt; get) &#123; A[j + h] = A[j]; j = j - h; &#125; A[j + h] = get; &#125; h = (h - 1) / 3; // 递减增量 &#125;&#125;int main()&#123; int A[] = &#123; 5, 2, 9, 4, 7, 6, 1, 3, 8 &#125;;// 从小到大希尔排序 int n = sizeof(A) / sizeof(int); ShellSort(A, n); printf("希尔排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 4.4 算法分析以23, 10, 4, 1的步长序列进行希尔排序： 希尔排序是不稳定的排序算法，虽然一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱。 比如序列：{ 3, 5, 10, 8, 7, 2, 8, 1, 20, 6 }，h=2时分成两个子序列 { 3, 10, 7, 8, 20 } 和 { 5, 8, 2, 1, 6 } ，未排序之前第二个子序列中的8在前面，现在对两个子序列进行插入排序，得到 { 3, 7, 8, 10, 20 } 和 { 1, 2, 5, 6, 8 } ，即 { 3, 1, 7, 2, 8, 5, 10, 6, 20, 8 } ，两个8的相对次序发生了改变。 ​ 希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。 5、归并排序（Merge Sort）​ 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 5.1 算法描述​ 归并排序的实现分为递归实现与非递归(迭代)实现。递归实现的归并排序是算法设计中分治策略的典型应用，我们将一个大问题分割成小问题分别解决，然后用所有小问题的答案来解决整个大问题。非递归(迭代)实现的归并排序首先进行是两两归并，然后四四归并，然后是八八归并，一直下去直到归并了整个数组。 归并排序算法主要依赖归并(Merge)操作。归并操作指的是将两个已经排序的序列合并成一个序列的操作，归并操作步骤如下： 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列 设定两个指针，最初位置分别为两个已经排序序列的起始位置 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置 重复步骤3直到某一指针到达序列尾 将另一序列剩下的所有元素直接复制到合并序列尾 5.2 动图演示 5.3 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;stdio.h&gt;#include &lt;limits.h&gt;// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(nlogn)// 最优时间复杂度 ---- O(nlogn)// 平均时间复杂度 ---- O(nlogn)// 所需辅助空间 ------ O(n)// 稳定性 ------------ 稳定void Merge(int A[], int left, int mid, int right)// 合并两个已排好序的数组A[left...mid]和A[mid+1...right]&#123; int len = right - left + 1; int *temp = new int[len]; // 辅助空间O(n) int index = 0; int i = left; // 前一数组的起始元素 int j = mid + 1; // 后一数组的起始元素 while (i &lt;= mid &amp;&amp; j &lt;= right) &#123; temp[index++] = A[i] &lt;= A[j] ? A[i++] : A[j++]; // 带等号保证归并排序的稳定性 &#125; while (i &lt;= mid) &#123; temp[index++] = A[i++]; &#125; while (j &lt;= right) &#123; temp[index++] = A[j++]; &#125; for (int k = 0; k &lt; len; k++) &#123; A[left++] = temp[k]; &#125;&#125;void MergeSortRecursion(int A[], int left, int right) // 递归实现的归并排序(自顶向下)&#123; if (left == right) // 当待排序的序列长度为1时，递归开始回溯，进行merge操作 return; int mid = (left + right) / 2; MergeSortRecursion(A, left, mid); MergeSortRecursion(A, mid + 1, right); Merge(A, left, mid, right);&#125;void MergeSortIteration(int A[], int len) // 非递归(迭代)实现的归并排序(自底向上)&#123; int left, mid, right;// 子数组索引,前一个为A[left...mid]，后一个子数组为A[mid+1...right] for (int i = 1; i &lt; len; i *= 2) // 子数组的大小i初始为1，每轮翻倍 &#123; left = 0; while (left + i &lt; len) // 后一个子数组存在(需要归并) &#123; mid = left + i - 1; right = mid + i &lt; len ? mid + i : len - 1;// 后一个子数组大小可能不够 Merge(A, left, mid, right); left = right + 1; // 前一个子数组索引向后移动 &#125; &#125;&#125;int main()&#123; int A1[] = &#123; 6, 5, 3, 1, 8, 7, 2, 4 &#125;; // 从小到大归并排序 int A2[] = &#123; 6, 5, 3, 1, 8, 7, 2, 4 &#125;; int n1 = sizeof(A1) / sizeof(int); int n2 = sizeof(A2) / sizeof(int); MergeSortRecursion(A1, 0, n1 - 1); // 递归实现 MergeSortIteration(A2, n2); // 非递归实现 printf("递归实现的归并排序结果："); for (int i = 0; i &lt; n1; i++) &#123; printf("%d ", A1[i]); &#125; printf("\n"); printf("非递归实现的归并排序结果："); for (int i = 0; i &lt; n2; i++) &#123; printf("%d ", A2[i]); &#125; printf("\n"); return 0;&#125; 上述代码对序列{ 6, 5, 3, 1, 8, 7, 2, 4 }进行归并排序的实例如下 使用归并排序为一列数字进行排序的宏观过程： 归并排序除了可以对数组进行排序，还可以高效的求出数组小和（即单调和）以及数组中的逆序对。 5.4 算法分析归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 6、快速排序（Quick Sort）​ 快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 6.1 算法描述​ 选择一个基准，小于放左边，大于放右边。 快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 6.2 动图演示 ​ 上图是每次将基准交换。 ​ 或者如下，最后再将基准交换。 ​ 方法其实很简单：分别从初始序列“6 1 2 7 9 3 4 5 10 8”两端开始“探测”。先从右往左找一个小于6的数，再从左往右找一个大于6的数，然后交换他们。这里可以用两个变量i和j，分别指向序列最左边和最右边。我们为这两个变量起个好听的名字“哨兵i”和“哨兵j”。刚开始的时候让哨兵i指向序列的最左边（即i=1），指向数字6。让哨兵j指向序列的最右边（即=10），指向数字。 ​ 首先哨兵j开始出动。因为此处设置的基准数是最左边的数，所以需要让哨兵j先出动，这一点非常重要（请自己想一想为什么）。哨兵j一步一步地向左挪动（即j–），直到找到一个小于6的数停下来。接下来哨兵i再一步一步向右挪动（即i++），直到找到一个数大于6的数停下来。最后哨兵j停在了数字5面前，哨兵i停在了数字7面前。 ​ 现在交换哨兵i和哨兵j所指向的元素的值。交换之后的序列如下： ​ 6 1 2 5 9 3 4 7 10 8 ​ 到此，第一次交换结束。接下来开始哨兵j继续向左挪动（再友情提醒，每次必须是哨兵j先出发）。他发现了4（比基准数6要小，满足要求）之后停了下来。哨兵i也继续向右挪动的，他发现了9（比基准数6要大，满足要求）之后停了下来。此时再次进行交换，交换之后的序列如下： 6 1 2 5 4 3 9 7 10 8 ​ 第二次交换结束，“探测”继续。哨兵j继续向左挪动，他发现了3（比基准数6要小，满足要求）之后又停了下来。哨兵i继续向右移动，糟啦！此时哨兵i和哨兵j相遇了，哨兵i和哨兵j都走到3面前。说明此时“探测”结束。我们将基准数6和3进行交换。交换之后的序列如下： 3 1 2 5 4 6 9 7 10 8 ​ 到此第一轮“探测”真正结束。此时以基准数6为分界点，6左边的数都小于等于6，6右边的数都大于等于6。回顾一下刚才的过程，其实哨兵j的使命就是要找小于基准数的数，而哨兵i的使命就是要找大于基准数的数，直到i和j碰头为止。 注：最好的是设置一个临时变量，做覆盖操作而不是一直做交换操作。 6.3 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;stdio.h&gt;// 分类 ------------ 内部比较排序// 数据结构 --------- 数组// 最差时间复杂度 ---- 每次选取的基准都是最大（或最小）的元素，导致每次只划分出了一个分区，需要进行n-1次划分才能结束递归，时间复杂度为O(n^2)// 最优时间复杂度 ---- 每次选取的基准都是中位数，这样每次都均匀的划分出两个分区，只需要logn次划分就能结束递归，时间复杂度为O(nlogn)// 平均时间复杂度 ---- O(nlogn)// 所需辅助空间 ------ 主要是递归造成的栈空间的使用(用来保存left和right等局部变量)，取决于递归树的深度，一般为O(logn)，最差为O(n) // 稳定性 ---------- 不稳定void Swap(int A[], int i, int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125;int Partition(int A[], int left, int right) // 划分函数&#123; int pivot = A[right]; // 这里每次都选择最后一个元素作为基准 int tail = left - 1; // tail为小于基准的子数组最后一个元素的索引 for (int i = left; i &lt; right; i++) // 遍历基准以外的其他元素 &#123; if (A[i] &lt;= pivot) // 把小于等于基准的元素放到前一个子数组末尾 &#123; Swap(A, ++tail, i); &#125; &#125; Swap(A, tail + 1, right); // 最后把基准放到前一个子数组的后边，剩下的子数组既是大于基准的子数组 // 该操作很有可能把后面元素的稳定性打乱，所以快速排序是不稳定的排序算法 return tail + 1; // 返回基准的索引&#125;void QuickSort(int A[], int left, int right)&#123; if (left &gt;= right) return; int pivot_index = Partition(A, left, right); // 基准的索引 QuickSort(A, left, pivot_index - 1); QuickSort(A, pivot_index + 1, right);&#125;int main()&#123; int A[] = &#123; 5, 2, 9, 4, 7, 6, 1, 3, 8 &#125;; // 从小到大快速排序 int n = sizeof(A) / sizeof(int); QuickSort(A, 0, n - 1); printf("快速排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 使用快速排序法对一列数字进行排序的过程： 快速排序是不稳定的排序算法，不稳定发生在基准元素与A[tail+1]交换的时刻。 比如序列：{ 1, 3, 4, 2, 8, 9, 8, 7, 5 }，基准元素是5，一次划分操作后5要和第一个8进行交换，从而改变了两个元素8的相对次序。 Java系统提供的Arrays.sort函数。对于基础类型，底层使用快速排序。对于非基础类型，底层使用归并排序。请问是为什么？ 答：这是考虑到排序算法的稳定性。对于基础类型，相同值是无差别的，排序前后相同值的相对位置并不重要，所以选择更为高效的快速排序，尽管它是不稳定的排序算法；而对于非基础类型，排序前后相等实例的相对位置不宜改变，所以选择稳定的归并排序。 7、堆排序（Heap Sort）​ 堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 7.1 算法描述 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 7.2 动图演示 构建初始堆 整体排序流程 7.3 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;stdio.h&gt;// 分类 -------------- 内部比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(nlogn)// 最优时间复杂度 ---- O(nlogn)// 平均时间复杂度 ---- O(nlogn)// 所需辅助空间 ------ O(1)// 稳定性 ------------ 不稳定void Swap(int A[], int i, int j)&#123; int temp = A[i]; A[i] = A[j]; A[j] = temp;&#125;void Heapify(int A[], int i, int size) // 从A[i]向下进行堆调整&#123; int left_child = 2 * i + 1; // 左孩子索引 int right_child = 2 * i + 2; // 右孩子索引 int max = i; // 选出当前结点与其左右孩子三者之中的最大值 if (left_child &lt; size &amp;&amp; A[left_child] &gt; A[max]) max = left_child; if (right_child &lt; size &amp;&amp; A[right_child] &gt; A[max]) max = right_child; if (max != i) &#123; Swap(A, i, max); // 把当前结点和它的最大(直接)子节点进行交换 Heapify(A, max, size); // 递归调用，继续从当前结点向下进行堆调整 &#125;&#125;int BuildHeap(int A[], int n) // 建堆，时间复杂度O(n)&#123; int heap_size = n; for (int i = heap_size / 2 - 1; i &gt;= 0; i--) // 从每一个非叶结点开始向下进行堆调整 Heapify(A, i, heap_size); return heap_size;&#125;void HeapSort(int A[], int n)&#123; int heap_size = BuildHeap(A, n); // 建立一个最大堆 while (heap_size &gt; 1) // 堆（无序区）元素个数大于1，未完成排序 &#123; // 将堆顶元素与堆的最后一个元素互换，并从堆中去掉最后一个元素 // 此处交换操作很有可能把后面元素的稳定性打乱，所以堆排序是不稳定的排序算法 Swap(A, 0, --heap_size); Heapify(A, 0, heap_size); // 从新的堆顶元素开始向下进行堆调整，时间复杂度O(logn) &#125;&#125;int main()&#123; int A[] = &#123; 5, 2, 9, 4, 7, 6, 1, 3, 8 &#125;;// 从小到大堆排序 int n = sizeof(A) / sizeof(int); HeapSort(A, n); printf("堆排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 堆排序算法的演示： 动画中在排序过程之前简单的表现了创建堆的过程以及堆的逻辑结构。 堆排序是不稳定的排序算法，不稳定发生在堆顶元素与A[i]交换的时刻。 比如序列：{ 9, 5, 7, 5 }，堆顶元素是9，堆排序下一步将9和第二个5进行交换，得到序列 { 5, 5, 7, 9 }，再进行堆调整得到{ 7, 5, 5, 9 }，重复之前的操作最后得到{ 5, 5, 7, 9 }从而改变了两个5的相对次序。 以上为比较排序。 以下为非比较排序。 8、计数排序（Counting Sort）​ 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 ​ 通俗地理解，例如有10个年龄不同的人，假如统计出有8个人的年龄不比小明大（即小于等于小明的年龄，这里也包括了小明），那么小明的年龄就排在第8位，通过这种思想可以确定每个人的位置，也就排好了序。当然，年龄一样时需要特殊处理（保证稳定性）：通过反向填充目标数组，填充完毕后将对应的数字统计递减，可以确保计数排序的稳定性。 8.1 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 8.2 动图演示12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include&lt;iostream&gt;using namespace std;// 分类 ------------ 内部非比较排序// 数据结构 --------- 数组// 最差时间复杂度 ---- O(n + k)// 最优时间复杂度 ---- O(n + k)// 平均时间复杂度 ---- O(n + k)// 所需辅助空间 ------ O(n + k)// 稳定性 ----------- 稳定const int k = 100; // 基数为100，排序[0,99]内的整数int C[k]; // 计数数组void CountingSort(int A[], int n)&#123; for (int i = 0; i &lt; k; i++) // 初始化，将数组C中的元素置0(此步骤可省略，整型数组元素默认值为0) &#123; C[i] = 0; &#125; for (int i = 0; i &lt; n; i++) // 使C[i]保存着等于i的元素个数 &#123; C[A[i]]++; &#125; for (int i = 1; i &lt; k; i++) // 使C[i]保存着小于等于i的元素个数，排序后元素i就放在第C[i]个输出位置上 &#123; C[i] = C[i] + C[i - 1]; &#125; int *B = (int *)malloc((n) * sizeof(int));// 分配临时空间,长度为n，用来暂存中间数据 for (int i = n - 1; i &gt;= 0; i--) // 从后向前扫描保证计数排序的稳定性(重复元素相对次序不变) &#123; B[--C[A[i]]] = A[i]; // 把每个元素A[i]放到它在输出数组B中的正确位置上 // 当再遇到重复元素时会被放在当前元素的前一个位置上保证计数排序的稳定性 &#125; for (int i = 0; i &lt; n; i++) // 把临时空间B中的数据拷贝回A &#123; A[i] = B[i]; &#125; free(B); // 释放临时空间 &#125;int main()&#123; int A[] = &#123; 15, 22, 19, 46, 27, 73, 1, 19, 8 &#125;; // 针对计数排序设计的输入，每一个元素都在[0,100]上且有重复元素 int n = sizeof(A) / sizeof(int); CountingSort(A, n); printf("计数排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; ​ 计数排序的时间复杂度和空间复杂度与数组A的数据范围（A中元素的最大值与最小值的差加上1）有关，因此对于数据范围很大的数组，计数排序需要大量时间和内存。 例如：对0到99之间的数字进行排序，计数排序是最好的算法，然而计数排序并不适合按字母顺序排序人名，将计数排序用在基数排序算法中，能够更有效的排序数据范围很大的数组。 8.4 算法分析​ 计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 9、桶排序（Bucket Sort）​ 桶排序也叫箱排序。桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：将数组元素映射到有限数量个桶里，利用计数排序可以定位桶的边界，每个桶再各自进行桶内排序（使用其它排序算法或以递归方式继续使用桶排序）。 9.1 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。 9.2 图片演示 9.3 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;iostream&gt;using namespace std;// 分类 ------------- 内部非比较排序// 数据结构 --------- 数组// 最差时间复杂度 ---- O(nlogn)或O(n^2)，只有一个桶，取决于桶内排序方式// 最优时间复杂度 ---- O(n)，每个元素占一个桶// 平均时间复杂度 ---- O(n)，保证各个桶内元素个数均匀即可// 所需辅助空间 ------ O(n + bn)// 稳定性 ----------- 稳定/* 本程序用数组模拟桶 */const int bn = 5; // 这里排序[0,49]的元素，使用5个桶就够了，也可以根据输入动态确定桶的数量int C[bn]; // 计数数组，存放桶的边界信息void InsertionSort(int A[], int left, int right)&#123; for (int i = left + 1; i &lt;= right; i++) // 从第二张牌开始抓，直到最后一张牌 &#123; int get = A[i]; int j = i - 1; while (j &gt;= left &amp;&amp; A[j] &gt; get) &#123; A[j + 1] = A[j]; j--; &#125; A[j + 1] = get; &#125;&#125;int MapToBucket(int x)&#123; return x / 10; // 映射函数f(x)，作用相当于快排中的Partition，把大量数据分割成基本有序的数据块&#125;void CountingSort(int A[], int n)&#123; for (int i = 0; i &lt; bn; i++) &#123; C[i] = 0; &#125; for (int i = 0; i &lt; n; i++) // 使C[i]保存着i号桶中元素的个数 &#123; C[MapToBucket(A[i])]++; &#125; for (int i = 1; i &lt; bn; i++) // 定位桶边界：初始时，C[i]-1为i号桶最后一个元素的位置 &#123; C[i] = C[i] + C[i - 1]; &#125; int *B = (int *)malloc((n) * sizeof(int)); for (int i = n - 1; i &gt;= 0; i--)// 从后向前扫描保证计数排序的稳定性(重复元素相对次序不变) &#123; int b = MapToBucket(A[i]); // 元素A[i]位于b号桶 B[--C[b]] = A[i]; // 把每个元素A[i]放到它在输出数组B中的正确位置上 // 桶的边界被更新：C[b]为b号桶第一个元素的位置 &#125; for (int i = 0; i &lt; n; i++) &#123; A[i] = B[i]; &#125; free(B);&#125;void BucketSort(int A[], int n)&#123; CountingSort(A, n); // 利用计数排序确定各个桶的边界（分桶） for (int i = 0; i &lt; bn; i++) // 对每一个桶中的元素应用插入排序 &#123; int left = C[i]; // C[i]为i号桶第一个元素的位置 int right = (i == bn - 1 ? n - 1 : C[i + 1] - 1);// C[i+1]-1为i号桶最后一个元素的位置 if (left &lt; right) // 对元素个数大于1的桶进行桶内插入排序 InsertionSort(A, left, right); &#125;&#125;int main()&#123; int A[] = &#123; 29, 25, 3, 49, 9, 37, 21, 43 &#125;;// 针对桶排序设计的输入 int n = sizeof(A) / sizeof(int); BucketSort(A, n); printf("桶排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 下图给出了对{ 29, 25, 3, 49, 9, 37, 21, 43 }进行桶排序的简单演示过程 。 ​ 桶排序不是比较排序，不受到O(nlogn)下限的影响，它是鸽巢排序的一种归纳结果，当所要排序的数组值分散均匀的时候，桶排序拥有线性的时间复杂度。 9.4 算法分析​ 桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 10、基数排序（Radix Sort）​ 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 10.1 算法描述 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 10.2 动图演示 10.3 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include&lt;iostream&gt;using namespace std;// 分类 ------------- 内部非比较排序// 数据结构 ---------- 数组// 最差时间复杂度 ---- O(n * dn)// 最优时间复杂度 ---- O(n * dn)// 平均时间复杂度 ---- O(n * dn)// 所需辅助空间 ------ O(n * dn)// 稳定性 ----------- 稳定const int dn = 3; // 待排序的元素为三位数及以下const int k = 10; // 基数为10，每一位的数字都是[0,9]内的整数int C[k];int GetDigit(int x, int d) // 获得元素x的第d位数字&#123; int radix[] = &#123; 1, 1, 10, 100 &#125;;// 最大为三位数，所以这里只要到百位就满足了 return (x / radix[d]) % 10;&#125;void CountingSort(int A[], int n, int d)// 依据元素的第d位数字，对A数组进行计数排序&#123; for (int i = 0; i &lt; k; i++) &#123; C[i] = 0; &#125; for (int i = 0; i &lt; n; i++) &#123; C[GetDigit(A[i], d)]++; &#125; for (int i = 1; i &lt; k; i++) &#123; C[i] = C[i] + C[i - 1]; &#125; int *B = (int*)malloc(n * sizeof(int)); for (int i = n - 1; i &gt;= 0; i--) &#123; int dight = GetDigit(A[i], d); // 元素A[i]当前位数字为dight B[--C[dight]] = A[i]; // 根据当前位数字，把每个元素A[i]放到它在输出数组B中的正确位置上 // 当再遇到当前位数字同为dight的元素时，会将其放在当前元素的前一个位置上保证计数排序的稳定性 &#125; for (int i = 0; i &lt; n; i++) &#123; A[i] = B[i]; &#125; free(B);&#125;void LsdRadixSort(int A[], int n) // 最低位优先基数排序&#123; for (int d = 1; d &lt;= dn; d++) // 从低位到高位 CountingSort(A, n, d); // 依据第d位数字对A进行计数排序&#125;int main()&#123; int A[] = &#123; 20, 90, 64, 289, 998, 365, 852, 123, 789, 456 &#125;;// 针对基数排序设计的输入 int n = sizeof(A) / sizeof(int); LsdRadixSort(A, n); printf("基数排序结果："); for (int i = 0; i &lt; n; i++) &#123; printf("%d ", A[i]); &#125; printf("\n"); return 0;&#125; 下图给出了对{ 329, 457, 657, 839, 436, 720, 355 }进行基数排序的简单演示过程 ​ 基数排序的时间复杂度是O(n * dn)，其中n是待排序元素个数，dn是数字位数。这个时间复杂度不一定优于O(n log n)，dn的大小取决于数字位的选择（比如比特位数），和待排序数据所属数据类型的全集的大小；dn决定了进行多少轮处理，而n是每轮处理的操作数目。 如果考虑和比较排序进行对照，基数排序的形式复杂度虽然不一定更小，但由于不进行比较，因此其基本操作的代价较小，而且如果适当的选择基数，dn一般不大于log n，所以基数排序一般要快过基于比较的排序，比如快速排序。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序并不是只能用于整数排序。 10.4 算法分析​ 基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 ​ 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序算法</tag>
        <tag>排序</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指Offer（一）：二维数组中的查找]]></title>
    <url>%2F2018%2F07%2F12%2F%E5%89%91%E6%8C%87Offer%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[剑指Offer（一）：二维数组中的查找摘要 在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 一、前言本系列文章为《剑指offer》刷题笔记。 刷题平台：牛客网 二、题目在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 1、思路​ 首先选取数组中右上角的数字。如果该数字等于要查找的数字，查找过程结束；如果该数字大于要查找的数组，剔除这个数字所在的列；如果该数字小于要查找的数字，剔除这个数字所在的行。也就是说如果要查找的数字不在数组的右上角，则每一次都在数组的查找范围中剔除一行或者一列，这样每一步都可以缩小查找的范围，直到找到要查找的数字，或者查找范围为空。 2、举例如果在一个二维数组中找到数字7，则返回true，如果没有找到，则返回false。 查找过程如下： 3、编程实现C++： 1234567891011121314151617181920212223class Solution &#123;public: bool Find(int target, vector&lt;vector&lt;int&gt; &gt; array) &#123; int rows = array.size(); int cols = array[0].size(); if(!array.empty() &amp;&amp; rows &gt; 0 &amp;&amp; cols &gt; 0)&#123; int row = 0; int col = cols - 1; while(row &lt; rows &amp;&amp; col &gt;= 0)&#123; if(array[row][col] == target)&#123; return true; &#125; else if(array[row][col] &gt; target)&#123; --col; &#125; else&#123; ++row; &#125; &#125; &#125; return false; &#125;&#125;; Python2.7： 12345678910111213141516171819# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here rows = len(array) cols = len(array[0]) if rows &gt; 0 and cols &gt; 0: row = 0 col = cols - 1 while row &lt; rows and col &gt;= 0: if target == array[row][col]: return True elif target &lt; array[row][col]: col -= 1 else: row += 1 return False]]></content>
      <categories>
        <category>剑指Offer</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
        <tag>数组</tag>
        <tag>笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解朴素贝叶斯]]></title>
    <url>%2F2018%2F07%2F06%2F%E7%90%86%E8%A7%A3%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[理解朴素贝叶斯（1）先验概率、后验概率、联合概率、全概率如果我对这个西瓜没有任何了解，包括瓜的颜色、形状、瓜蒂是否脱落。按常理来说，西瓜成熟的概率大概是 60%。那么，这个概率 P(瓜熟) 就被称为先验概率。 即，先验概率是根据以往经验和分析得到的概率，先验概率无需样本数据，不受任何条件的影响。就像只根据常识而不根据西瓜状态来判断西瓜是否熟，这就是先验概率。 一个判断西瓜是否成熟的常识，就是看瓜蒂是否脱落。一般来说，瓜蒂脱落的情况下，西瓜成熟的概率大一些，大概是 75%。如果把瓜蒂脱落当作一种结果，然后去推测西瓜成熟的概率，这个概率 P(瓜熟 | 瓜蒂脱落) 就被称为后验概率。后验概率类似于条件概率。 知道了先验概率和后验概率，我们再来看看什么是联合概率。P(瓜熟，瓜蒂脱落) 称之为联合分布，它表示瓜熟了且瓜蒂脱落的概率。关于联合概率，满足下列乘法等式： P(瓜熟，瓜蒂脱落) = P(瓜熟**|瓜蒂脱落)**·P(瓜蒂脱落)=P(瓜蒂脱落|瓜熟)·P(瓜熟) 其中，P(瓜熟 | 瓜蒂脱落) 就是刚刚介绍的后验概率，表示在“瓜蒂脱落”的条件下，“瓜熟”的概率。P(瓜蒂脱落 | 瓜熟) 表示在“瓜熟”的情况下，“瓜蒂脱落”的概率。 如何计算瓜蒂脱落的概率呢？实际上可以分成两种情况：一种是瓜熟状态下瓜蒂脱落的概率，另一种是瓜生状态下瓜蒂脱落的概率。瓜蒂脱落的概率就是这两种情况之和。因此，我们就推导出了全概率公式： P(瓜蒂脱落)=P(瓜蒂脱落|瓜熟)·P(瓜熟)+P(瓜蒂脱落|瓜生)·P(瓜生) （2）单个特征判断瓜熟好了，介绍完先验概率、后验概率、联合概率、全概率后，我们来看这样一个问题：西瓜的状态分成两种：瓜熟与瓜生，概率分别为 0.6 与 0.4，且瓜熟里面瓜蒂脱落的概率是 0.8，瓜生里面瓜蒂脱落的概率是 0.4。那么，如果我现在挑到了一个瓜蒂脱落的瓜，则该瓜是好瓜的概率多大？ 显然，这是一个计算后验概率的问题，根据我们上面推导的联合概率和全概率公式，可以求出： 一项一项来看： 条件概率 P(瓜蒂脱落 | 瓜熟) = 0.8 先验概率 P(瓜熟) = 0.6 条件概率 P(瓜蒂脱落 | 瓜生) = 0.4 先验概率 P(瓜生) = 0.4 将以上数值带入上式，得： 注意，以上这种计算后验概率的公式就是利用贝叶斯定理。 （3）多个特征判断瓜熟判断一个瓜是否熟了，除了要看瓜蒂是否脱落，还要看瓜的形状和颜色。形状有圆和尖之分，颜色有深绿、浅绿、青色之分。我们可以使用刚刚引入的贝叶斯定理思想来尝试解决这个问题。 现在，特征由原来的 1 个，变成现在的 3 个，我们用 X 表示特征，用 Y 表示瓜的类型（瓜熟还是瓜生）。则根据贝叶斯定理，后验概率 P(Y=ck | X=x) 的表达式为： 其中，ck 表示类别，k 为类别个数。本例中，k = 1，2，c1 表示瓜熟，c2 表示瓜生。上面的公式看似有点复杂，但其实与上一节单特征（瓜蒂是否脱落）的形式是一致的。 有一点需要注意，这里的特征 X 不再是单一的，而是包含了 3 个特征。因此，条件概率 P(X=x | Y=ck) 假设各个条件相互独立，也就是说假设不同特征之间是相互独立的。这样，P(X=x | Y=ck) 就可以写成： 其中，n 为特征个数，j 表示当前所属特征。针对这个例子，P(X=x | Y=ck) 可以写成： 这种条件独立性的假设就是朴素贝叶斯法“朴素”二字的由来。这一假设让朴素贝叶斯法变得简单，但是有时候会牺牲一定的分类准确率。 ​ 利用朴素贝叶斯思想，我们就可以把后验概率写成： 上面的公式看上去比较复杂，其实只是样本特征增加了，形式上与上一节 P(瓜熟 | 瓜蒂脱落) 是一致的。 现在，一个西瓜，观察了它的瓜蒂、形状、颜色三个特征，就能根据上面的朴素贝叶斯公式，分别计算 c1（瓜熟）和 c2（瓜生）的概率，即 P(Y=c1 | X=x) 和 P(Y=c2 | X=x)。然后再比较 P(Y=c1 | X=x) 和 P(Y=c2 | X=x) 值的大小： 若 P(Y=c1 | X=x) &gt; P(Y=c2 | X=x)，则判断瓜熟； 若 P(Y=c1 | X=x) &lt; P(Y=c2 | X=x)，则判断瓜生。 值得注意的是上式中的分母部分，对于所有的 ck 来说，都是一样的。因此，分母可以省略，不同的 ck，仅比较 P(Y=ck | X=x) 的分子即可： （2）朴素贝叶斯分类买瓜之前，还有一件事情要做，就是搜集样本数据。通过网上资料和查阅，获得了一组包含 10 组样本的数据。这组数据是不同瓜蒂、形状、颜色对应的西瓜是生是熟。我把这组数据当成是历史经验数据，以它为标准。 其中，瓜蒂分为脱落和未脱，形状分为圆形和尖形，颜色分为深绿、浅绿、青色。不同特征组合对应着瓜熟或者瓜生。 现在，挑了一个西瓜，它的瓜蒂脱落、形状圆形、颜色青色。这时候，就完全可以根据样本数据和朴素贝叶斯法来计算后验概率。 首先，对于瓜熟的情况： 瓜熟的先验概率： P(瓜熟) = 6 / 10 = 0.6。 条件概率： P(脱落 | 瓜熟) = 4 / 6 = 2 / 3。 条件概率： P(圆形 | 瓜熟) = 4 / 6 = 2 / 3。 条件概率： P(青色 | 瓜熟) = 2 / 6 = 1 / 3。 计算后验概率分子部分：P(瓜熟) × P(脱落 | 瓜熟) × P(圆形 | 瓜熟) × P(青色 | 瓜熟) = 0.6 × (2 / 3) × (2 / 3) × (1 / 3) = 4 / 45。 然后，对于瓜生的情况： 瓜生的先验概率： P(瓜生) = 4 / 10 = 0.4。 条件概率： P(脱落 | 瓜生) = 1 / 4 = 0.25。 条件概率： P(圆形 | 瓜生) = 1 / 4 = 0.25。 条件概率： P(青色 | 瓜生) = 1 / 4 = 0.25。 计算后验概率分子部分：P(瓜生) × P(脱落 | 瓜生) × P(圆形 | 瓜生) × P(青色 | 瓜生) = 0.4 × 0.25 × 0.25 × 0.25 = 1 / 160。 因为 4 / 45 &gt; 1 / 160，所以预测为瓜熟。终于计算完了，很肯定这个西瓜瓜蒂脱落、形状圆形、颜色青色，应该是熟瓜。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名解析]]></title>
    <url>%2F2018%2F07%2F03%2F%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[域名解析域名有钱最好买.com的，不要买.cn的，我自己买的.cn的好多坑，.top的相对就比较便宜（low）。 此次在腾讯云买的dadavision.cn。 之前.cn的一直要我备案，否则DNS解析不了，而且还证书审核失败。 在github项目的setting里面设置域名 添加域名解析 这里注意 在添加绑定域名后需要注意以下问题（未绑定可以不用） 需要将github上CNAME里面内容在文件工程中GitBlog\source\CNAME 这里注意新建的CNAME不要有后缀名。]]></content>
      <categories>
        <category>域名解析</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>next</tag>
        <tag>教程</tag>
        <tag>域名</tag>
        <tag>域名解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pytorch0.4.0升级概述新手教程]]></title>
    <url>%2F2018%2F06%2F20%2FPytorch0-4-0%E5%8D%87%E7%BA%A7%E6%A6%82%E8%BF%B0%E6%96%B0%E6%89%8B%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[pytorch0.4支持了Windows系统的开发，在首页即可使用pip安装pytorch和torchvision。 说白了，以下文字就是来自官方文档60分钟入门的简要翻译. pytorch是啥python的科学计算库，使得NumPy可用于GPU计算，并提供了一个深度学习平台使得灵活性和速度最大化 入门Tensors(张量) Tensors与NumPy的ndarrays类似，另外可以使用GPU加速计算 未初始化的5*3的矩阵:x = torch.empty(5, 3) 随机初始化的矩阵:x = torch.rand(5, 3) 全零矩阵,定义数据类型:x = torch.zeros(5, 3, dtype=torch.long) 由数据构造矩阵:x = torch.tensor([5.5, 3]) 由已存在张量构造矩阵，性质与之前张量一致: 12x = x.new_ones(5, 3, dtype=torch.double) x = torch.randn_like(x, dtype=torch.float) 获取维度:print(x.size()) Operations有多种operation的格式，这里考虑加法 12y = torch.rand(5, 3)print(x + y) 1print(torch.add(x, y)) 123result = torch.empty(5, 3)torch.add(x, y, out=result)print(result) 123# adds x to yy.add_(x)print(y) operations中需要改变张量本身的值，可以在operation后加,比如`x.copy(y), x.t_()` 索引:print(x[:, 1]) 改变维度:x.view(-1, 8) 和Numpy的联系torch tensor 和 numpy array之间可以进行相互转换，他们会共享内存位置，改变一个，另一个会跟着改变。 tensor to array1234a = torch.ones(5)b = a.numpy()a.add_(1)print(a,b) array to tensor123456import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b) CUDA Tensorstensor可以使用.to方法将其移动到任何设备。 123456789# let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # a CUDA device object y = torch.ones_like(x, device=device) # directly create a tensor on GPU x = x.to(device) # or just use strings ``.to(&quot;cuda&quot;)`` z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double)) # ``.to`` can also change dtype together! Autograd(自动求导)pytorch神经网络的核心模块就是autograd，autograd模块对Tensors上的所有operations提供了自动求导。 Tensortorch.Tensor是模块中的核心类，如果设置属性.requires_grad = True,开始追踪张量上的所有节点操作，指定其是否计算梯度。使用.backward()方法进行所有梯度的自动求导，张量的梯度会累积到.grad属性中。 .detach()停止张量的追踪，从梯度计算中分离出来；另外在评估模型时一般使用代码块with torch.no_grad():,因为模型中通常训练的参数也会有.requires_grad = True,这样写可以停止全部张量的梯度更新。 Function类是autograd的变体，Tensor和Function相互交错构建成无环图，编码了完整的计算过程，每个Variable(变量)都有.grad_fn属性，引用一个已经创建了的Tensor的Function. 如上，使用.backward()计算梯度。如果张量是一个标量(只有一个元素),不需要对.backward()指定参数；如果张量不止一个元素，需要指定.backward()的参数，其匹配张量的维度。 1234567891011121314151617import torchx = torch.ones(2, 2, requires_grad=True)print(x)y = x + 2print(y)print(y.grad_fn)z = y * y * 3out = z.mean()print(z, out)a = torch.randn(2, 2)a = ((a * 3) / (a - 1))print(a.requires_grad)a.requires_grad_(True) # 改变a张量内在的属性print(a.requires_grad)b = (a * a).sum()print(b.grad_fn) Gradients反向传播时，由于out是一个标量，out.backward()等效于out.backward(torch.tensor(1)) 123456789101112131415161718192021out.backward()print(x.grad)x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000: y = y * 2print(y)gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(gradients)print(x.grad)print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad(): print((x ** 2).requires_grad) 神经网络神经网络可以用torch.nn构建。nn依赖于autograd定义模型和求导，nn.Module定义网络层，方法forward(input)返回网络输出。 举例说明，如下是对数字图片分类的卷积网络架构。 这是一个简单的前馈神经网络，将输入数据依次通过几层网络层后最终得到输出。 神经网络典型的训练步骤如下： 定义神经网络及学习的参数(权重) 迭代输入数据 将输入数据输入到网络结构中 计算代价函数 误差向后传播 更新网络权重 weight = weight - learning_rate * gradient 定义网络123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) out: 1234567Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 可以仅定义forward()函数，当使用autograd时backward()被自动定义。可以在forward()函数中使用任何operation操作。 net.parameters()返回模型中的可学习参数。 123params = list(net.parameters())print(len(params))print(params[0].size()) # conv1&apos;s .weight 使所有参数的梯度归零然后开始计算梯度 12net.zero_grad()out.backward(torch.randn(1, 10)) 代价函数代价函数将(output,target)作为输入，计算output与target之间的距离。 nn模块中有几种不同的代价函数选择，最简单的是nn.MSELoss，计算均方误差 eg： 1234567output = net(input)target = torch.arange(1, 11) # a dummy target, for exampletarget = target.view(1, -1) # make it the same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target)print(loss) 按照向后传播的方向传播loss，使用grad_fn可以查看整个流程的计算图 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 使用loss.backward()，流程中所有requres_grad=True的张量累积它的梯度至.grad 123print(loss.grad_fn) # MSELossprint(loss.grad_fn.next_functions[0][0]) # Linearprint(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 向后传播loss.backward()传播误差， 123456789net.zero_grad() # zeroes the gradient buffers of all parametersprint(&apos;conv1.bias.grad before backward&apos;)print(net.conv1.bias.grad)loss.backward()print(&apos;conv1.bias.grad after backward&apos;)print(net.conv1.bias.grad) 更新权重误差每次传播后，需要对权重进行更新，简单的更新方式如下： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) torch.optim实现了这一过程，并有着不同的更新规则GD, Nesterov-SGD, Adam, RMSProp， 1234567891011import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update note: 每次迭代时由于梯度的累积，需要手动将梯度归零optimizer.zero_grad()]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch</tag>
        <tag>Pytorch0.4.0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[唐老师研究生毕业演讲]]></title>
    <url>%2F2018%2F06%2F20%2F%E5%94%90%E8%80%81%E5%B8%88%E7%A0%94%E7%A9%B6%E7%94%9F%E6%AF%95%E4%B8%9A%E6%BC%94%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[尊敬的何院士、各位嘉宾、各位老师、同学们： 大家下午好！]]></content>
      <categories>
        <category>毕业演讲</category>
      </categories>
      <tags>
        <tag>毕业演讲</tag>
        <tag>唐老师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10+Anaconda+tensorflow1.8+pytorch0.4.0注意事项]]></title>
    <url>%2F2018%2F06%2F12%2Fwin10-Anaconda-tensorflow1-8-pytorch0-4-0%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[注意事项 Anaconda会自带python3.6，所以为了避免环境配置乱套，删除本机独立安装的python。 不要按网上配置虚拟环境，比如TensorFlow配置一个虚拟环境，pytorch配置一个虚拟环境……keras……cafee……只要一个环境即可。 win10+Anaconda3-5.2.0+tensorflow1.8+pytorch0.4.0 在windows PowerShell中确定python环境在Anaconda中 Anaconda注意：自动添加到环境变量其它事项一路安装，普通套路。 TensorFlow1、TensorFlow或者pytorch，无论什么框架，越新越好，新的功能多bug少，这里用的TensorFlow1.8。 2、TensorFlow和pytorch最好都用conda装，但是这里conda install tensorflow只会给你比较低的版本，所以TensorFlow安装在这里使用了：1conda install --channel https://conda.anaconda.org/conda-forge tensorflow 可能会下载很慢，如果有部分下载未成功，重新执行命令即可，之前下载的会保留。 Pytorch同样遵循使用最新框架原则。我自己电脑用的pytorch-CPU版本，所以执行：12conda install conda install pytorch-cpu -c pytorchpip install torchvision 在实验室服务器用的是cuda9.0版本12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 安装成功之后效果： PyCharm注意事项在不用虚拟幻境情况下，好处就是不用切换环境配置了，直接一个 Anaconda\python.exe全部搞定，如图： 附录鄙视链：caffe-pytorch-tensorflow-caffe2- caffetorch-slim-tensorflow-keras]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Anaconda</tag>
        <tag>tensorflow</tag>
        <tag>pytorch</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch0.4.0重大更新]]></title>
    <url>%2F2018%2F06%2F11%2FPyTorch0.4.0%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[一、重大核心变化包括 Tensor/Variable 合并 零维张量 数据类型 迁移指南 二、现添加的新特征包括 Tensors： 全面支持高级索引 快速傅立叶变换 神经网络： 计算时的存储权衡 bottleneck-识别代码中热点（hotspots）的工具 torch.distributions 24 个基础的概率分布 增加cdf、方差、信息熵、困惑度等 分布式训练 易于使用的 Launcher utility NCCL2 后端 C++拓展 Windows 支持 ONNX 改进 RNN 支持 三、性能改进 四、Bug 修复 五、torchvision的一些变化1.torchvision.transform中函数torchvision.transforms.`Scale(*args, **kwargs)即将被函数torchvision.transforms.Resize`(size, interpolation=2)代替。（参考官方文档：点击打开链接） 2.torchvision.transform中函数torchvision.transforms.`RandomSizedCrop(*args, **kwargs)即将被函数torchvision.transforms.RandomResizedCrop`(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)代替。（参考官方文档：点击打开链接） 六、安装方式——Windows安装【方法一】pip直接安装。官网（点击打开链接）给出的安装步骤如下图所示（根据CUDA版本以及Python版本选择）。 Run this command: 12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 这里我是下载好torch-0.4.0-cp36-cp36m-win_amd64.whl文件到本地之后才安装的。进入Anaconda Prompt，然后进入文件所在目录： 打开cmd命令提示符，先利用anaconda创建一个虚拟环境，命名为pytorch4 1conda create -n pytorch4 python=3.6 激活刚才创建好的虚拟环境 1activate pytorch4 安装pytorch0.4.0 1pip install torch-0.4.0-cp35-cp35m-win_amd64.whl 注：根据自己的配置选择whl下载来链接 安装torchvision1pip install torchvision 简单测试安装是否成功123pythonimport torchprint(torch.__version__) 如果输出0.4.0，那么恭喜Windows下的PyTorch0.4.0安装成功！]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>笔记</tag>
        <tag>PyTorch</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mnist手写数字识别]]></title>
    <url>%2F2018%2F06%2F09%2FMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[minist_mpl.py1234567891011121314151617181920212223242526272829303132333435 #encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense,Activation,Dropoutfrom keras.optimizers import SGD,Adadeltafrom keras.models import save_modelimport matplotlib.pyplot as plt(x_train,y_train),(x_test,y_test) = mnist.load_data()x_train = x_train.reshape(60000,28*28).astype('float32') #转换数据格式x_test = x_test.reshape(10000,28*28).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train,10) #one-hot编码y_test = keras.utils.to_categorical(y_test,10)print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)modle = Sequential()#第一层隐层，64个神经元modle.add(Dense(256,activation='relu',input_dim=28*28))#第二层隐层，64个神经元modle.add(Dense(256,activation='relu'))modle.add(Dropout(0.5))#输出层，10个神经元modle.add(Dense(10,activation='softmax'))sgd = SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)modle.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy'])modle.fit(x_train,y_train,epochs=10,batch_size=128)score = modle.evaluate(x_test,y_test,batch_size=128)print(score)modle.save('MLP_minist.h5') 123456789101112131415161718192021222324252627282930313233343536373839#encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequential,save_modelfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2Dfrom keras.optimizers import SGD,Adadelta(x_train,y_train),(x_test,y_test) = mnist.load_data() #加载数据print(x_train.shape,x_test.shape)x_train = x_train.reshape(60000,28,28,1).astype('float32') #二维数据x_test = x_test.reshape(10000,28,28,1).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train) #one-hot编码y_test = keras.utils.to_categorical(y_test)num_classes = y_test.shape[1]model = Sequential() #创建序列模型model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1))) #第一层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Conv2D(64,(3,3),activation='relu')) #第二层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Flatten()) #铺平当前节点model.add(Dense(128,activation='relu')) #全连接层model.add(Dropout(0.5)) #随机失活model.add(Dense(num_classes,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) #编译模型model.fit(x_train,y_train,batch_size=128,epochs=10) #训练模型score = model.evaluate(x_test,y_test,batch_size=128) #评价模型print(score) #打印分类准确率model.save('CNN_minist.h5')]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Keras</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交叉验证集、测试集]]></title>
    <url>%2F2018%2F06%2F09%2F%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%2F</url>
    <content type="text"><![CDATA[什么有交叉验证：主要是因为训练集较小。无法直接像前面那样只分出训练集，验证集，测试就可以了（简单交叉验证）。 最佳的数据分类情况是把数据集分为三部分，分别为：训练集(train set)，验证集(validation set)和测试集(test set) 验证集和测试集两者的主要区别是：验证集用于进一步确定模型中的超参数(例如正则项系数、ANN{Artificial Neural Network}中隐含层的节点个数、网络层数、迭代次数、学习率)而测试集只是用于评估模型的精确度(即泛化能力)！ 举个例子：假设建立一个BP神经网络，对于隐含层的节点数目，我们并没有很好的方法去确定。一般将节点数设定为某一具体的值，通过训练集训练出相应的参数后，再由交叉验证集去检测该模型的误差；然后再改变节点数，重复上述过程，直到交叉验证误差最小。此时的节点数可以认为是最优节点数，即该节点数(这个参数)是通过交叉验证集得到的。 而测试集是在确定了所有参数之后，根据测试误差来评判这个学习模型的；也可以说是用来评估模型的泛化能力。所以，验证集主要主要是用于模型的调参。 “交叉验证法“ (cross validation)先将数据集D 划分为k 个大小相似的互斥子集， 即D = D**1 ∪ D**2 ∪**… ∪ Dk, Di ∩ Dj = ø (i≠j) . 每个子集Di 都尽可能保持数据分布的一致性，即从D 中通过分层采样得到. 然后，每次用k-1 个子集的并集作为训练集，余下的那个子集作为测试集;这样就可获得k组训练/测试集，从而可进行k 次训练和测试，最终返回的是这k 个测试结果的均值**。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为” k 折**/倍**交叉验证“ (k-fold cross validation). k 最常用的取值是10 ，此时称为10折交叉验证; 其他常用的k 值有5、20 等. 为了避免其他属性携带的信息被训练集中未出现的属性值”抹去”，在估计概率值时通常要进行”平滑” (smoothing) ，常用”拉普拉斯修正“，所以，P(c)和P(xi|c)修正为： 其中N 表示训练集D 中可能的类别数，Ni表示第 与留出法相似，将数据集D 划分为k 个子集同样存在多种划分方式.为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p 次。最终的评估结果是这p 次k 折交叉验证结果的均值，例如常见的有”10 次10 折交叉验证。 假定数据集D中包含m个样本，若令k=m ，则得到了交叉验证法的一个特例:留一法(Leave- One-Out比，简称LOO) . 显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集**—**每个子集包含一个样本;留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D 训练出的模型很相似.因此，留一法的评估结果往往被认为比较准确.然而，留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1 百万个样本，则需训练1 百万个模型)，而这还是在未考虑算法调参的情况下.另外，留一法的估计结果也未必永远比其他评估方法准确;”没有免费的午餐”定理对实验评估方法同样适用.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP-单词拆分 I]]></title>
    <url>%2F2018%2F06%2F09%2FDP-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20I%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。说明：拆分时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入: s = “leetcode”, wordDict = [“leet”, “code”]输出: true解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 示例 2：输入: s = “applepenapple”, wordDict = [“apple”, “pen”]输出: true解释: 返回 true 因为 “applepenapple” 可以被拆分成 “apple pen apple”。注意你可以重复使用字典中的单词。 示例 3：输入: s = “catsandog”, wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出: false 算法思路动态规划的思路：将问题拆分成更小的子问题。用dp[i]表示0到i的子字符串是否可以拆分成满足条件的单词，在计算dp[i]的时候，我们已经知道dp[0],dp[1],…,dp[i-1],如果以i为结尾的j~i子串是满足条件的，并且0~j的子串也是在字典中的，那么dp[i]就是true。用公式表示就是： dp[j]&amp;&amp;s.substring[j,i+1]∈dict DP实现123456789101112131415class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123;; boolean [] dp = new boolean[s.length()+1]； dp[0] = true; for(int i = 0; i &lt; s.length(); i++)&#123; for(int j = 0; j &lt;= i; j++)&#123; if(dp[j] &amp;&amp; wordDict.contains(s.substring(j, i+1)))&#123; dp[i+1] = true; break; &#125; &#125; &#125; return dp[s.length()]; &#125;&#125; DFS解法，超时1234567891011121314151617181920212223242526class Solution &#123; boolean dfs(String s, List&lt;String&gt; wordDict, int index)&#123; // 超时 String left = s.substring(index, s.length()); if(wordDict.contains(left))&#123; return true; &#125; List&lt;Integer&gt; list = new ArrayList(); for(int i = index; i &lt; s.length(); i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; list.add(i+1); &#125; &#125; for(Integer each:list) &#123; if(dfs(s, wordDict, each))&#123; return true; &#125; &#125; return false; &#125; boolean flag = false; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; return dfs(s, wordDict, 0); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP+回溯-单词拆分 II]]></title>
    <url>%2F2018%2F06%2F09%2FDP%2B%E5%9B%9E%E6%BA%AF-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20II%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，在字符串中增加空格来构建一个句子，使得句子中所有的单词都在词典中。返回所有这些可能的句子。说明：分隔时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入:s = “catsanddog”wordDict = [“cat”, “cats”, “and”, “sand”, “dog”]输出:[“cats and dog”,“cat sand dog”] 示例 2：输入:s = “pineapplepenapple”wordDict = [“apple”, “pen”, “applepen”, “pine”, “pineapple”]输出:[“pine apple pen apple”,“pineapple pen apple”,“pine applepen apple”]解释: 注意你可以重复使用字典中的单词。 示例 3：输入:s = “catsandog”wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出:[] 算法思路这道题类似 Word Break I 判断是否能把字符串拆分为字典里的单词 @LeetCode 只不过要求计算的并不仅仅是是否能拆分，而是要求出所有的拆分方案。因此用递归。但是直接递归做会超时，原因是LeetCode里有几个很长但是无法拆分的情况，所以就先跑一遍Word Break I，先判断能否拆分，然后再进行拆分。 DP实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 class Solution &#123; boolean isBreak(String s, List&lt;String&gt; wordDict) &#123; boolean[] canBreak = new boolean[s.length()+1]; canBreak[0] = true; for(int i=1; i&lt;=s.length(); i++) &#123; boolean flag = false; for(int j=0; j&lt;i; j++) &#123; if(canBreak[j] &amp;&amp; wordDict.contains(s.substring(j,i))) &#123; flag = true; break; &#125; &#125; canBreak[i] = flag; &#125; return canBreak[s.length()]; &#125; void dfs(String s, List&lt;String&gt; wordDict, String str, int index)&#123; String result = str; //记录字符串状态 int len = s.length(); String tmp = s.substring(index, len); if(wordDict.contains(tmp))&#123; //最后一段存在于字典中，则保存结果 str += tmp; res.add(str); &#125; List&lt;Integer&gt; listIndex = new ArrayList(); List&lt;String&gt; listStr = new ArrayList(); for(int i = index; i &lt; len; i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; listIndex.add(i+1); listStr.add(temp); &#125; &#125; String temp = result; //保存递归前的字符串状态，以便回溯 for(int i = 0; i &lt; listIndex.size(); i++)&#123; result += listStr.get(i) + " "; dfs(s, wordDict, result, listIndex.get(i)); result = temp; &#125; &#125; List&lt;String&gt; res = new ArrayList(); public List&lt;String&gt; wordBreak(String s, List&lt;String&gt; wordDict) &#123; if(!isBreak(s, wordDict)) return res; String str = ""; dfs(s, wordDict, str, 0); return res; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查准率precision和查全率recall、F1]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9F%A5%E5%87%86%E7%8E%87precision%E5%92%8C%E6%9F%A5%E5%85%A8%E7%8E%87recall%E3%80%81F1%2F</url>
    <content type="text"><![CDATA[真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），分别用TP、FP、TN、FN表示相应样例数，样例总数=TP+FP+TN+FN；分类结果混淆矩阵： 真实情况 预测结果 正例 反例 正例 TP（真正例） FN（假反例） 反例 FP（假正例） TN（真反例） 查准率（precision）：被认定为正例的里面，判断正确的比例。 查全率（recall）：真实正例里，被判断出为正例的比例。 查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低,可以通过P-R曲线来取两者的平衡值 若一个学习器P-R曲线被另一个学习器的曲线完全”包住“,则可断言后者的性能优于前者， 例如图中学习器A 的性能优于学习器C; 如果两个学习器的P-R 曲线发生了交叉7,例如图中的A 与B ，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较然而，在很多情形下，人们往往仍希望把学习器A 与B 比出个高低. 这时一个比较合理的判据是比较P-R 曲线节面积的大小。 “平衡点“是”查准率=查全率“时的取值。 但更常用的使用F1来衡量查准率与查全率； F1基于查准率与查全率的调和平均： ，sum为样例总数， 具体应用中可能对P和R有不同的倚重。比如商品推荐中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，这时候查准率更重要。而在逃犯检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。 F1度量的一般形式F**β（加权调和平均**）就可以表达这种偏好。 即 ß = 1时退化为标准的F1,当β&gt;1意味着P占比重更大，反之则是R。 ROC、AUCROC:全称“受试者工作特征”，表达了模型的泛化能力。其纵坐标为“TPR真正例率”；横坐标为“FPR假正例率”。 ROC曲线根据模型的排序结果，一个个划分正负，每次得出两个值TPR,FPR。很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。为啥在ROC曲线画一个y=x呢，那表示随机猜测的ROC。 AUC:ROC曲线下的面积.AUC=1,是完美分类器(并不存在)；0.5。AUC 越大，意味着辨别能力越强。 与P-R图相似，如果一条ROC曲线包含另一条ROC曲线，则前者的学习器性能更优越。如果曲线有交叉，则可以通过计算AUC大小得到。 代价敏感错误率、代价曲线代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>机器学习</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子串-最长公共子序列]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[最长公共子串DP实现1234567891011121314151617181920212223242526272829public static int maxSubStr(String str1, String str2) &#123; int result = 0; int index = 0; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int [len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; dp[i][j] = dp[i-1][j-1] + 1; // 状态转移 if(dp[i][j] &gt; result) &#123; result = dp[i][j]; index = i; // 记录最大子串的最后一个下标 &#125; // result = result &gt; dp[i][j] ? result : dp[i][j]; &#125;else &#123; dp[i][j] = 1; result = result &gt; dp[i][j] ? result : dp[i][j]; &#125; &#125; &#125; &#125; System.out.println(result); for(int i = index - result + 1; i &lt;= index; i++) &#123; System.out.print(str1.charAt(i) + " "); &#125; return result;&#125; 最长公共子序列####动态规划假设Z=&lt;z1,z2,⋯,zk&gt;是X与Y的LCS， 我们观察到如果Xm=Yn，则Zk=Xm=Yn，有Zk−1是Xm−1与Yn−1的LCS；如果Xm≠Yn，则Zk是Xm与Yn−1的LCS，或者是Xm−1与Yn的LCS。因此，求解LCS的问题则变成递归求解的两个子问题。但是，上述的递归求解的办法中，重复的子问题多，效率低下。改进的办法——用空间换时间，用数组保存中间状态，方便后面的计算。这就是动态规划（DP)的核心思想了。DP求解LCS用二维数组c[i][j]记录串x1x2⋯xi与y1y2⋯yj的LCS长度，则可得到状态转移方程 DP实现1234567891011121314151617181920public static int maxSubSequence(String str1, String str2) &#123; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int[len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else &#123; dp[i][j] = dp[i-1][j] &gt; dp[i][j-1] ? dp[i-1][j] : dp[i][j-1]; &#125; &#125;else if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = 1; &#125; &#125; &#125; System.out.println(dp[len1 - 1][len2 - 1]); return dp[len1-1][len2-1];&#125;]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫]]></title>
    <url>%2F2018%2F06%2F09%2FPython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[文本爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#encoding:utf-8import requests,urllib3.request,time,osimport random,csv,socket,http.clientfrom bs4 import BeautifulSoupdef get_contend(url, data = None): #获取网页中html代码 header=&#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'zh-CN,zh;q=0.9', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; timeout = random.choice(range(80,180)) while True: try: rep = requests.get(url,headers = header,timeout=timeout) rep.encoding = 'utf-8' break except socket.timeout as e: print ('3',e) time.sleep(random.choice.range(8,15)) except socket.error as e: print ('4',e) time.sleep(random.choice.range(20,60)) except http.client.BadStatusLine as e: print ('5',e) time.sleep(random.choice.range(30,80)) except http.client.IncompleteRead as e: print ('6',e) time.sleep(random.choice.range(5,15)) return rep.textdef get_data(html_text): final = [] bs = BeautifulSoup(html_text,'html.parser') #创建BeautifulSoup对象 body = bs.body #获取body部分 data = body.find('div',&#123;'id':'7d'&#125;) #找到需要爬取部分的div ul = data.find('ul') #获取ul部分 li = ul.find_all('li') #获取所有的li for day in li: #对li标签中内容进行遍历 temp = [] date =day.find('h1').string #找到日期 temp.append(date) #将日期添加到temp中 p = day.find_all('p') #找到每个li中的所有p标签 temp.append(p[0].string,) #第一个p标签中的天气状况添加到temp if p[1].find('span') == None: t_highest = None else: t_highest = p[1].find('span').string #找到最高温 t_highest = t_highest.replace('C','') t_lowest = p[1].find('i').string # 找到最低温 t_lowest = t_lowest.replace('C','') temp.append(t_highest) temp.append(t_lowest) final.append(temp) return finaldef write_data(data,name): #将数据写入文件 file_name = name with open(file_name, 'a', errors='ignore', newline='') as f: f_csv = csv.writer(f) f_csv.writerows(data)if __name__ == '__main__': url = 'http://www.weather.com.cn/weather/101190401.shtml' html = get_contend(url) result = get_data(html) print(result) write_data(result,'weather.csv') 图虫图片爬取按标签爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273__author__ = 'Result_Lv'#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_album_name(json_dict): album_name = [] postlist = json_dict['postList'] for i in range(len(postlist)): if postlist[i]['title'] == '': #图集标题为空时，命名为默认 album_name.append('Default' + str(i)) else: album_name.append(postlist[i]['title']) return album_namedef get_photo_id(json_dict): #获得所有照片的ID author_id = [] album_id = [] post_list = json_dict['postList'] for i in range(len(post_list)): #获取每个图集的照片ID photo_id = [] author_id.append(post_list[i]['author_id']) # 获取每个图集作者ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list album_id.append(photo_id) return author_id,album_iddef download_album(path,album_name,author_id,album_id): #下载图集 for i in range(len(album_id)): if not os.path.exists(path + album_name[i]): #若不存在对应图集的文件夹 try: os.makedirs(path + album_name[i]) #以图集名创建文件夹 except OSError as e: print(e) continue print('正在下载第' + str(i + 1) + '个图册:' + album_name[i]) for j in range(len(album_id[i])): fileurl = 'https://photo.tuchong.com/' + str(author_id[i]) +'/f/' + str(album_id[i][j]) + '.jpg' #生成每张照片Url filename = path + album_name[i] + '/' + str(j+1) + '.jpg' #命名照片 print(' 正在下载第' + str(j+1) + '张照片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 #爬取页数 path = 'F:/少女/' #存放路径 for i in range(page): url = 'https://tuchong.com/rest/tags/少女/posts?page=' + str(i+1) + '&amp;count=20&amp;order=weekly' #tag的json地址 json_dict = get_json(url) album_name = get_album_name(json_dict) para = get_photo_id(json_dict) author_id = para[0] album_id = para[1] download_album(path,album_name,author_id,album_id) 按作者爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): #解析json header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_all_photo_id(json_dict): #获得所有照片的ID #post_id = [] photo_id = [] post_list = json_dict['post_list'] author_id = post_list[0]['author_id'] #获取作者ID author_name = post_list[0]['site']['name'] #获取作者姓名 # for i in range(len(post_list)): #获取所有图集ID # post_id.append(post_list[i]['post_id']) for i in range(len(post_list)): #获取每个图集的照片ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list return author_name,author_id,photo_iddef download_photo(path,author_id,photo_id): #下载全部照片 if not os.path.exists(path): os.makedirs(path) for i in range(len(photo_id)): filename = path + '/' + str(i+1) + '.jpg' fileurl = 'https://photo.tuchong.com/' + str(author_id) + '/f/' + str(photo_id[i]) + '.jpg' print(' 第' + str(i + 1) + '张图片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 for i in range(page): url = 'https://thomaskksj.tuchong.com/rest/2/sites/395013/posts?count=20&amp;page=' + str(i + 1) #作者主页的json地址 print('正在下载第' + str(i+1) + '页:' + url) json_dict = get_json(url) para = get_all_photo_id(json_dict) author_name = para[0] author_id = para[1] photo_id = para[2] path = 'F:/' + author_name + '/page' + str(i + 1) download_photo(path,author_id,photo_id)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偏差、方差、噪声]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%81%8F%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。 偏差（Bias）和方差（Variance）偏差（Bias）：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。 方差（Variance）：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。 偏差：形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与指标； 方差：形容数据分散程度的，算是“无监督的”，客观的指标。 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度. 偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的. 给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小. 一般来说偏差与方差是有冲突的，这称为偏差—方差窘境，给定学习任务，假定我们能控制学习算法的训练程度（例如决策树可控制层数，神经网络可控制训练轮数，集成学习方法可控制基学习器个数），则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以便学习器产生显著变化，此时偏差主导了泛化错误率;随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率;在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合. 为什么KNN（k最近邻k-Nearest Neighbor）算法在增大k时，偏差会变大；但RF（Random Forest随机森林）增大树的数目时偏差却保持不变；GBDT（Gradient Boosting）在增大树的数目时偏差却又能变小。 对于KNN算法，k值越大，表示模型的学习能力越弱，因为k越大，它越倾向于从“面”上考虑做出判断，而不是具体地考虑一个样本近身的情况来做出判断，所以，它的偏差会越来越大。 对于RF，我们实际上是部分实现了多次训练取均值的效果，每次训练得到的树都是一个很强的学习者，每一个的方差都比较大，但综合起来就会比较小。好比一个很强的学习者学习时，刮着西风，它会据此调整自己的瞄准方法，另一个很强的学习者学习时刮着东风，（西风、东风可以理解为不同训练集中的噪声）它也会据此调整自己的瞄准方法，在测试样本时，一个误差向西，一个误差向东，刚好起到互相抵消的作用，所以方差会比较小。但是由于每棵树的偏差都差不多，所以，我们取平均时，偏差不会怎么变化。 为什么说是部分实现了多次训练取均值的效果而不是全部呢？因为我们在训练各棵树时，是通过抽样样本集来实现多次训练的，不同的训练集中不可避免地会有重合的情况，此时，就不能认为是独立的多次训练了，各个训练得到的树之间的方差会产生一定的相关性，训练集中重合的样本越多，则两棵树之间的方差的相关性越强，就越难达成方差互相抵消的效果。 对于GBDT，N棵树之间根本就不是一种多次训练取均值的关系，而是N棵树组成了相关关联，层层递进的超级学习者，可想而知，它的方差一定是比较大的。但由于它的学习能力比较强，所以，它的偏差是很小的，而且树的棵树越多，学习能力就越强，偏差就越小。也就是说，只要学习次数够多，预测的均值会无限接近于目标。简单讲就是GBDT的N棵树实际上是一个有机关联的模型，不能认为是N个模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>机器学习</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分糖果]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%88%86%E7%B3%96%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[题目描述o 分糖果：科大讯飞第二道编程题 o 小明和小红是好朋友，但最近遇到一个棘手的问题，有一盒糖果要分成两份但是每颗糖果质量都不尽相同， o 但为了分配的公平每份糖的糖果数量相差不得超过1，在此条件下两份糖果的质量差距尽可能小。 o 输入一行数，包含一个数n，代表糖果数量，后面一次是n个整数一次表示每个糖果的质量，每个糖果的质量都是1到450 o 之间的一个整数，每盒最多有20个糖果。 o 输出：每个样例输出两个数字分别为两堆糖果的质量，如不相同，先小后大。 o 样例：输入：5 9 6 5 8 7 o 输出：17 18 算法思想o 回溯，在数量差值为1的结果中找出最小的质量差 Python实现1234567891011121314151617def divide(candies, num, select, sum, total, index): global min global res if(abs(total-sum*2) &lt; min): res = sum min = abs(total-sum*2) result.append(select) for i in range(len(candies)): if(index == num-1): return3 select.append(candies[index]) sum += candies[index] temp = select.copy() if(len(select) &lt;= int(num/2)+1): index += 1 divide(candies, num, temp, sum, total, index) sum -= select[len(select) - 1] select.remove(select[len(select)-1])]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub搭建个人博客]]></title>
    <url>%2F2018%2F06%2F09%2FHexo%2BGitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Hexo+GitHub搭建个人博客目录： 前言： ​ 准备工作 搭建github博客 ​ 创建仓库 ​ 绑定域名 配置SSH key ​ 测试是否成功 使用hexo写博客 ​ hexo简介 ​ 原理 注意事项 安装 初始化 ​ 修改主题 ​ 上传到github 保留CNAME、README.md等文件 常用hexo命令 ​ _config.yml 写博客 最终效果 https://dadavision.cn/ 技巧 1.前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台； 等等； 1.1. 准备工作在开始一切之前，你必须已经： 有一个github账号，没有的话去注册一个； 安装了node.js、npm，并了解相关基础知识； 安装了git for windows（或者其它git客户端） 本文所使用的环境： Windows10企业版 node.js@8.11.2 git@2.17.1 hexo@3.7.1 2.搭建github博客2.1. 创建仓库新建一个名为你的用户名.github.io的仓库，比如说，如果你的github用户名是test，那么你就新建test.github.io的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是 http://test.github.io 了，是不是很方便？ 由此可见，每一个github账户最多只能创建一个这样可以直接使用域名访问的仓库。 几个注意的地方： 注册的邮箱一定要验证，否则不会成功； 仓库名字必须是：username.github.io，其中username是你的用户名； 仓库创建成功不会立即生效，需要过一段时间，大概10-30分钟，或者更久，我的等了半个小时才生效； 创建成功后，默认会在你这个仓库里生成一些示例页面，以后你的网站所有代码都是放在这个仓库里啦。 2.2. 绑定域名当然，你不绑定域名肯定也是可以的，就用默认的 xxx.github.io 来访问，如果你想更个性一点，想拥有一个属于自己的域名，那也是OK的。 首先你要注册一个域名，域名注册以前总是推荐去godaddy，现在觉得其实国内的阿里云、腾讯云也挺不错的，价格也不贵，毕竟是大公司，放心！或者说万网，本质上都是在万网上。 绑定域名分2种情况：带www和不带www的。 域名配置最常见有2种方式，CNAME和A记录，CNAME填写域名，A记录填写IP，由于不带www方式只能采用A记录，所以必须先ping一下你的用户名.github.io的IP，然后到你的域名DNS设置页，将A记录指向你ping出来的IP，将CNAME指向你的用户名.github.io，这样可以保证无论是否添加www都可以访问，如下： 然后到你的github项目根目录新建一个名为CNAME的文件（无后缀），里面填写你的域名，加不加www看你自己喜好，因为经测试： 如果你填写的是没有www的，比如 mygit.me，那么无论是访问 http://www.mygit.me 还是 http://mygit.me ，都会自动跳转到 http://mygit.me 如果你填写的是带www的，比如 www.mygit.me ，那么无论是访问 http://www.mygit.me 还是 http://mygit.me ，都会自动跳转到 http://www.mygit.me 如果你填写的是其它子域名，比如 abc.mygit.me，那么访问 http://abc.mygit.me 没问题，但是访问 http://mygit.me ，不会自动跳转到 http://abc.mygit.me 另外说一句，在你绑定了新域名之后，原来的你的用户名.github.io并没有失效，而是会自动跳转到你的新域名。 3.配置SSH key为什么要配置这个呢？因为你提交代码肯定要拥有你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。 1$ cd ~/. ssh #检查本机已存在的ssh密钥 如果提示：No such file or directory 说明你是第一次使用git。 1ssh-keygen -t rsa -C &quot;邮件地址&quot; 然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到.ssh\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key： 将刚复制的内容粘贴到key那里，title随便填，保存。 3.1. 测试是否成功1$ ssh -T git@github.com # 注意邮箱地址不用改 如果提示Are you sure you want to continue connecting (yes/no)?，输入yes，然后会看到： Hi liuxianan! You’ve successfully authenticated, but GitHub does not provide shell access. 看到这个信息说明SSH已配置成功！ 此时你还需要配置： 12$ git config --global user.name &quot;dadavision&quot;// 你的github用户名，非昵称$ git config --global user.email &quot;dadavision@qq.com&quot;// 填写你的github注册邮箱 具体这个配置是干嘛的我没仔细深究。 4.使用hexo写博客4.1. hexo简介Hexo是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。 官网： http://hexo.iogithub: https://github.com/hexojs/hexo 4.2. 原理由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 4.3. 注意事项安装之前先来说几个注意事项： 很多命令既可以用Windows的cmd来完成，也可以使用git bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行； hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导； hexo有2种_config.yml文件，一个是根目录下的全局的_config.yml，一个是各个theme下的； 4.4. 安装1$ npm install -g hexo 4.5. 初始化在电脑的某个地方新建一个名为hexo的文件夹（名字可以随便取），比如我的是F:\Workspaces\hexo，由于这个文件夹将来就作为你存放代码的地方，所以最好不要随便放。 12$ cd /f/GitBlog/hexo/$ hexo init hexo会自动下载一些文件到这个目录，包括node_modules，目录结构如下图： 这里有个’更新博客’，这里涉及一个技巧，后面讲。 12$ hexo g # 生成$ hexo s # 启动服务 执行以上命令之后，hexo就会在public文件夹生成相关html文件，这些文件将来都是要提交到github去的： hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容，很多人会碰到浏览器一直在转圈但是就是加载不出来的问题，一般情况下是因为端口占用的缘故，因为4000这个端口太常见了，可以用hexo clean。 第一次初始化的时候hexo已经帮我们写了一篇名为 Hello World 的文章，默认的主题比较丑。 4.6. 修改主题既然默认主题很丑，那我们别的不做，首先来替换一个好看点的主题。这是 官方主题。 个人比较喜欢的2个主题：hexo-theme-jekyll 和 hexo-theme-yilia。 首先下载这个主题： 12$ cd /f/Workspaces/hexo/$ git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 下载后的主题都在这里： 另外一种更方便的方式是： 我用的next主题。 直接从官网github上，clone down下来，解压后直接放进文件夹更快。 修改_config.yml中的theme: landscape改为theme: next，然后重新执行hexo g来重新生成。 如果出现一些莫名其妙的问题，可以先执行hexo clean来清理一下public的内容，然后再来重新生成和发布。 4.7. 上传之前在上传代码到github之前，一定要记得先把你以前所有代码下载下来（虽然github有版本管理，但备份一下总是好的），因为从hexo提交代码时会把你以前的所有代码都删掉。 4.8. 上传到github如果你一切都配置好了，发布上传很容易，一句hexo d就搞定，当然关键还是你要把所有东西配置好。 首先，ssh key肯定要配置好。 其次，配置_config.yml中有关deploy的部分： 正确写法： 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:DaDaVision/DaDaVision.github.io.git branch: master 错误写法： 1234deploy: type: github repository: https://github.com:DaDaVision/DaDaVision.github.io.git branch: master 后面一种写法是hexo2.x的写法，现在已经不行了，无论是哪种写法，此时直接执行hexo d的话一般会报如下错误： 1Deployer not found: github 或者 Deployer not found: git 原因是还需要安装一个插件： 1npm install hexo-deployer-git --save 其它命令不确定，部署这个命令一定要用git bash，否则会提示Permission denied (publickey). 打开你的git bash，输入hexo d就会将本次有改动的代码全部提交，没有改动的不会. 4.9. 保留CNAME、README.md等文件提交之后网页上一看，发现以前其它代码都没了，此时不要慌，一些非md文件可以把他们放到source文件夹下，这里的所有文件都会原样复制（除了md文件）到public目录的： 由于hexo默认会把所有md文件都转换成html，包括README.md，所有需要每次生成之后、上传之前，手动将README.md复制到public目录，并删除README.html。 这里需要注意： 在添加绑定域名后需要注意以下问题（未绑定可以不用） 需要将github上CNAME里面内容在文件工程中GitBlog\source\CNAME 这里注意新建的CNAME不要有后缀名。 4.10. 常用hexo命令常见命令 1234567hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 缩写： 1234hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy 组合命令： 12hexo s -g #生成并本地预览hexo d -g #生成并上传 4.11. _config.yml这里面都是一些全局配置，每个参数的意思都比较简单明了，所以就不作详细介绍了。 需要特别注意的地方是，冒号后面必须有一个空格，否则可能会出问题。 4.12. 写博客定位到我们的hexo根目录，执行命令： 1hexo new &apos;哈喽&apos; hexo会帮我们在_posts下生成相关md文件，我们只需要打开这个文件就可以开始写博客了，默认生成如下内容： 当然你也可以直接自己新建md文件，用这个命令的好处是帮我们自动生成了时间。 一般完整格式如下： 123456789---title: postName #文章页面上的显示名称，一般是中文date: 2018-7-02 15:30:16 #文章生成时间，一般不改，当然也可以任意修改categories: 默认分类 #分类tags: [tag1,tag2,tag3] #文章标签，可空，多标签请用格式，注意:后面有个空格description: 附加一段文章摘要，字数最好在140字以内，会出现在meta的description里面---以下是正文 那么hexo new page &#39;postName&#39;命令和hexo new &#39;postName&#39;有什么区别呢？ 1hexo new page &quot;my-second-blog&quot; 生成如下： 最终部署时生成：GitBlog\public\my-second-blog\index.html，但是它不会作为文章出现在博文目录。 4.12.1. 写博客工具那么用什么工具写博客呢？ 推荐用Typora和Hbuilder X。 4.12.2. 如何让博文列表不显示全部内容默认情况下，生成的博文目录会显示全部的文章内容，如何设置文章摘要的长度呢？ 答案是在合适的位置加上&lt;!--more--&gt;即可。 5.最终效果可以访问我的git博客来查看效果：https://dadavision.cn 6.技巧其中生成，上传github有很多命令，以下方法可以减少繁琐，就是创建.bat批处理文件： 123f:cd GitBloghexo clean &amp; hexo g &amp;&amp; gulp &amp; hexo d &amp; hexo s 还可以设置全局快捷键：]]></content>
      <categories>
        <category>个人博客</category>
      </categories>
      <tags>
        <tag>个人博客</tag>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>next</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回溯法思想]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[子集树与排列树当所给问题是从n个元素的集合S中找出满足某种性质的子集时，解空间为子集树。例如：0-1背包问题 (选或不选问题)当所给问题是从n个元素的集合S中找出满足某种性质的排列时，解空间为排列树。例如：旅行售货员问题（选择顺序问题） 算法结构 深度优先搜索与广度优先搜索算法有何区别深度优先搜索法不全部保留结点，扩展完的结点从数据存储结构栈中弹出删去，在栈中存储的结点数就是解空间树的深度，因此它占用空间较少。所以，当搜索树的结点较多，用其它方法易产生内存溢出时，深度优先搜索不失为一种有效的求解方法。广度优先搜索算法，一般需存储产生所有结点，占用的存储空间要比深度优先搜索大得多，因此，程序设计中，必须考虑溢出和节省内存空间的问题。但广度优先搜索法一般无回溯操作（即入栈和出栈的操作），所以运行速度比深度优先搜索要快些。 回溯与分支限界区别回溯法以深度优先的方式搜索解空间树T，而分支限界法则以广度优先或以最小耗费优先的方式搜索解空间树T。它们在问题的解空间树T上搜索的方法不同，适合解决的问题也就不同。一般情况下，回溯法的求解目标是找出T中满足约束条件的所有解的方案，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。相对而言，分支限界算法的解空间比回溯法大得多，因此当内存容量有限时，回溯法成功的可能性更大。 最优化处理问题在处理最优问题时，采用穷举法、回溯法或分支限界法都可以通过利用当前最优解和上界函数加速。仅就对限界剪支的效率而言，优先队列的分支限界法显然要更充分一些。在穷举法中通过上界函数与当前情况下函数值的比较可以直接略过不合要求的情况而省去了更进一步的枚举和判断；回溯法则因为层次的划分，可以在上界函数值小于当前最优解时，剪去以该结点为根的子树，也就是节省了搜索范围；分支限界法在这方面除了可以做到回溯法能做到的之外，同时若采用优先队列的分支限界法，用上界函数作为活结点的优先级，一旦有叶结点成为当前扩展结点，就意味着该叶结点所对应的解即为最优解，可以立即终止其余的过程。在前面的例题中曾说明，优先队列的分支限界法更象是有选择、有目的地进行搜索，时间效率、空间效率都是比较高的。 算法总结一个问题是该用递推、贪心、搜索还是动态规划，完全是由这个问题本身阶段间状态的转移方式决定的！每个阶段只有一个状态-&gt;递推；每个阶段的最优状态都是由上一个阶段的最优状态得到的-&gt;贪心；每个阶段的最优状态是由之前所有阶段的状态的组合得到的-&gt;搜索；每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到而不管之前这个状态是如何得到的-&gt;动态规划。 动态规划1.求最优解问题2.整体问题的最优解依赖于各个子问题的最优解3.把大问题分解成小问题，小问题之间还有相互重叠的更小的子问题4.从上往下分析，从下往上求解，避免重复求解小问题]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
