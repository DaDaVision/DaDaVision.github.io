<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[唐老师研究生毕业演讲]]></title>
    <url>%2F2018%2F06%2F20%2F%E5%94%90%E8%80%81%E5%B8%88%E7%A0%94%E7%A9%B6%E7%94%9F%E6%AF%95%E4%B8%9A%E6%BC%94%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[尊敬的何院士、各位嘉宾、各位老师、同学们： 大家下午好！ 我是计算机科学与软件工程学院2018届硕士毕业生唐益军。此刻我站在这里，除了感到兴奋和荣幸之外，其实更多的是不解。学院的张老师让我来做毕业生代表致辞，我其实一直以为只有优秀毕业生才可以来。我是说，即使真的是只有优秀毕业生才可以来，我也会不择手段地想尽各种办法上来说几句话。可是，母校的不拘一格和张老师的英明决定让我的那些“不择手段”竟然到现在都没有用武之地，我想这可能是我在毕业之际留下的唯一遗憾。 Anyway, 今天我被委以“重任”站在了这里，我希望能和大家分享一下这几年从母校、学院以及学校的各位老师那里感受到的几点体会，希望能和大家共勉。并且我一厢情愿地以为这也是你们的体会，即使不是，希望大家今天在现场配合下我，假装这些是。好，那我们开始。 1. 在校期间学习“学习” 确切地说是，在校期间一定要好好学习学习，即使这样做的结果是注定要单身整个大学和研究生生涯也没有关系。学习的过程就是将我们从一个傻傻分不清楚的“学生”变成一个高富帅或者是白富美型的技能满点的社会精英。从这个结果看过去，具体地说，从我目前的身体状态看过去，我的学习过程基本就是失败的。对了，插一句，我要重新定义下“学习”的概念：“不要停止投资自己的脑子和身子”。 我知道，很多同学和我一样都曾不断疑问：我学了这么多年的文化知识，可是去校门外一看几乎和社会上的企业机构所需要的技能是脱节的，我该咋办？拿我们软件专业来说，市面上普通需要的是做后台开发，前端设计，安卓IOS开发、大数据开发，甚至是机器学习人工智能，乃至最近比较火的区块链技术。可是，学校从来不教我们这些东西，至少是从来没有直接教给我们如何用这些技术开发项目。 但事实上，学校的各位老师用心良苦，很多人都没有看出来。今天就让我来揭开这个惊天大秘密吧。注意听，学校教给我们的一直都是“如何学习”的能力。我刚刚说“在校期间学习学习”。第一个“学习”是动词，第二个“学习”是名词，意思就是：我们要学习“如何学习”这件事儿。 走上你未来的工作岗位后，你可能会发现：光会后台开发（我是说软件专业的同学）不够用了，公司可能有个临时项目需要你会Python，可是老师没教过，不过好在我们有“如何学习”的能力，所以做到知识的快速迁移与学习一定，注意，是一定，一定没有问题。这一点，我无比自信，甚至是坚信，相信你们也一样。 我小时候看过一部电影叫《神鞭》。故事情节不重要了，只记得影片结尾有一句台词：“鞭没了，但神还在。”这和我们的学校教育是不是很像，最后忘记了所有具体的知识技能，但是我们的“学习能力”留下来了。我想这才是未来我们在社会上一直能够无往不利的最强能力，至少也应该是之一吧。 2. 在用中学和学中用 软件专业当然也包括其他专业，很多老师也就是我们的研究生导师给我们提供了很多研究课题项目。对于软件专业的同学来说，要求动手能力很强，也就是大家常说的代码能力要很强。 我记得刚开始的时候，很多同学，至少包括我，我们都觉得自己的能力不够来做项目，于是就想着再去找一些书系统看看前面的内容，来达到现在“具备做这个课题项目的能力”，也就是我们常说的“我想先学会了再去做”。这样的后果就是，你永远“学不够”、“学不会”。 后来我们的师兄师姐告诉我们过来人的经验，说这种蠢事谁年轻的时候没干过呢，其实你们应该直接做课题做项目，在做的过程中遇到不会的去查，然后回来继续做，反复折腾，反复验证，项目也就被推进着做完了。简单地说这个过程就是“在用中学和学中用”。 可是我觉得师兄师姐没有说清楚这件事儿，尽管从结果上看让人屡试不爽。但是，今天既然在这里演讲，我觉得非常有必要稍显专业地把这件事儿再稍微说明白一点。 所以后来我总结道（其实也不是我总结的，你们就姑且当是我总结的吧）：当你需要获取某项技能或者是搞定一件事儿的时候，一定要想办法在最短时间内弄清楚这个技能或搞定这件事儿需要哪些“最少必要知识”，然后迅速掌握它。请大家记住了，在你掌握了这些“最少必要知识”的瞬间，其实你就已经入门了，你就走上了学习这项技能的快速路径或具备了搞定这件事儿的起码素质。剩下的部分，只需要你在“用中学和学中用”。 而在研究生期间我们需要攻克的那些课题项目，其实大部分所需要的“最少必要知识”我们已经具备了，除非你在本科期间光顾着谈恋爱了，虽然我觉得从某种意义上来讲，没有什么比谈恋爱更重要的事儿了，如果真的有，你就再谈一个嘛。 从这个角度看过去，从掌握“最少必要知识”再到“用中学和学中用”，其实这就是我刚刚说的第一点中的“如何学习的能力”。我不得不反复强调这件事儿，因为对学生来说，即使我们今天毕业了，学习还是一件非常重要的事儿，即使和谈恋爱相比，它不能成为最重要了，我想至少也应该是之一吧。 3. 有死磕到底的精神 刚刚说到很多研究课题项目，所以研究生期间晚上12点才准备回宿舍，其实一点都不值得炫耀，尽管我和很多同学一样，都喜欢在12点或者1点准备走的时候拍一张照片发到朋友圈。 我印象比较深刻的是，大概是研一下学期有天晚上大概11点多了，实验室只剩下我和一位年轻的王老师。我已经准备收拾一下就撤了。 当时我们那个王老师一看我要走，一把拿住我：“你干嘛去？”那个眼神就像在说，今晚程序调不出来不许走！我当时楞了一小会，你们要知道，对于一个资深的胖子来说，正常情况下，我什么都愿意吃，就是不愿意吃苦，尤其是这么大半夜的。可是，我当时看到王老师含情脉脉地盯着我，我们四目相对，那时那刻，我们都知道彼此心里在想什么：他知道我想走，我知道他不想让我走。最终我还是从了他，只好说：“我先去上个洗手间”。 但是，回头想想我十分感谢王老师。从我踏入那个实验室开始，我几乎每天都能看到他至少工作到晚上11点以后，从未间断过。早上也来得很早，我为此还非常“不合常理”地起了好几个大早。可是不管我有多早，他都已经在了。不过，我也一直都没有好意思问他为什么这么拼命？就怕王老师告诉我：知道你为什么这么胖了吧？ 所以如果说，我身上有那么一点点死磕到底，有那么一点点从来都不放过自己的精神，我想都是王老师日日夜夜对我以身作则的影响，尽管我从来没有向他表白过，但今天我想借这个机会说一句：王老师，我爱你。没想到，我人生中第一次公开场合的表白献给了一位老男人。作为毕业生代表，刚刚那句“我爱你”，我其实也是替大家说给那些我们研究生生涯中的老师们听的。实在不好意思，没有经过大家同意，就这么把很多人的“第一次”送出去了。 4. 做一名终身学习者 毕业找工作的时候，我本来的打算是去那个浦发银行。因为大家都说来钱最快的是金融投资行业，所以我也十分天真，为了钱什么都干得出来。可是，有一天我的研究生导师王江涛老师找到我，十分中肯地说：“小唐啊，你看研究生这几年，你除了体重，其他的学习能力和工作能力，其实都没有得到多大的提升。我的意思是，你要不要毕业后在我这里再学习几年？反正你还年轻，而且你估计也找不到什么好工作的吧。” 我当时一听，那个心里真不是滋味，心想：王老师的为人怎么这么好？为我考虑得如此周到。尤其是，他也觉得我还需要学习，并且即使我毕业了也愿意继续教我培养我。我当时感动得一塌糊涂，鼻涕泪水全下来了，头一扬，眼含泪水对着王老师说：那太好了。我愿意啊。 我是想说，我们在场的每一个人都要做一名终身学习者，即使我们今天从母校毕业了，即使我们暂时脱离了理论意义上的“学生”身份。尤其是当你觉得自己力有不逮，心有不余的时候，通过学习新的事物，不一定是我们的专业知识，总是会有一种让我们脚能够踩在地上的厚实感，对了，这个词叫“踏实”也叫“安全感”。 所以，我一直愿意承认和告诉别人，我过去、现在和未来都是一个学生，即使费尽千辛万苦地勉强拿到了学位，勉强从母校毕业了。但正如乔布斯曾经说过的那样：stay foolish, stay hungry. 永远保持谦卑的心态，永远要如饥似渴地对待那些新的事物。正是这样的一个空杯心态才是我们可以不断进步的基本条件，也是我们作为一个终身学习者的标配。 5. 身体是革命的本钱 前不久我们团队去浙江嘉兴南湖搞团建，我们当时要做一个叫“定向越野”的项目，总共有11个定点。但是游戏规则只需要走4个点，可是带我们小队的领队是一位跑过马拉松的老师。他决定带我们跑完11个定点，我们从8点半左右开始出发，一直到中午12点半才完成任务。本来走走停停中间歇歇，即使对我这样一个胖子，跑完全程也显得相对十分轻松。 可是我们这位老师毕竟是跑过马松的，全程都是在小跑。刚开始我还受得了，时间长了，我的生理防线没有丝毫犹豫直接崩溃，整个人都在打摆。一向自诩心理强大的我，在那一刻我的心理防线也几乎临近崩溃边缘。所以后半场我几乎在众人拖着的情况下走完了。 好，接下来是本次演讲中最励志的部分了。大家请听好了，非常励志的部分来了。 团建回来以后，我做出了一个非常慎重的决定：我要减肥！注意，我要解释一下，在胖子界有头有脸且有过多年从业经验的我来说，我从来没想过要跨界转行。 但是一想到这次团建我的糟糕表现，以及我觉得我未来是要干大事儿。你们知道，干大事一定要身体特别好，因为大事干成的时间周期一般有点长，工作强度也特别特别大。所以如果身体不好，基本我要干的“大事儿”也就黄了。 因此，基于这样的考虑，我觉得我要减肥了。于是，我去报了一个健身房，还请了两个私教。基本上把我身上的那点积蓄和信用卡都刷爆了。当然，这不重要。 我想说的是，身体真的是革命的本钱。这事儿实在太重要了，即使和谈恋爱相比也很重要，如果你能懂我的意思的话。 好，现在请允许我代表2018届计算机科学与软件工程学院所有硕士毕业生，向我们的母校华东师范大学、向计算机科学与软件工程学院的何院士以及其他全体老师表示衷心的感谢。 没有你们的教育与培养，光靠我们自己的努力，我们根本无法茁壮成长到现在，而且长势非常喜人，甚至还有点喜人过了头，比如说我。请大家和我一起把掌声送给所有所有的老师，感谢你们这些年不辞辛劳的付出，也祝愿何院士和全体老师在以后的日子里工作顺利！身体健康！ 同时，借此机会我想代表学院所有硕士毕业生同学向我们的父母表示感谢，感谢他们多年来的养育之恩。尤其是在我们20多岁血气方刚、风华正茂的年龄，本该承担起一部分家庭责任的时候，居然还一直支持我们求学读书，不管是精神上还是经济上。当然，最重要的还是经济上，因为我想对你们说，你们的儿子女儿已经在精神上坚强得让你们难以想象了。并且从今天开始，我们也有经济实力，可能只有软件专业的学生才敢说这种话，我们也有经济实力给予你们更多的报答。后天就是父亲节，大家可以想想能干点什么了。 最后，也是最重要的部分，Class of 2018, Congradulations. It is your big day, so stop thinking about anything today but enjoy it ! And rock to death. 好，谢谢大家。我也最后代表一次大家，请允许我代表全体2018届研究生感谢母校，感谢计算机科学与软件工程学院，感谢所有老师的教导，感谢今天到场的咱爸咱妈们，并且祝福你们！谢谢！谢谢！]]></content>
      <categories>
        <category>毕业演讲</category>
      </categories>
      <tags>
        <tag>毕业演讲</tag>
        <tag>唐老师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10+Anaconda+tensorflow1.8+pytorch0.4.0注意事项]]></title>
    <url>%2F2018%2F06%2F12%2Fwin10-Anaconda-tensorflow1-8-pytorch0-4-0%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[注意事项 Anaconda会自带python3.6，所以为了避免环境配置乱套，删除本机独立安装的python。 不要按网上配置虚拟环境，比如TensorFlow配置一个虚拟环境，pytorch配置一个虚拟环境……keras……cafee……只要一个环境即可。 win10+Anaconda3-5.2.0+tensorflow1.8+pytorch0.4.0 在windows PowerShell中确定python环境在Anaconda中 Anaconda注意：自动添加到环境变量其它事项一路安装，普通套路。 TensorFlow1、TensorFlow或者pytorch，无论什么框架，越新越好，新的功能多bug少，这里用的TensorFlow1.8。 2、TensorFlow和pytorch最好都用conda装，但是这里conda install tensorflow只会给你比较低的版本，所以TensorFlow安装在这里使用了：1conda install --channel https://conda.anaconda.org/conda-forge tensorflow 可能会下载很慢，如果有部分下载未成功，重新执行命令即可，之前下载的会保留。 Pytorch同样遵循使用最新框架原则。我自己电脑用的pytorch-CPU版本，所以执行：12conda install conda install pytorch-cpu -c pytorchpip install torchvision 在实验室服务器用的是cuda9.0版本12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 安装成功之后效果： PyCharm注意事项在不用虚拟幻境情况下，好处就是不用切换环境配置了，直接一个 Anaconda\python.exe全部搞定，如图： 附录鄙视链：caffe-pytorch-tensorflow-caffe2- caffetorch-slim-tensorflow-keras]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Anaconda</tag>
        <tag>tensorflow</tag>
        <tag>pytorch</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch0.4.0重大更新]]></title>
    <url>%2F2018%2F06%2F11%2FPyTorch0.4.0%E9%87%8D%E5%A4%A7%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[一、重大核心变化包括 Tensor/Variable 合并 零维张量 数据类型 迁移指南 二、现添加的新特征包括 Tensors： 全面支持高级索引 快速傅立叶变换 神经网络： 计算时的存储权衡 bottleneck-识别代码中热点（hotspots）的工具 torch.distributions 24 个基础的概率分布 增加cdf、方差、信息熵、困惑度等 分布式训练 易于使用的 Launcher utility NCCL2 后端 C++拓展 Windows 支持 ONNX 改进 RNN 支持 三、性能改进 四、Bug 修复 五、torchvision的一些变化1.torchvision.transform中函数torchvision.transforms.`Scale(*args, **kwargs)即将被函数torchvision.transforms.Resize`(size, interpolation=2)代替。（参考官方文档：点击打开链接） 2.torchvision.transform中函数torchvision.transforms.`RandomSizedCrop(*args, **kwargs)即将被函数torchvision.transforms.RandomResizedCrop`(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)代替。（参考官方文档：点击打开链接） 六、安装方式——Windows安装【方法一】pip直接安装。官网（点击打开链接）给出的安装步骤如下图所示（根据CUDA版本以及Python版本选择）。 Run this command: 12pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-win_amd64.whl pip3 install torchvision 这里我是下载好torch-0.4.0-cp36-cp36m-win_amd64.whl文件到本地之后才安装的。进入Anaconda Prompt，然后进入文件所在目录： 打开cmd命令提示符，先利用anaconda创建一个虚拟环境，命名为pytorch4 1conda create -n pytorch4 python=3.6 激活刚才创建好的虚拟环境 1activate pytorch4 安装pytorch0.4.0 1pip install torch-0.4.0-cp35-cp35m-win_amd64.whl 注：根据自己的配置选择whl下载来链接 安装torchvision1pip install torchvision 简单测试安装是否成功123pythonimport torchprint(torch.__version__) 如果输出0.4.0，那么恭喜Windows下的PyTorch0.4.0安装成功！]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>PyTorch</tag>
        <tag>机器学习</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mnist手写数字识别]]></title>
    <url>%2F2018%2F06%2F09%2FMnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[minist_mpl.py1234567891011121314151617181920212223242526272829303132333435 #encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense,Activation,Dropoutfrom keras.optimizers import SGD,Adadeltafrom keras.models import save_modelimport matplotlib.pyplot as plt(x_train,y_train),(x_test,y_test) = mnist.load_data()x_train = x_train.reshape(60000,28*28).astype('float32') #转换数据格式x_test = x_test.reshape(10000,28*28).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train,10) #one-hot编码y_test = keras.utils.to_categorical(y_test,10)print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)modle = Sequential()#第一层隐层，64个神经元modle.add(Dense(256,activation='relu',input_dim=28*28))#第二层隐层，64个神经元modle.add(Dense(256,activation='relu'))modle.add(Dropout(0.5))#输出层，10个神经元modle.add(Dense(10,activation='softmax'))sgd = SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True)modle.compile(loss='categorical_crossentropy',optimizer='adagrad',metrics=['accuracy'])modle.fit(x_train,y_train,epochs=10,batch_size=128)score = modle.evaluate(x_test,y_test,batch_size=128)print(score)modle.save('MLP_minist.h5') 123456789101112131415161718192021222324252627282930313233343536373839#encoding:utf-8import kerasfrom keras.datasets import mnistfrom keras.models import Sequential,save_modelfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2Dfrom keras.optimizers import SGD,Adadelta(x_train,y_train),(x_test,y_test) = mnist.load_data() #加载数据print(x_train.shape,x_test.shape)x_train = x_train.reshape(60000,28,28,1).astype('float32') #二维数据x_test = x_test.reshape(10000,28,28,1).astype('float32')x_train /= 255 #训练数据归一化x_test /= 255y_train = keras.utils.to_categorical(y_train) #one-hot编码y_test = keras.utils.to_categorical(y_test)num_classes = y_test.shape[1]model = Sequential() #创建序列模型model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1))) #第一层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Conv2D(64,(3,3),activation='relu')) #第二层卷积层model.add(MaxPooling2D(pool_size=(2,2))) #池化层model.add(Flatten()) #铺平当前节点model.add(Dense(128,activation='relu')) #全连接层model.add(Dropout(0.5)) #随机失活model.add(Dense(num_classes,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) #编译模型model.fit(x_train,y_train,batch_size=128,epochs=10) #训练模型score = model.evaluate(x_test,y_test,batch_size=128) #评价模型print(score) #打印分类准确率model.save('CNN_minist.h5')]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Keras</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交叉验证集、测试集]]></title>
    <url>%2F2018%2F06%2F09%2F%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%2F</url>
    <content type="text"><![CDATA[什么有交叉验证：主要是因为训练集较小。无法直接像前面那样只分出训练集，验证集，测试就可以了（简单交叉验证）。 最佳的数据分类情况是把数据集分为三部分，分别为：训练集(train set)，验证集(validation set)和测试集(test set) 验证集和测试集两者的主要区别是：验证集用于进一步确定模型中的超参数(例如正则项系数、ANN{Artificial Neural Network}中隐含层的节点个数、网络层数、迭代次数、学习率)而测试集只是用于评估模型的精确度(即泛化能力)！ 举个例子：假设建立一个BP神经网络，对于隐含层的节点数目，我们并没有很好的方法去确定。一般将节点数设定为某一具体的值，通过训练集训练出相应的参数后，再由交叉验证集去检测该模型的误差；然后再改变节点数，重复上述过程，直到交叉验证误差最小。此时的节点数可以认为是最优节点数，即该节点数(这个参数)是通过交叉验证集得到的。 而测试集是在确定了所有参数之后，根据测试误差来评判这个学习模型的；也可以说是用来评估模型的泛化能力。所以，验证集主要主要是用于模型的调参。 “交叉验证法“ (cross validation)先将数据集D 划分为k 个大小相似的互斥子集， 即D = D**1 ∪ D**2 ∪**… ∪ Dk, Di ∩ Dj = ø (i≠j) . 每个子集Di 都尽可能保持数据分布的一致性，即从D 中通过分层采样得到. 然后，每次用k-1 个子集的并集作为训练集，余下的那个子集作为测试集;这样就可获得k组训练/测试集，从而可进行k 次训练和测试，最终返回的是这k 个测试结果的均值**。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，为强调这一点，通常把交叉验证法称为” k 折**/倍**交叉验证“ (k-fold cross validation). k 最常用的取值是10 ，此时称为10折交叉验证; 其他常用的k 值有5、20 等. 为了避免其他属性携带的信息被训练集中未出现的属性值”抹去”，在估计概率值时通常要进行”平滑” (smoothing) ，常用”拉普拉斯修正“，所以，P(c)和P(xi|c)修正为： 其中N 表示训练集D 中可能的类别数，Ni表示第 与留出法相似，将数据集D 划分为k 个子集同样存在多种划分方式.为减小因样本划分不同而引入的差别，k折交叉验证通常要随机使用不同的划分重复p 次。最终的评估结果是这p 次k 折交叉验证结果的均值，例如常见的有”10 次10 折交叉验证。 假定数据集D中包含m个样本，若令k=m ，则得到了交叉验证法的一个特例:留一法(Leave- One-Out比，简称LOO) . 显然，留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集**—**每个子集包含一个样本;留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D 训练出的模型很相似.因此，留一法的评估结果往往被认为比较准确.然而，留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1 百万个样本，则需训练1 百万个模型)，而这还是在未考虑算法调参的情况下.另外，留一法的估计结果也未必永远比其他评估方法准确;”没有免费的午餐”定理对实验评估方法同样适用.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>模型评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查准率precision和查全率recall、F1]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9F%A5%E5%87%86%E7%8E%87precision%E5%92%8C%E6%9F%A5%E5%85%A8%E7%8E%87recall%E3%80%81F1%2F</url>
    <content type="text"><![CDATA[真正例（true positive）、假正例（false positive）、真反例（true negative）、假反例（false negative），分别用TP、FP、TN、FN表示相应样例数，样例总数=TP+FP+TN+FN；分类结果混淆矩阵： 真实情况 预测结果 正例 反例 正例 TP（真正例） FN（假反例） 反例 FP（假正例） TN（真反例） 查准率（precision）：被认定为正例的里面，判断正确的比例。 查全率（recall）：真实正例里，被判断出为正例的比例。 查准率和查全率是一对矛盾的度量.一般来说，查准率高时，查全率往往偏低;而查全率高时，查准率往往偏低,可以通过P-R曲线来取两者的平衡值 若一个学习器P-R曲线被另一个学习器的曲线完全”包住“,则可断言后者的性能优于前者， 例如图中学习器A 的性能优于学习器C; 如果两个学习器的P-R 曲线发生了交叉7,例如图中的A 与B ，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较然而，在很多情形下，人们往往仍希望把学习器A 与B 比出个高低. 这时一个比较合理的判据是比较P-R 曲线节面积的大小。 “平衡点“是”查准率=查全率“时的取值。 但更常用的使用F1来衡量查准率与查全率； F1基于查准率与查全率的调和平均： ，sum为样例总数， 具体应用中可能对P和R有不同的倚重。比如商品推荐中，为了尽可能少打扰用户，更希望推荐内容确是用户感兴趣的，这时候查准率更重要。而在逃犯检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。 F1度量的一般形式F**β（加权调和平均**）就可以表达这种偏好。 即 ß = 1时退化为标准的F1,当β&gt;1意味着P占比重更大，反之则是R。 ROC、AUCROC:全称“受试者工作特征”，表达了模型的泛化能力。其纵坐标为“TPR真正例率”；横坐标为“FPR假正例率”。 ROC曲线根据模型的排序结果，一个个划分正负，每次得出两个值TPR,FPR。很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。为啥在ROC曲线画一个y=x呢，那表示随机猜测的ROC。 AUC:ROC曲线下的面积.AUC=1,是完美分类器(并不存在)；0.5。AUC 越大，意味着辨别能力越强。 与P-R图相似，如果一条ROC曲线包含另一条ROC曲线，则前者的学习器性能更优越。如果曲线有交叉，则可以通过计算AUC大小得到。 代价敏感错误率、代价曲线代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫]]></title>
    <url>%2F2018%2F06%2F09%2FPython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[文本爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#encoding:utf-8import requests,urllib3.request,time,osimport random,csv,socket,http.clientfrom bs4 import BeautifulSoupdef get_contend(url, data = None): #获取网页中html代码 header=&#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'zh-CN,zh;q=0.9', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; timeout = random.choice(range(80,180)) while True: try: rep = requests.get(url,headers = header,timeout=timeout) rep.encoding = 'utf-8' break except socket.timeout as e: print ('3',e) time.sleep(random.choice.range(8,15)) except socket.error as e: print ('4',e) time.sleep(random.choice.range(20,60)) except http.client.BadStatusLine as e: print ('5',e) time.sleep(random.choice.range(30,80)) except http.client.IncompleteRead as e: print ('6',e) time.sleep(random.choice.range(5,15)) return rep.textdef get_data(html_text): final = [] bs = BeautifulSoup(html_text,'html.parser') #创建BeautifulSoup对象 body = bs.body #获取body部分 data = body.find('div',&#123;'id':'7d'&#125;) #找到需要爬取部分的div ul = data.find('ul') #获取ul部分 li = ul.find_all('li') #获取所有的li for day in li: #对li标签中内容进行遍历 temp = [] date =day.find('h1').string #找到日期 temp.append(date) #将日期添加到temp中 p = day.find_all('p') #找到每个li中的所有p标签 temp.append(p[0].string,) #第一个p标签中的天气状况添加到temp if p[1].find('span') == None: t_highest = None else: t_highest = p[1].find('span').string #找到最高温 t_highest = t_highest.replace('C','') t_lowest = p[1].find('i').string # 找到最低温 t_lowest = t_lowest.replace('C','') temp.append(t_highest) temp.append(t_lowest) final.append(temp) return finaldef write_data(data,name): #将数据写入文件 file_name = name with open(file_name, 'a', errors='ignore', newline='') as f: f_csv = csv.writer(f) f_csv.writerows(data)if __name__ == '__main__': url = 'http://www.weather.com.cn/weather/101190401.shtml' html = get_contend(url) result = get_data(html) print(result) write_data(result,'weather.csv') 图虫图片爬取按标签爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273__author__ = 'Result_Lv'#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_album_name(json_dict): album_name = [] postlist = json_dict['postList'] for i in range(len(postlist)): if postlist[i]['title'] == '': #图集标题为空时，命名为默认 album_name.append('Default' + str(i)) else: album_name.append(postlist[i]['title']) return album_namedef get_photo_id(json_dict): #获得所有照片的ID author_id = [] album_id = [] post_list = json_dict['postList'] for i in range(len(post_list)): #获取每个图集的照片ID photo_id = [] author_id.append(post_list[i]['author_id']) # 获取每个图集作者ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list album_id.append(photo_id) return author_id,album_iddef download_album(path,album_name,author_id,album_id): #下载图集 for i in range(len(album_id)): if not os.path.exists(path + album_name[i]): #若不存在对应图集的文件夹 try: os.makedirs(path + album_name[i]) #以图集名创建文件夹 except OSError as e: print(e) continue print('正在下载第' + str(i + 1) + '个图册:' + album_name[i]) for j in range(len(album_id[i])): fileurl = 'https://photo.tuchong.com/' + str(author_id[i]) +'/f/' + str(album_id[i][j]) + '.jpg' #生成每张照片Url filename = path + album_name[i] + '/' + str(j+1) + '.jpg' #命名照片 print(' 正在下载第' + str(j+1) + '张照片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 #爬取页数 path = 'F:/少女/' #存放路径 for i in range(page): url = 'https://tuchong.com/rest/tags/少女/posts?page=' + str(i+1) + '&amp;count=20&amp;order=weekly' #tag的json地址 json_dict = get_json(url) album_name = get_album_name(json_dict) para = get_photo_id(json_dict) author_id = para[0] album_id = para[1] download_album(path,album_name,author_id,album_id) 按作者爬取12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#encoding:utf-8import osimport jsonimport timeimport requestsimport numpy as npfrom urllib import request,errordef get_json(url): #解析json header = &#123;'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36' &#125; rep = requests.get(url, headers = header) #请求json地址 json_dict = json.loads(rep.text) #解析json return json_dict #返回json字典def get_all_photo_id(json_dict): #获得所有照片的ID #post_id = [] photo_id = [] post_list = json_dict['post_list'] author_id = post_list[0]['author_id'] #获取作者ID author_name = post_list[0]['site']['name'] #获取作者姓名 # for i in range(len(post_list)): #获取所有图集ID # post_id.append(post_list[i]['post_id']) for i in range(len(post_list)): #获取每个图集的照片ID for j in range(len(post_list[i]['images'])): photo_id.append(post_list[i]['images'][j]['img_id']) #将所有每个图集里的照片全部添加到list return author_name,author_id,photo_iddef download_photo(path,author_id,photo_id): #下载全部照片 if not os.path.exists(path): os.makedirs(path) for i in range(len(photo_id)): filename = path + '/' + str(i+1) + '.jpg' fileurl = 'https://photo.tuchong.com/' + str(author_id) + '/f/' + str(photo_id[i]) + '.jpg' print(' 第' + str(i + 1) + '张图片:' + fileurl) with open(filename,'w'): try: request.urlretrieve(fileurl,filename) #下载照片 time.sleep(np.random.rand()) #下载间隔 except error.HTTPError as e: print(e)if __name__ == '__main__': page = 3 for i in range(page): url = 'https://thomaskksj.tuchong.com/rest/2/sites/395013/posts?count=20&amp;page=' + str(i + 1) #作者主页的json地址 print('正在下载第' + str(i+1) + '页:' + url) json_dict = get_json(url) para = get_all_photo_id(json_dict) author_name = para[0] author_id = para[1] photo_id = para[2] path = 'F:/' + author_name + '/page' + str(i + 1) download_photo(path,author_id,photo_id)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP-单词拆分 I]]></title>
    <url>%2F2018%2F06%2F09%2FDP-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20I%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。说明：拆分时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入: s = “leetcode”, wordDict = [“leet”, “code”]输出: true解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 示例 2：输入: s = “applepenapple”, wordDict = [“apple”, “pen”]输出: true解释: 返回 true 因为 “applepenapple” 可以被拆分成 “apple pen apple”。注意你可以重复使用字典中的单词。 示例 3：输入: s = “catsandog”, wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出: false 算法思路动态规划的思路：将问题拆分成更小的子问题。用dp[i]表示0到i的子字符串是否可以拆分成满足条件的单词，在计算dp[i]的时候，我们已经知道dp[0],dp[1],…,dp[i-1],如果以i为结尾的j~i子串是满足条件的，并且0~j的子串也是在字典中的，那么dp[i]就是true。用公式表示就是： dp[j]&amp;&amp;s.substring[j,i+1]∈dict DP实现123456789101112131415class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123;; boolean [] dp = new boolean[s.length()+1]； dp[0] = true; for(int i = 0; i &lt; s.length(); i++)&#123; for(int j = 0; j &lt;= i; j++)&#123; if(dp[j] &amp;&amp; wordDict.contains(s.substring(j, i+1)))&#123; dp[i+1] = true; break; &#125; &#125; &#125; return dp[s.length()]; &#125;&#125; DFS解法，超时1234567891011121314151617181920212223242526class Solution &#123; boolean dfs(String s, List&lt;String&gt; wordDict, int index)&#123; // 超时 String left = s.substring(index, s.length()); if(wordDict.contains(left))&#123; return true; &#125; List&lt;Integer&gt; list = new ArrayList(); for(int i = index; i &lt; s.length(); i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; list.add(i+1); &#125; &#125; for(Integer each:list) &#123; if(dfs(s, wordDict, each))&#123; return true; &#125; &#125; return false; &#125; boolean flag = false; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; return dfs(s, wordDict, 0); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP+回溯-单词拆分 II]]></title>
    <url>%2F2018%2F06%2F09%2FDP%2B%E5%9B%9E%E6%BA%AF-%E5%8D%95%E8%AF%8D%E6%8B%86%E5%88%86%20II%2F</url>
    <content type="text"><![CDATA[题目描述给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，在字符串中增加空格来构建一个句子，使得句子中所有的单词都在词典中。返回所有这些可能的句子。说明：分隔时可以重复使用字典中的单词。你可以假设字典中没有重复的单词。示例 1：输入:s = “catsanddog”wordDict = [“cat”, “cats”, “and”, “sand”, “dog”]输出:[“cats and dog”,“cat sand dog”] 示例 2：输入:s = “pineapplepenapple”wordDict = [“apple”, “pen”, “applepen”, “pine”, “pineapple”]输出:[“pine apple pen apple”,“pineapple pen apple”,“pine applepen apple”]解释: 注意你可以重复使用字典中的单词。 示例 3：输入:s = “catsandog”wordDict = [“cats”, “dog”, “sand”, “and”, “cat”]输出:[] 算法思路这道题类似 Word Break I 判断是否能把字符串拆分为字典里的单词 @LeetCode 只不过要求计算的并不仅仅是是否能拆分，而是要求出所有的拆分方案。因此用递归。但是直接递归做会超时，原因是LeetCode里有几个很长但是无法拆分的情况，所以就先跑一遍Word Break I，先判断能否拆分，然后再进行拆分。 DP实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 class Solution &#123; boolean isBreak(String s, List&lt;String&gt; wordDict) &#123; boolean[] canBreak = new boolean[s.length()+1]; canBreak[0] = true; for(int i=1; i&lt;=s.length(); i++) &#123; boolean flag = false; for(int j=0; j&lt;i; j++) &#123; if(canBreak[j] &amp;&amp; wordDict.contains(s.substring(j,i))) &#123; flag = true; break; &#125; &#125; canBreak[i] = flag; &#125; return canBreak[s.length()]; &#125; void dfs(String s, List&lt;String&gt; wordDict, String str, int index)&#123; String result = str; //记录字符串状态 int len = s.length(); String tmp = s.substring(index, len); if(wordDict.contains(tmp))&#123; //最后一段存在于字典中，则保存结果 str += tmp; res.add(str); &#125; List&lt;Integer&gt; listIndex = new ArrayList(); List&lt;String&gt; listStr = new ArrayList(); for(int i = index; i &lt; len; i++)&#123; String temp = s.substring(index, i+1); if(wordDict.contains(temp))&#123; listIndex.add(i+1); listStr.add(temp); &#125; &#125; String temp = result; //保存递归前的字符串状态，以便回溯 for(int i = 0; i &lt; listIndex.size(); i++)&#123; result += listStr.get(i) + " "; dfs(s, wordDict, result, listIndex.get(i)); result = temp; &#125; &#125; List&lt;String&gt; res = new ArrayList(); public List&lt;String&gt; wordBreak(String s, List&lt;String&gt; wordDict) &#123; if(!isBreak(s, wordDict)) return res; String str = ""; dfs(s, wordDict, str, 0); return res; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偏差、方差、噪声]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%81%8F%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%99%AA%E5%A3%B0%2F</url>
    <content type="text"><![CDATA[代价敏感错误率：为不同错误类型赋予不同的权重。不同类型的错误所造成的后果不同.例如在医疗诊断中，错误地把患者诊断为健康人与错误地把健康人诊断为患者，看起来都是犯了”一次错误”但后者的影响是增加了进一步检查的麻烦，前者的后果却可能是丧失了拯救生命的最佳时机;再如，门禁系统错误地把可通行人员拦在门外，将使得用户体验不佳，但错误地把陌生人放进门内，则会造成严重的安全事故.为权衡不同类型错误所造成的不同损失，可为错误赋予”非均等代价” 。 在非均等代价下， ROC 曲线不能直接反映出学习器的期望总体代价，而”代价曲线“ 则可达到该目的.代价曲线图的横轴是取值为[0，1]的正例概率代价;纵轴是取值为[0，1] 的归一化代价。 偏差（Bias）和方差（Variance）偏差（Bias）：描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。 方差（Variance）：描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。 偏差：形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与指标； 方差：形容数据分散程度的，算是“无监督的”，客观的指标。 噪声：表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度. 偏差一方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的. 给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小. 一般来说偏差与方差是有冲突的，这称为偏差—方差窘境，给定学习任务，假定我们能控制学习算法的训练程度（例如决策树可控制层数，神经网络可控制训练轮数，集成学习方法可控制基学习器个数），则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以便学习器产生显著变化，此时偏差主导了泛化错误率;随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率;在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合. 为什么KNN（k最近邻k-Nearest Neighbor）算法在增大k时，偏差会变大；但RF（Random Forest随机森林）增大树的数目时偏差却保持不变；GBDT（Gradient Boosting）在增大树的数目时偏差却又能变小。 对于KNN算法，k值越大，表示模型的学习能力越弱，因为k越大，它越倾向于从“面”上考虑做出判断，而不是具体地考虑一个样本近身的情况来做出判断，所以，它的偏差会越来越大。 对于RF，我们实际上是部分实现了多次训练取均值的效果，每次训练得到的树都是一个很强的学习者，每一个的方差都比较大，但综合起来就会比较小。好比一个很强的学习者学习时，刮着西风，它会据此调整自己的瞄准方法，另一个很强的学习者学习时刮着东风，（西风、东风可以理解为不同训练集中的噪声）它也会据此调整自己的瞄准方法，在测试样本时，一个误差向西，一个误差向东，刚好起到互相抵消的作用，所以方差会比较小。但是由于每棵树的偏差都差不多，所以，我们取平均时，偏差不会怎么变化。 为什么说是部分实现了多次训练取均值的效果而不是全部呢？因为我们在训练各棵树时，是通过抽样样本集来实现多次训练的，不同的训练集中不可避免地会有重合的情况，此时，就不能认为是独立的多次训练了，各个训练得到的树之间的方差会产生一定的相关性，训练集中重合的样本越多，则两棵树之间的方差的相关性越强，就越难达成方差互相抵消的效果。 对于GBDT，N棵树之间根本就不是一种多次训练取均值的关系，而是N棵树组成了相关关联，层层递进的超级学习者，可想而知，它的方差一定是比较大的。但由于它的学习能力比较强，所以，它的偏差是很小的，而且树的棵树越多，学习能力就越强，偏差就越小。也就是说，只要学习次数够多，预测的均值会无限接近于目标。简单讲就是GBDT的N棵树实际上是一个有机关联的模型，不能认为是N个模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>笔记</tag>
        <tag>性能评估</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长公共子串-最长公共子序列]]></title>
    <url>%2F2018%2F06%2F09%2F%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[最长公共子串DP实现1234567891011121314151617181920212223242526272829public static int maxSubStr(String str1, String str2) &#123; int result = 0; int index = 0; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int [len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; dp[i][j] = dp[i-1][j-1] + 1; // 状态转移 if(dp[i][j] &gt; result) &#123; result = dp[i][j]; index = i; // 记录最大子串的最后一个下标 &#125; // result = result &gt; dp[i][j] ? result : dp[i][j]; &#125;else &#123; dp[i][j] = 1; result = result &gt; dp[i][j] ? result : dp[i][j]; &#125; &#125; &#125; &#125; System.out.println(result); for(int i = index - result + 1; i &lt;= index; i++) &#123; System.out.print(str1.charAt(i) + " "); &#125; return result;&#125; 最长公共子序列####动态规划假设Z=&lt;z1,z2,⋯,zk&gt;是X与Y的LCS， 我们观察到如果Xm=Yn，则Zk=Xm=Yn，有Zk−1是Xm−1与Yn−1的LCS；如果Xm≠Yn，则Zk是Xm与Yn−1的LCS，或者是Xm−1与Yn的LCS。因此，求解LCS的问题则变成递归求解的两个子问题。但是，上述的递归求解的办法中，重复的子问题多，效率低下。改进的办法——用空间换时间，用数组保存中间状态，方便后面的计算。这就是动态规划（DP)的核心思想了。DP求解LCS用二维数组c[i][j]记录串x1x2⋯xi与y1y2⋯yj的LCS长度，则可得到状态转移方程 DP实现1234567891011121314151617181920public static int maxSubSequence(String str1, String str2) &#123; int len1 = str1.length(); int len2 = str2.length(); int [][] dp = new int[len1][len2]; for(int i = 0; i &lt; len1; ++i) &#123; for(int j = 0; j &lt; len2; ++j) &#123; if(i &gt; 0 &amp;&amp; j &gt; 0) &#123; if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = dp[i-1][j-1] + 1; &#125;else &#123; dp[i][j] = dp[i-1][j] &gt; dp[i][j-1] ? dp[i-1][j] : dp[i][j-1]; &#125; &#125;else if(str1.charAt(i) == str2.charAt(j)) &#123; dp[i][j] = 1; &#125; &#125; &#125; System.out.println(dp[len1 - 1][len2 - 1]); return dp[len1-1][len2-1];&#125;]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分糖果]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%88%86%E7%B3%96%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[题目描述o 分糖果：科大讯飞第二道编程题 o 小明和小红是好朋友，但最近遇到一个棘手的问题，有一盒糖果要分成两份但是每颗糖果质量都不尽相同， o 但为了分配的公平每份糖的糖果数量相差不得超过1，在此条件下两份糖果的质量差距尽可能小。 o 输入一行数，包含一个数n，代表糖果数量，后面一次是n个整数一次表示每个糖果的质量，每个糖果的质量都是1到450 o 之间的一个整数，每盒最多有20个糖果。 o 输出：每个样例输出两个数字分别为两堆糖果的质量，如不相同，先小后大。 o 样例：输入：5 9 6 5 8 7 o 输出：17 18 算法思想o 回溯，在数量差值为1的结果中找出最小的质量差 Python实现1234567891011121314151617def divide(candies, num, select, sum, total, index): global min global res if(abs(total-sum*2) &lt; min): res = sum min = abs(total-sum*2) result.append(select) for i in range(len(candies)): if(index == num-1): return3 select.append(candies[index]) sum += candies[index] temp = select.copy() if(len(select) &lt;= int(num/2)+1): index += 1 divide(candies, num, temp, sum, total, index) sum -= select[len(select) - 1] select.remove(select[len(select)-1])]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回溯法思想]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[子集树与排列树当所给问题是从n个元素的集合S中找出满足某种性质的子集时，解空间为子集树。例如：0-1背包问题 (选或不选问题)当所给问题是从n个元素的集合S中找出满足某种性质的排列时，解空间为排列树。例如：旅行售货员问题（选择顺序问题） 算法结构 深度优先搜索与广度优先搜索算法有何区别深度优先搜索法不全部保留结点，扩展完的结点从数据存储结构栈中弹出删去，在栈中存储的结点数就是解空间树的深度，因此它占用空间较少。所以，当搜索树的结点较多，用其它方法易产生内存溢出时，深度优先搜索不失为一种有效的求解方法。广度优先搜索算法，一般需存储产生所有结点，占用的存储空间要比深度优先搜索大得多，因此，程序设计中，必须考虑溢出和节省内存空间的问题。但广度优先搜索法一般无回溯操作（即入栈和出栈的操作），所以运行速度比深度优先搜索要快些。 回溯与分支限界区别回溯法以深度优先的方式搜索解空间树T，而分支限界法则以广度优先或以最小耗费优先的方式搜索解空间树T。它们在问题的解空间树T上搜索的方法不同，适合解决的问题也就不同。一般情况下，回溯法的求解目标是找出T中满足约束条件的所有解的方案，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。相对而言，分支限界算法的解空间比回溯法大得多，因此当内存容量有限时，回溯法成功的可能性更大。 最优化处理问题在处理最优问题时，采用穷举法、回溯法或分支限界法都可以通过利用当前最优解和上界函数加速。仅就对限界剪支的效率而言，优先队列的分支限界法显然要更充分一些。在穷举法中通过上界函数与当前情况下函数值的比较可以直接略过不合要求的情况而省去了更进一步的枚举和判断；回溯法则因为层次的划分，可以在上界函数值小于当前最优解时，剪去以该结点为根的子树，也就是节省了搜索范围；分支限界法在这方面除了可以做到回溯法能做到的之外，同时若采用优先队列的分支限界法，用上界函数作为活结点的优先级，一旦有叶结点成为当前扩展结点，就意味着该叶结点所对应的解即为最优解，可以立即终止其余的过程。在前面的例题中曾说明，优先队列的分支限界法更象是有选择、有目的地进行搜索，时间效率、空间效率都是比较高的。 算法总结一个问题是该用递推、贪心、搜索还是动态规划，完全是由这个问题本身阶段间状态的转移方式决定的！每个阶段只有一个状态-&gt;递推；每个阶段的最优状态都是由上一个阶段的最优状态得到的-&gt;贪心；每个阶段的最优状态是由之前所有阶段的状态的组合得到的-&gt;搜索；每个阶段的最优状态可以从之前某个阶段的某个或某些状态直接得到而不管之前这个状态是如何得到的-&gt;动态规划。 动态规划1.求最优解问题2.整体问题的最优解依赖于各个子问题的最优解3.把大问题分解成小问题，小问题之间还有相互重叠的更小的子问题4.从上往下分析，从下往上求解，避免重复求解小问题]]></content>
      <categories>
        <category>面试题</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F08%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
